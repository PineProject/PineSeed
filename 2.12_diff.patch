diff --git a/ent/src/yb/cdc/cdc_service.cc b/ent/src/yb/cdc/cdc_service.cc
index 198e66093f..0fbf46f98a 100644
--- a/ent/src/yb/cdc/cdc_service.cc
+++ b/ent/src/yb/cdc/cdc_service.cc
@@ -1665,8 +1665,10 @@ void CDCServiceImpl::UpdateTabletPeersWithMinReplicatedIndex(
   auto enable_update_local_peer_min_index =
       GetAtomicFlag(&FLAGS_enable_update_local_peer_min_index);
 
-  for (auto& [tablet_id, tablet_info] : *tablet_min_checkpoint_map) {
+  for (auto& each_tablet : *tablet_min_checkpoint_map) {
     std::shared_ptr<tablet::TabletPeer> tablet_peer;
+    auto tablet_id = each_tablet.first;
+    auto tablet_info = each_tablet.second;
 
     Status s = tablet_manager_->GetTabletPeer(tablet_id, &tablet_peer);
     if (s.IsNotFound()) {
@@ -1793,8 +1795,10 @@ Status CDCServiceImpl::DeleteCDCStateTableMetadata(
   }
 
   // Iterating over set and deleting entries from the cdc_state table.
-  for (const auto& [tablet_id, stream_id] : cdc_state_entries_to_delete) {
+  for (const auto& ech_entry : cdc_state_entries_to_delete) {
     std::shared_ptr<tablet::TabletPeer> tablet_peer;
+    auto tablet_id = ech_entry.first;
+    auto stream_id = ech_entry.second;
     Status s = tablet_manager_->GetTabletPeer(tablet_id, &tablet_peer);
     if (!s.ok()) {
       LOG(WARNING) << " Could not delete the entry for stream" << stream_id << " and the tablet "
@@ -1805,7 +1809,7 @@ Status CDCServiceImpl::DeleteCDCStateTableMetadata(
       auto* const delete_req = delete_op->mutable_request();
       QLAddStringHashValue(delete_req, tablet_id);
       QLAddStringRangeValue(delete_req, stream_id);
-      Status s = session->TEST_ApplyAndFlush(delete_op);
+      Status s = session->ApplyAndFlush(delete_op);
       if (!s.ok()) {
         LOG(WARNING) << "Unable to flush operations to delete cdc streams: " << s;
         return s.CloneAndPrepend("Error deleting cdc stream rows from cdc_state table");
diff --git a/ent/src/yb/cdc/cdcsdk_producer.cc b/ent/src/yb/cdc/cdcsdk_producer.cc
index ab8011514c..91c9cd7a6a 100644
--- a/ent/src/yb/cdc/cdcsdk_producer.cc
+++ b/ent/src/yb/cdc/cdcsdk_producer.cc
@@ -611,7 +611,7 @@ Status GetChangesForCDCSDK(
       }
       txn_participant->SetIntentRetainOpIdAndTime(
           data.op_id, MonoDelta::FromMilliseconds(GetAtomicFlag(&FLAGS_cdc_intent_retention_ms)));
-      RETURN_NOT_OK(txn_participant->context()->GetLastReplicatedData(&data));
+      txn_participant->context()->GetLastReplicatedData(&data);
       time = ReadHybridTime::SingleTime(data.log_ht);
 
       // This should go to cdc_state table.
diff --git a/ent/src/yb/integration-tests/cdcsdk_ysql-test.cc b/ent/src/yb/integration-tests/cdcsdk_ysql-test.cc
index e2dd1e61b9..c1a66c2353 100644
--- a/ent/src/yb/integration-tests/cdcsdk_ysql-test.cc
+++ b/ent/src/yb/integration-tests/cdcsdk_ysql-test.cc
@@ -153,7 +153,7 @@ class CDCSDKYsqlTest : public CDCSDKTestBase {
     table.AddColumns({master::kCdcCheckpoint}, req);
 
     auto session = client->NewSession();
-    ASSERT_OK(session->TEST_ApplyAndFlush(op));
+    ASSERT_OK(session->ApplyAndFlush(op));
 
     LOG(INFO) << strings::Substitute(
         "Verifying tablet: $0, stream: $1, op_id: $2", tablet_id, stream_id,
@@ -195,7 +195,7 @@ class CDCSDKYsqlTest : public CDCSDKTestBase {
     // so even if the request has returned, it doesn't mean that the rows have been deleted yet.
     ASSERT_OK(WaitFor(
         [&]() {
-          EXPECT_OK(session->TEST_ApplyAndFlush(op));
+          EXPECT_OK(session->ApplyAndFlush(op));
           auto row_block = ql::RowsResult(op.get()).GetRowBlock();
           if (row_block->row_count() == 0) {
             return true;
@@ -1405,8 +1405,6 @@ TEST_F(CDCSDKYsqlTest, YB_DISABLE_TEST_IN_TSAN(TestDropTableBeforeXClusterStream
   ASSERT_EQ(!delete_resp.has_error(), true);
 }
 
-=======
->>>>>>> 733f21c36f... [CDCSDK] Data inconsistency in CDC after restart of tserver
 TEST_F(CDCSDKYsqlTest, YB_DISABLE_TEST_IN_TSAN(TestCheckPointPersistencyNodeRestart)) {
   FLAGS_enable_update_local_peer_min_index = false;
   FLAGS_update_min_cdc_indices_interval_secs = 1;
diff --git a/ent/src/yb/integration-tests/twodc_ysql-test.cc b/ent/src/yb/integration-tests/twodc_ysql-test.cc
index 270da0c514..8284f848f9 100644
--- a/ent/src/yb/integration-tests/twodc_ysql-test.cc
+++ b/ent/src/yb/integration-tests/twodc_ysql-test.cc
@@ -859,8 +859,8 @@ TEST_P(TwoDCYsqlTest, ColocatedDatabaseDifferentColocationIds) {
 
 TEST_P(TwoDCYsqlTest, DeleteTableChecks) {
   YB_SKIP_TEST_IN_TSAN();
-  constexpr int kNTabletsPerTable = 1;
-  std::vector<uint32_t> tables_vector = {kNTabletsPerTable, kNTabletsPerTable};
+  constexpr int kNT = 1; // Tablets per table.
+  std::vector<uint32_t> tables_vector = {kNT, kNT, kNT}; // Each entry is a table. (Currently 3)
   auto tables = ASSERT_RESULT(SetUpWithParams(tables_vector, tables_vector, 1));
   const string kUniverseId = ASSERT_RESULT(GetUniverseId(&producer_cluster_));
 
@@ -895,16 +895,53 @@ TEST_P(TwoDCYsqlTest, DeleteTableChecks) {
     }
   }
 
-  // 2. Setup replication.
+  // Set aside one table for AlterUniverseReplication.
+  std::shared_ptr<client::YBTable> producer_alter_table, consumer_alter_table;
+  producer_alter_table = producer_tables.back();
+  producer_tables.pop_back();
+  consumer_alter_table = consumer_tables.back();
+  consumer_tables.pop_back();
+
+  // 2a. Setup replication.
   ASSERT_OK(SetupUniverseReplication(producer_cluster(), consumer_cluster(), consumer_client(),
                                      kUniverseId, producer_tables));
 
-  // 3. Verify everything is setup correctly.
+  // Verify everything is setup correctly.
   master::GetUniverseReplicationResponsePB get_universe_replication_resp;
   ASSERT_OK(VerifyUniverseReplication(consumer_cluster(), consumer_client(), kUniverseId,
       &get_universe_replication_resp));
-  ASSERT_OK(CorrectlyPollingAllTablets(consumer_cluster(),
-                                       tables_vector.size() * kNTabletsPerTable));
+  ASSERT_OK(CorrectlyPollingAllTablets(
+      consumer_cluster(), static_cast<uint32_t>(producer_tables.size() * kNT)));
+
+  // 2b. Alter Replication
+  {
+    auto* consumer_leader_mini_master = ASSERT_RESULT(consumer_cluster()->GetLeaderMiniMaster());
+    auto master_proxy = std::make_shared<master::MasterReplicationProxy>(
+        &consumer_client()->proxy_cache(),
+        consumer_leader_mini_master->bound_rpc_addr());
+    master::AlterUniverseReplicationRequestPB alter_req;
+    master::AlterUniverseReplicationResponsePB alter_resp;
+    alter_req.set_producer_id(kUniverseId);
+    alter_req.add_producer_table_ids_to_add(producer_alter_table->id());
+    rpc::RpcController rpc;
+    rpc.set_timeout(MonoDelta::FromSeconds(kRpcTimeout));
+    ASSERT_OK(master_proxy->AlterUniverseReplication(alter_req, &alter_resp, &rpc));
+    ASSERT_FALSE(alter_resp.has_error());
+    // Wait until we have the new table listed in the existing universe config.
+    ASSERT_OK(LoggedWaitFor(
+        [&]() -> Result<bool> {
+          master::GetUniverseReplicationResponsePB tmp_resp;
+          RETURN_NOT_OK(VerifyUniverseReplication(consumer_cluster(), consumer_client(),
+                                                  kUniverseId, &tmp_resp));
+          return tmp_resp.entry().tables_size() == static_cast<int64>(producer_tables.size() + 1);
+        },
+        MonoDelta::FromSeconds(kRpcTimeout), "Verify table created with alter."));
+
+    ASSERT_OK(CorrectlyPollingAllTablets(
+        consumer_cluster(), static_cast<uint32_t>((producer_tables.size() + 1) * kNT)));
+  }
+  producer_tables.push_back(producer_alter_table);
+  consumer_tables.push_back(consumer_alter_table);
 
   auto data_replicated_correctly = [&](int num_results) -> Result<bool> {
     for (const auto& consumer_table : consumer_tables) {
@@ -928,16 +965,21 @@ TEST_P(TwoDCYsqlTest, DeleteTableChecks) {
                     MonoDelta::FromSeconds(20), "IsDataReplicatedCorrectly"));
 
   // Attempt to destroy the producer and consumer tables.
-  string producer_table_name = producer_tables[0]->name().ToString();
-  string producer_table_id = producer_tables[0]->id();
-  string consumer_table_name = consumer_tables[0]->name().ToString();
-  string consumer_table_id = consumer_tables[0]->id();
-  ASSERT_NOK(DeleteTable(&producer_cluster_, &producer_table_id));
-  ASSERT_NOK(DeleteTable(&consumer_cluster_, &consumer_table_id));
+  for (size_t i = 0; i < producer_tables.size(); ++i) {
+    string producer_table_id = producer_tables[i]->id();
+    string consumer_table_id = consumer_tables[i]->id();
+    ASSERT_NOK(DeleteTable(&producer_cluster_, &producer_table_id));
+    ASSERT_NOK(DeleteTable(&consumer_cluster_, &consumer_table_id));
+  }
 
-  FLAGS_enable_delete_truncate_xcluster_replicated_table = true;
-  ASSERT_OK(DeleteTable(&producer_cluster_, &producer_table_id));
-  ASSERT_OK(DeleteTable(&consumer_cluster_, &consumer_table_id));
+  ASSERT_OK(DeleteUniverseReplication(kUniverseId));
+
+  for (size_t i = 0; i < producer_tables.size(); ++i) {
+    string producer_table_id = producer_tables[i]->id();
+    string consumer_table_id = consumer_tables[i]->id();
+    ASSERT_OK(DeleteTable(&producer_cluster_, &producer_table_id));
+    ASSERT_OK(DeleteTable(&consumer_cluster_, &consumer_table_id));
+  }
 
   Destroy();
 }
diff --git a/ent/src/yb/master/catalog_manager.h b/ent/src/yb/master/catalog_manager.h
index ad1f56c81f..6ee37362cc 100644
--- a/ent/src/yb/master/catalog_manager.h
+++ b/ent/src/yb/master/catalog_manager.h
@@ -92,15 +92,6 @@ class CatalogManager : public yb::master::CatalogManager, SnapshotCoordinatorCon
 
   void DumpState(std::ostream* out, bool on_disk_dump = false) const override;
 
-  CHECKED_STATUS HandlePlacementUsingReplicationInfo(const ReplicationInfoPB& replication_info,
-                                                     const TSDescriptorVector& all_ts_descs,
-                                                     consensus::RaftConfigPB* config) override;
-
-  // Populates ts_descs with all tservers belonging to a certain placement.
-  void GetTsDescsFromPlacementInfo(const PlacementInfoPB& placement_info,
-                                   const TSDescriptorVector& all_ts_descs,
-                                   TSDescriptorVector* ts_descs);
-
   // Fills the heartbeat response with the decrypted universe key registry.
   CHECKED_STATUS FillHeartbeatResponse(const TSHeartbeatRequestPB* req,
                                        TSHeartbeatResponsePB* resp) override;
@@ -403,7 +394,8 @@ class CatalogManager : public yb::master::CatalogManager, SnapshotCoordinatorCon
                                              const Result<CDCStreamId>& stream_id,
                                              std::function<void()> on_success_cb = nullptr);
 
-  void MergeUniverseReplication(scoped_refptr<UniverseReplicationInfo> info);
+  void MergeUniverseReplication(scoped_refptr<UniverseReplicationInfo> info,
+                                std::string original_id);
   CHECKED_STATUS DeleteUniverseReplicationUnlocked(scoped_refptr<UniverseReplicationInfo> info);
   void MarkUniverseReplicationFailed(scoped_refptr<UniverseReplicationInfo> universe,
                                      const Status& failure_status);
diff --git a/ent/src/yb/master/catalog_manager_ent.cc b/ent/src/yb/master/catalog_manager_ent.cc
index 2f18933dad..92184719b4 100644
--- a/ent/src/yb/master/catalog_manager_ent.cc
+++ b/ent/src/yb/master/catalog_manager_ent.cc
@@ -1287,6 +1287,12 @@ Status CatalogManager::RecreateTable(const NamespaceId& new_namespace_id,
     }
   }
 
+  req.set_is_matview(meta.is_matview());
+
+  if (meta.has_matview_pg_table_id()) {
+    req.set_matview_pg_table_id(meta.matview_pg_table_id());
+  }
+
   RETURN_NOT_OK(CreateTable(&req, &resp, /* RpcContext */nullptr));
   table_data->new_table_id = resp.table_id();
   LOG_WITH_FUNC(INFO) << "New table id " << table_data->new_table_id << " for "
@@ -2387,44 +2393,6 @@ void CatalogManager::DumpState(std::ostream* out, bool on_disk_dump) const {
   // TODO: dump snapshots
 }
 
-
-Status CatalogManager::HandlePlacementUsingReplicationInfo(
-    const ReplicationInfoPB& replication_info,
-    const TSDescriptorVector& all_ts_descs,
-    consensus::RaftConfigPB* config) {
-  TSDescriptorVector ts_descs;
-  GetTsDescsFromPlacementInfo(replication_info.live_replicas(), all_ts_descs, &ts_descs);
-  RETURN_NOT_OK(super::HandlePlacementUsingPlacementInfo(replication_info.live_replicas(),
-                                                      ts_descs,
-                                                      consensus::PeerMemberType::VOTER, config));
-  for (int i = 0; i < replication_info.read_replicas_size(); i++) {
-    GetTsDescsFromPlacementInfo(replication_info.read_replicas(i), all_ts_descs, &ts_descs);
-    RETURN_NOT_OK(super::HandlePlacementUsingPlacementInfo(replication_info.read_replicas(i),
-                                                           ts_descs,
-                                                           consensus::PeerMemberType::OBSERVER,
-                                                           config));
-  }
-  return Status::OK();
-}
-
-void CatalogManager::GetTsDescsFromPlacementInfo(const PlacementInfoPB& placement_info,
-                                                 const TSDescriptorVector& all_ts_descs,
-                                                 TSDescriptorVector* ts_descs) {
-  ts_descs->clear();
-  for (const auto& ts_desc : all_ts_descs) {
-    TSDescriptor* ts_desc_ent = down_cast<TSDescriptor*>(ts_desc.get());
-    if (placement_info.has_placement_uuid()) {
-      string placement_uuid = placement_info.placement_uuid();
-      if (ts_desc_ent->placement_uuid() == placement_uuid) {
-        ts_descs->push_back(ts_desc);
-      }
-    } else if (ts_desc_ent->placement_uuid() == "") {
-      // Since the placement info has no placement id, we know it is live, so we add this ts.
-      ts_descs->push_back(ts_desc);
-    }
-  }
-}
-
 template <typename Registry, typename Mutex>
 bool ShouldResendRegistry(
     const std::string& ts_uuid, bool has_registration, Registry* registry, Mutex* mutex) {
@@ -3879,6 +3847,7 @@ void CatalogManager::AddCDCStreamToUniverseAndInitConsumer(
 
   bool merge_alter = false;
   bool validated_all_tables = false;
+  std::vector<CDCConsumerStreamInfo> consumer_info;
   {
     auto l = universe->LockForWrite();
     if (l->is_deleted_or_failed()) {
@@ -3897,7 +3866,6 @@ void CatalogManager::AddCDCStreamToUniverseAndInitConsumer(
 
       auto& validated_tables = l->pb.validated_tables();
 
-      std::vector<CDCConsumerStreamInfo> consumer_info;
       consumer_info.reserve(l->pb.tables_size());
       for (const auto& table : validated_tables) {
         CDCConsumerStreamInfo info;
@@ -3939,18 +3907,20 @@ void CatalogManager::AddCDCStreamToUniverseAndInitConsumer(
   }
 
   if (validated_all_tables) {
-    // Also update the set of consumer tables.
+    GStringPiece final_id(universe->id());
+    // If this is an 'alter', merge back into primary command now that setup is a success.
+    if (merge_alter) {
+      final_id.remove_suffix(sizeof(".ALTER")-1 /* exclude \0 ending */);
+      MergeUniverseReplication(universe, final_id.ToString());
+    }
+    // Update the in-memory cache of consumer tables.
     LockGuard lock(mutex_);
-    auto l = universe->LockForRead();
-    for (const auto& table : l->pb.validated_tables()) {
-      xcluster_consumer_tables_to_stream_map_[table.second].emplace(universe->id(), *stream_id);
+    for (const auto& info : consumer_info) {
+      auto c_table_id = info.consumer_table_id;
+      auto c_stream_id = info.stream_id;
+      xcluster_consumer_tables_to_stream_map_[c_table_id].emplace(final_id.ToString(), c_stream_id);
     }
   }
-
-  // If this is an 'alter', merge back into primary command now that setup is a success.
-  if (merge_alter) {
-    MergeUniverseReplication(universe);
-  }
 }
 
 /*
@@ -4057,24 +4027,19 @@ Status CatalogManager::InitCDCConsumer(
   return Status::OK();
 }
 
-void CatalogManager::MergeUniverseReplication(scoped_refptr<UniverseReplicationInfo> universe) {
+void CatalogManager::MergeUniverseReplication(scoped_refptr<UniverseReplicationInfo> universe,
+                                              std::string original_id) {
   // Merge back into primary command now that setup is a success.
-  GStringPiece original_producer_id(universe->id());
-  if (!original_producer_id.ends_with(".ALTER")) {
-    return;
-  }
-  original_producer_id.remove_suffix(sizeof(".ALTER")-1 /* exclude \0 ending */);
-  LOG(INFO) << "Merging CDC universe: " << universe->id()
-            << " into " << original_producer_id.ToString();
+  LOG(INFO) << "Merging CDC universe: " << universe->id() << " into " << original_id;
 
   scoped_refptr<UniverseReplicationInfo> original_universe;
   {
     SharedLock lock(mutex_);
     TRACE("Acquired catalog manager lock");
 
-    original_universe = FindPtrOrNull(universe_replication_map_, original_producer_id.ToString());
+    original_universe = FindPtrOrNull(universe_replication_map_, original_id);
     if (original_universe == nullptr) {
-      LOG(ERROR) << "Universe not found: " << original_producer_id.ToString();
+      LOG(ERROR) << "Universe not found: " << original_id;
       return;
     }
   }
@@ -4779,8 +4744,7 @@ bool CatalogManager::IsTableCdcProducer(const TableInfo& table_info) const {
       auto s = entry.second->LockForRead();
       // for xCluster the first entry will be the table_id
       const auto& table_id = s->table_id();
-      if (!table_id.empty() && table_id.Get(0) == tid &&
-          !(s->is_deleting() || s->is_deleted())) {
+      if (!table_id.empty() && table_id.Get(0) == tid && !s->started_deleting()) {
         return true;
       }
     }
diff --git a/ent/src/yb/tools/yb-admin_client_ent.cc b/ent/src/yb/tools/yb-admin_client_ent.cc
index f64c80b37c..4056b5cbf3 100644
--- a/ent/src/yb/tools/yb-admin_client_ent.cc
+++ b/ent/src/yb/tools/yb-admin_client_ent.cc
@@ -275,6 +275,7 @@ Status ClusterAdminClient::CreateNamespaceSnapshot(const TypedNamespaceName& ns)
     req.set_exclude_system_tables(true);
     req.add_relation_type_filter(master::USER_TABLE_RELATION);
     req.add_relation_type_filter(master::INDEX_TABLE_RELATION);
+    req.add_relation_type_filter(master::MATVIEW_TABLE_RELATION);
     return master_ddl_proxy_->ListTables(req, &resp, rpc);
   }));
 
@@ -290,7 +291,8 @@ Status ClusterAdminClient::CreateNamespaceSnapshot(const TypedNamespaceName& ns)
     tables[i].set_pgschema_name(table.pgschema_name());
 
     RSTATUS_DCHECK(table.relation_type() == master::USER_TABLE_RELATION ||
-            table.relation_type() == master::INDEX_TABLE_RELATION, InternalError,
+            table.relation_type() == master::INDEX_TABLE_RELATION ||
+            table.relation_type() == master::MATVIEW_TABLE_RELATION, InternalError,
             Format("Invalid relation type: $0", table.relation_type()));
     RSTATUS_DCHECK_EQ(table.namespace_().name(), ns.name, InternalError,
                Format("Invalid namespace name: $0", table.namespace_().name()));
diff --git a/java/yb-cql/src/test/java/org/yb/cql/TestIndex.java b/java/yb-cql/src/test/java/org/yb/cql/TestIndex.java
index b2082dd7ea..1874a9cd07 100644
--- a/java/yb-cql/src/test/java/org/yb/cql/TestIndex.java
+++ b/java/yb-cql/src/test/java/org/yb/cql/TestIndex.java
@@ -2514,7 +2514,17 @@ public class TestIndex extends BaseCQLTest {
     // Index table: Hash key column.
     assertQuery(tp, "SELECT * FROM test_in2 WHERE v1 IN ()", "");
     // Index table: Range key column.
+    RocksDBMetrics tableMetrics = getRocksDBMetric("test_in2");
+    RocksDBMetrics indexMetrics = getRocksDBMetric("i2");
+    LOG.info("Initial: table {}, index {}", tableMetrics, indexMetrics);
     assertQuery(tp, "SELECT * FROM test_in2 WHERE v1 = 'v1' AND r IN ()", "");
+    tableMetrics = getRocksDBMetric("test_in2").subtract(tableMetrics);
+    indexMetrics = getRocksDBMetric("i2").subtract(indexMetrics);
+    LOG.info("Difference: table {}, index {}", tableMetrics, indexMetrics);
+    assertTrue(tableMetrics.nextCount == 0);
+    assertTrue(tableMetrics.seekCount == 0);
+    assertTrue(indexMetrics.nextCount == 0);
+    assertTrue(indexMetrics.seekCount == 0);
     // Index table: Non-key column.
     assertQuery(tp, "SELECT * FROM test_in2 WHERE v1 = 'v1' AND r = 'bar' AND v2 IN ()", "");
   }
diff --git a/java/yb-cql/src/test/java/org/yb/cql/TestIndexSelection.java b/java/yb-cql/src/test/java/org/yb/cql/TestIndexSelection.java
index 891c958d90..995574e56e 100644
--- a/java/yb-cql/src/test/java/org/yb/cql/TestIndexSelection.java
+++ b/java/yb-cql/src/test/java/org/yb/cql/TestIndexSelection.java
@@ -204,6 +204,33 @@ public class TestIndexSelection extends BaseCQLTest {
                 "Row[1, 2, 2, 1, 2, 2, 102]");
   }
 
+  @Test
+  public void testNullsInIndexScan() throws Exception {
+    // Table.
+    session.execute("CREATE TABLE test_secondary_index1" +
+                    "  ( h1 INT, r1 INT, i1 INT, i2 INT, " +
+                    "    PRIMARY KEY (h1, r1) )" +
+                    "  WITH transactions = {'enabled' : true}");
+    // Index.
+    session.execute("CREATE INDEX second_idx1 ON test_secondary_index1(r1, i1, i2)");
+
+    // Insert data.
+    session.execute("INSERT INTO test_secondary_index1(h1, r1, i1, i2)" +
+                    "  VALUES (1, 2, null, 3)");
+    // Make sure secondary index is chosen.
+    assertQuery("EXPLAIN SELECT * FROM test_secondary_index1" +
+                "  WHERE r1 = 2 AND i1 = null AND i2 IN (1,3,2)",
+                "Row[Index Only Scan using yugatest.second_idx1 on" +
+                " yugatest.test_secondary_index1]" +
+                "Row[  Key Conditions: (r1 = 2)" +
+                "                                                  ]" +
+                "Row[  Filter: (i1 = NULL) AND (i2 IN expr)" +
+                "                                      ]");
+
+    assertQuery("SELECT * FROM test_secondary_index1 WHERE r1 = 2 AND i1 = null AND i2 IN (1,3,2)",
+                "Row[1, 2, NULL, 3]");
+  }
+
   @Test
   public void testOrderByIndexScan() throws Exception {
     // Table.
diff --git a/java/yb-pgsql/src/test/java/org/yb/pgsql/BasePgSQLTest.java b/java/yb-pgsql/src/test/java/org/yb/pgsql/BasePgSQLTest.java
index 23f7c625f0..ddde287980 100644
--- a/java/yb-pgsql/src/test/java/org/yb/pgsql/BasePgSQLTest.java
+++ b/java/yb-pgsql/src/test/java/org/yb/pgsql/BasePgSQLTest.java
@@ -64,6 +64,15 @@ import java.util.stream.Stream;
 public class BasePgSQLTest extends BaseMiniClusterTest {
   private static final Logger LOG = LoggerFactory.getLogger(BasePgSQLTest.class);
 
+  /** Corresponds to the original value of YB_MIN_UNUSED_OID. */
+  protected final long FIRST_YB_OID = 8000;
+
+  /** Matches Postgres' FirstBootstrapObjectId */
+  protected final long FIRST_BOOTSTRAP_OID = 10000;
+
+  /** Matches Postgres' FirstNormalObjectId */
+  protected final long FIRST_NORMAL_OID = 16384;
+
   // Postgres settings.
   protected static final String DEFAULT_PG_DATABASE = "yugabyte";
   protected static final String DEFAULT_PG_USER = "yugabyte";
diff --git a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestIndexBackfill.java b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestIndexBackfill.java
index f29ae6986f..75d6202d38 100644
--- a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestIndexBackfill.java
+++ b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestIndexBackfill.java
@@ -82,7 +82,7 @@ public class TestIndexBackfill extends BasePgSQLTest {
           Statement stmt = conn.createStatement()) {
         backfillThreadStarted.countDown();
         insertDone.await(AWAIT_TIMEOUT_SEC, TimeUnit.SECONDS);
-        // This will wait for pg_index.indisready=true
+        // This will wait for pg_index.indisvalid=true
         stmt.executeUpdate("CREATE INDEX " + indexName + " ON " + tableName + "(v ASC)");
       } catch (Exception ex) {
         LOG.error("CREATE INDEX thread failed", ex);
diff --git a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgAuthorization.java b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgAuthorization.java
index b27ba5bc6a..5959cdfecf 100644
--- a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgAuthorization.java
+++ b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgAuthorization.java
@@ -86,6 +86,8 @@ public class TestPgAuthorization extends BasePgSQLTest {
       statement.execute("CREATE ROLE su LOGIN SUPERUSER");
       statement.execute("CREATE ROLE some_role LOGIN");
       statement.execute("CREATE ROLE some_group ROLE some_role");
+      statement.execute("CREATE ROLE yb_db_admin_member LOGIN");
+      statement.execute("GRANT yb_db_admin TO yb_db_admin_member");
     }
 
     try (Connection connection = getConnectionBuilder().withUser("su").connect();
@@ -172,6 +174,7 @@ public class TestPgAuthorization extends BasePgSQLTest {
       // Non-superuser cannot set session authorization to other roles.
       runInvalidQuery(statement, "SET SESSION AUTHORIZATION some_group", PERMISSION_DENIED);
       runInvalidQuery(statement, "SET SESSION AUTHORIZATION unprivileged", PERMISSION_DENIED);
+      runInvalidQuery(statement, "SET SESSION AUTHORIZATION yb_db_admin", PERMISSION_DENIED);
 
       assertEquals("some_role", getSessionUser(statement));
       assertEquals("some_role", getCurrentUser(statement));
@@ -188,6 +191,22 @@ public class TestPgAuthorization extends BasePgSQLTest {
       assertEquals("some_role", getCurrentUser(statement));
     }
 
+    // Test yb_db_admin members can set session authorization.
+    try (Connection connection = getConnectionBuilder().withUser("yb_db_admin_member").connect();
+         Statement statement = connection.createStatement()) {
+      // Users have been reset, since this is a new session.
+      statement.execute("SET SESSION AUTHORIZATION unprivileged");
+      assertEquals("unprivileged", getSessionUser(statement));
+      assertEquals("unprivileged", getCurrentUser(statement));
+    }
+
+    // Test yb_db_admin members cannot set session authorization to a superuser.
+    try (Connection connection = getConnectionBuilder().withUser("yb_db_admin_member").connect();
+        Statement statement = connection.createStatement()) {
+      // yb_db_admin members cannot set session authorization to superuser role.
+      runInvalidQuery(statement, "SET SESSION AUTHORIZATION su", PERMISSION_DENIED);
+    }
+
     final AtomicInteger state = new AtomicInteger(0);
     final Lock lock = new ReentrantLock();
     final Condition condition = lock.newCondition();
@@ -2721,6 +2740,7 @@ public class TestPgAuthorization extends BasePgSQLTest {
          Statement statement1 = connection1.createStatement()) {
 
       statement1.execute("CREATE ROLE test_role LOGIN");
+      statement1.execute("CREATE ROLE test_role2 LOGIN");
 
       try (Connection connection2 = getConnectionBuilder().withTServer(1)
           .withUser("test_role").connect();
@@ -2728,6 +2748,7 @@ public class TestPgAuthorization extends BasePgSQLTest {
         runInvalidQuery(statement2, "CREATE ROLE tr", PERMISSION_DENIED);
         runInvalidQuery(statement2, "CREATE DATABASE tdb", PERMISSION_DENIED);
         runInvalidQuery(statement2, "CREATE ROLE su SUPERUSER", "must be superuser");
+        runInvalidQuery(statement2, "SET SESSION AUTHORIZATION test_role2",PERMISSION_DENIED);
 
         // Grant CREATEROLE from connection 1.
         statement1.execute("ALTER ROLE test_role CREATEROLE");
@@ -2738,6 +2759,7 @@ public class TestPgAuthorization extends BasePgSQLTest {
         statement2.execute("CREATE ROLE tr");
         runInvalidQuery(statement2, "CREATE DATABASE tdb", PERMISSION_DENIED);
         runInvalidQuery(statement2, "CREATE ROLE su SUPERUSER", "must be superuser");
+        runInvalidQuery(statement2, "SET SESSION AUTHORIZATION test_role2", PERMISSION_DENIED);
 
         // Grant CREATEDB from connection 1.
         statement1.execute("ALTER ROLE test_role CREATEDB");
@@ -2747,6 +2769,7 @@ public class TestPgAuthorization extends BasePgSQLTest {
         // New attribute observed on connection 2 after heartbeat.
         statement2.execute("CREATE DATABASE tdb");
         runInvalidQuery(statement2, "CREATE ROLE su SUPERUSER", "must be superuser");
+        runInvalidQuery(statement2, "SET SESSION AUTHORIZATION test_role2", PERMISSION_DENIED);
 
         // Grant SUPERUSER from connection 1.
         statement1.execute("ALTER ROLE test_role SUPERUSER");
@@ -2755,10 +2778,23 @@ public class TestPgAuthorization extends BasePgSQLTest {
 
         // New attribute observed on connection 2 after heartbeat.
         statement2.execute("CREATE ROLE su SUPERUSER");
+        runInvalidQuery(statement2, "SET SESSION AUTHORIZATION test_role2", PERMISSION_DENIED);
 
         // "test_role" still cannot set their session authorization, despite having
         // superuser privileges.
         runInvalidQuery(statement2, "SET SESSION AUTHORIZATION su", PERMISSION_DENIED);
+
+        // Grant yb_db_admin from connection 1.
+        statement1.execute("GRANT yb_db_admin TO test_role");
+
+        waitForTServerHeartbeat();
+
+        // New attribute observed on connection 2 after heartbeat.
+        statement2.execute("SET SESSION AUTHORIZATION test_role2");
+
+        // "test_role" still cannot set their session authorization to a superuser,
+        // despite having yb_db_admin privileges.
+        runInvalidQuery(statement2, "SET SESSION AUTHORIZATION su", PERMISSION_DENIED);
       }
     }
   }
diff --git a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgDdlFaultTolerance.java b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgDdlFaultTolerance.java
index 6a5b4c8a03..c822413b0c 100644
--- a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgDdlFaultTolerance.java
+++ b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgDdlFaultTolerance.java
@@ -330,6 +330,37 @@ public class TestPgDdlFaultTolerance extends BasePgSQLTest {
     }
   }
 
+  /*
+   * Test failure injection for REFRESHes on materialized views.
+   */
+  @Test
+  public void testRefreshMatviewFailureInjection() throws Exception {
+      try (Connection connection = getConnectionBuilder().connect();
+           Statement statement = connection.createStatement()) {
+
+      statement.execute("CREATE TABLE test (col int)");
+      statement.execute("CREATE MATERIALIZED VIEW mv AS SELECT * FROM test");
+
+      HostAndPort masterLeaderAddress = getMasterLeaderAddress();
+
+      setYSQLCatalogWriteRejection(masterLeaderAddress, 100);
+
+      runInvalidQuery(statement,"REFRESH MATERIALIZED VIEW mv",
+                      "Injected random failure for testing");
+
+      setYSQLCatalogWriteRejection(masterLeaderAddress, 0);
+
+      // Materialized view should still be usable.
+      statement.execute("SELECT * FROM mv");
+      statement.execute("INSERT INTO test VALUES (1)");
+      statement.execute("REFRESH MATERIALIZED VIEW mv");
+
+      Set<Row> expectedRows = new HashSet<>();
+      expectedRows.add(new Row(1));
+      assertRowSet(statement, "SELECT * from mv", expectedRows);
+    }
+  }
+
   public boolean checkIntermittentYsqlWriteFailure(HostAndPort masterLeaderAddress,
                                                   Statement statement) throws Exception {
 
diff --git a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgIsolationRegress.java b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgIsolationRegress.java
index 7da040f748..9b2a1e02d7 100644
--- a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgIsolationRegress.java
+++ b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgIsolationRegress.java
@@ -32,7 +32,7 @@ public class TestPgIsolationRegress extends BasePgSQLTest {
 
   @Override
   public int getTestMethodTimeoutSec() {
-    return 1600;
+    return 1800;
   }
 
   private void runIsolationRegressTest() throws Exception {
diff --git a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgRegressPgStat.java b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgRegressPgStat.java
new file mode 100644
index 0000000000..6f4e4b0c89
--- /dev/null
+++ b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgRegressPgStat.java
@@ -0,0 +1,34 @@
+// Copyright (c) YugaByte, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+package org.yb.pgsql;
+
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.yb.util.YBTestRunnerNonTsanOnly;
+
+/**
+ * Runs the pg_regress test suite on YB code.
+ */
+@RunWith(value=YBTestRunnerNonTsanOnly.class)
+public class TestPgRegressPgStat extends BasePgSQLTest {
+  @Override
+  public int getTestMethodTimeoutSec() {
+    return 1800;
+  }
+
+  @Test
+  public void testPgStat() throws Exception {
+    runPgRegressTest("yb_pg_stat_schedule");
+  }
+}
diff --git a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgTransactions.java b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgTransactions.java
index 05cf43a88d..14ec50db16 100644
--- a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgTransactions.java
+++ b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgTransactions.java
@@ -1046,4 +1046,19 @@ public class TestPgTransactions extends BasePgSQLTest {
       Collections.emptyMap(),
       Collections.singletonMap("yb_enable_read_committed_isolation", "false"));
   }
+
+  @Test
+  public void testMiscellaneous() throws Exception {
+    // Test issue #12004 - READ COMMITTED isolation in YSQL maps to REPEATABLE READ if
+    // yb_enable_read_committed_isolation=false. In this case, if the first statement takes an
+    // explicit locking, a transaction should be present/created.
+    try (Statement s1 = getConnectionBuilder().connect().createStatement();) {
+      s1.execute("CREATE TABLE test (k int PRIMARY KEY, v INT)");
+      s1.execute("INSERT INTO test values (1, 1)");
+      s1.execute("BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED");
+      s1.execute("SELECT * FROM test WHERE k=1 for update");
+      s1.execute("COMMIT");
+      s1.execute("DROP TABLE test");
+    }
+  }
 }
diff --git a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgUniqueConstraint.java b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgUniqueConstraint.java
index 8f0e4b4ab3..5a6bed8d9a 100644
--- a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgUniqueConstraint.java
+++ b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestPgUniqueConstraint.java
@@ -266,45 +266,56 @@ public class TestPgUniqueConstraint extends BasePgSQLTest {
   @Test
   public void addUniqueWithInclude() throws Exception {
     try (Statement stmt = connection.createStatement()) {
-      stmt.execute("CREATE TABLE test(i1 int, i2 int)");
-      stmt.execute("ALTER TABLE test ADD CONSTRAINT test_constr UNIQUE (i1) INCLUDE (i2)");
+      stmt.execute("CREATE TABLE test(i1 int, i2 int, i3 int)");
+      stmt.execute("ALTER TABLE test ADD CONSTRAINT test_constr UNIQUE (i3) INCLUDE (i1, i2)");
 
       // Check that index is created properly
       assertQuery(
           stmt,
-          "SELECT pg_get_indexdef(i.indexrelid)\n" +
-              "FROM pg_index i JOIN pg_class c ON i.indexrelid = c.oid\n" +
-              "WHERE i.indrelid = 'test'::regclass",
+          "SELECT pg_get_indexdef(i.indexrelid)" +
+              " FROM pg_index i" +
+              " WHERE i.indrelid = 'test'::regclass",
           new Row("CREATE UNIQUE INDEX test_constr ON public.test " +
-                      "USING lsm (i1 HASH) INCLUDE (i2)")
+                      "USING lsm (i3 HASH) INCLUDE (i1, i2)")
       );
 
-      // Valid insertions
-      stmt.execute("INSERT INTO test(i1, i2) VALUES (1, 3), (2, 4), (3, 3)");
+      // Valid DMLs
+      stmt.execute("INSERT INTO test(i3, i2) VALUES (1, 3), (2, 4), (3, 3), (4, NULL)");
+      stmt.execute("UPDATE test SET i2 = 2 WHERE i2 = 4");
+      stmt.execute("UPDATE test SET i1 = 1 WHERE i1 IS NULL AND i3 = 4");
+
+      // Invalid DMLs
+      runInvalidQuery(stmt, "INSERT INTO test(i3, i2) VALUES (1, 3)", "duplicate");
+      runInvalidQuery(stmt, "INSERT INTO test(i3, i2) VALUES (4, NULL)", "duplicate");
+      runInvalidQuery(stmt, "INSERT INTO test(i3) VALUES (4)", "duplicate");
 
-      // Invalid insertion
-      runInvalidQuery(stmt, "INSERT INTO test(i1, i2) VALUES (1, 3)", "duplicate");
+      // Check the resulting table content
+      assertQuery(stmt, "SELECT * FROM test ORDER BY i3, i2, i1",
+          new Row(null, 3, 1),
+          new Row(null, 2, 2),
+          new Row(null, 3, 3),
+          new Row(1, null, 4));
 
       // Selection containing inequality (<) on i1 is a full table scan until we support proper
       // range scan on index.
       assertQuery(
           stmt,
-          "EXPLAIN (COSTS OFF) SELECT * FROM test WHERE (i1, i2) < (4, 4)",
+          "EXPLAIN (COSTS OFF) SELECT * FROM test WHERE (i3, i2) < (4, 4)",
           new Row("Seq Scan on test"),
-          new Row("  Filter: (ROW(i1, i2) < ROW(4, 4))")
+          new Row("  Filter: (ROW(i3, i2) < ROW(4, 4))")
       );
 
       // Switch to a unique index without importing i2
       stmt.execute("ALTER TABLE test DROP CONSTRAINT test_constr");
-      stmt.execute("ALTER TABLE test ADD CONSTRAINT test_constr UNIQUE (i1)");
+      stmt.execute("ALTER TABLE test ADD CONSTRAINT test_constr UNIQUE (i3)");
 
       // Selection containing inequality (<) on i1 is a full table scan until we support proper
       // range scan on index.
       assertQuery(
           stmt,
-          "EXPLAIN (COSTS OFF) SELECT * FROM test WHERE (i1, i2) < (4, 4)",
+          "EXPLAIN (COSTS OFF) SELECT * FROM test WHERE (i3, i2) < (4, 4)",
           new Row("Seq Scan on test"),
-          new Row("  Filter: (ROW(i1, i2) < ROW(4, 4))")
+          new Row("  Filter: (ROW(i3, i2) < ROW(4, 4))")
       );
     }
   }
diff --git a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestTablespaceProperties.java b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestTablespaceProperties.java
index b4e9e07301..c32a0bfcdd 100644
--- a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestTablespaceProperties.java
+++ b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestTablespaceProperties.java
@@ -337,11 +337,10 @@ public class TestTablespaceProperties extends BasePgSQLTest {
       setupStatement.execute("CREATE TABLE negativeTestTable (a int)");
     }
 
-    final String not_enough_tservers_in_zone_msg = "Not enough tablet servers in " +
-                                                   "cloud3:region1:zone1";
-    final String not_enough_tservers_for_rf_msg = "Not enough live tablet servers to create " +
-                                                  "table with replication factor 5. 3 tablet " +
-                                                  "servers are alive";
+    final String not_enough_tservers_in_zone_msg =
+        "Not enough tablet servers in the requested placements. Need at least 2, have 1";
+    final String not_enough_tservers_for_rf_msg =
+        "Not enough tablet servers in the requested placements. Need at least 3, have 2";
 
     // Test creation of table in invalid tablespace.
     executeAndAssertErrorThrown(
diff --git a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestYbBackup.java b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestYbBackup.java
index 8b67832d67..5a7c5c1c78 100644
--- a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestYbBackup.java
+++ b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestYbBackup.java
@@ -283,7 +283,7 @@ public class TestYbBackup extends BasePgSQLTest {
   public void testColocatedWithColocationIdAlreadySet() throws Exception {
     String ybTablePropsSql = "SELECT c.relname, props.colocation_id"
         + " FROM pg_class c, yb_table_properties(c.oid) props"
-        + " WHERE c.oid >= 16384"
+        + " WHERE c.oid >= " + FIRST_NORMAL_OID
         + " ORDER BY c.relname";
     String uniqueIndexOnlySql = "SELECT b FROM test_tbl WHERE b = 3.14";
     try (Statement stmt = connection.createStatement()) {
@@ -369,7 +369,7 @@ public class TestYbBackup extends BasePgSQLTest {
     String ybTablePropsSql = "SELECT c.relname, tg.grpname, props.colocation_id"
         + " FROM pg_class c, yb_table_properties(c.oid) props"
         + " LEFT JOIN pg_yb_tablegroup tg ON tg.oid = props.tablegroup_oid"
-        + " WHERE c.oid >= 16384"
+        + " WHERE c.oid >= " + FIRST_NORMAL_OID
         + " ORDER BY c.relname";
     String uniqueIndexOnlySql = "SELECT b FROM test_tbl WHERE b = 3.14";
     try (Statement stmt = connection.createStatement()) {
diff --git a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestYsqlUpgrade.java b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestYsqlUpgrade.java
index b38dcb3398..d3d06029f7 100644
--- a/java/yb-pgsql/src/test/java/org/yb/pgsql/TestYsqlUpgrade.java
+++ b/java/yb-pgsql/src/test/java/org/yb/pgsql/TestYsqlUpgrade.java
@@ -36,7 +36,6 @@ import java.util.function.Function;
 import java.util.stream.Collectors;
 
 import org.apache.commons.lang3.StringUtils;
-import org.apache.commons.lang3.builder.HashCodeBuilder;
 import org.apache.commons.lang3.tuple.Pair;
 import org.junit.After;
 import org.junit.Before;
@@ -44,7 +43,6 @@ import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TestName;
 import org.junit.runner.RunWith;
-import com.yugabyte.jdbc.PgArray;
 import com.yugabyte.util.PGobject;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -819,6 +817,9 @@ public class TestYsqlUpgrade extends BasePgSQLTest {
       postSnapshotTemplate1 = takeSysCatalogSnapshot(stmt);
     }
 
+    assertYbbasectidIsConsistent("template1");
+    assertYbbasectidIsConsistent(customDbName);
+
     assertMigrationsWorked(preSnapshotCustom, postSnapshotCustom);
     assertMigrationsWorked(preSnapshotTemplate1, postSnapshotTemplate1);
 
@@ -1207,7 +1208,7 @@ public class TestYsqlUpgrade extends BasePgSQLTest {
 
   /** Whether this OID looks like it was auto-generated during initdb. */
   private boolean isSysGeneratedOid(Long oid) {
-    return oid >= 10000 /* FirstBootstrapObjectId */ && oid < 16384 /* FirstNormalObjectId */;
+    return oid >= FIRST_BOOTSTRAP_OID && oid < FIRST_NORMAL_OID;
   }
 
   private void assertAllOidsAreSysGenerated(Statement stmt, String tableName) throws Exception {
@@ -1373,6 +1374,43 @@ public class TestYsqlUpgrade extends BasePgSQLTest {
     });
   }
 
+  /**
+   * In issue #12258 we found that index's ybbasectid referencing table's ybctid was broken for
+   * shared inserts (operations used to insert a row across all databases) and indexed tables
+   * without primary key.
+   * <p>
+   * This test ensures this is no longer the case.
+   */
+  private void assertYbbasectidIsConsistent(String databaseName) throws Exception {
+    try (Connection conn = getConnectionBuilder().withDatabase(databaseName).connect();
+         Statement stmt = conn.createStatement()) {
+      // pg_depend has no primary key, so we use it and its pg_depend_reference_index to ensure
+      // index consistency after upgrade.
+      String seqScanSql = "SELECT * FROM pg_depend";
+      assertFalse(isIndexScan(stmt, seqScanSql, "" /* should not use any index */));
+      List<Row> seqScanRows =
+          getRowList(stmt, seqScanSql)
+              .stream()
+              .filter(r -> r.getLong(3) == 1259L /* pg_class oid*/)
+              .filter(r -> r.getLong(4) >= FIRST_YB_OID && r.getLong(4) < FIRST_BOOTSTRAP_OID)
+              .sorted()
+              .collect(Collectors.toList());
+      assertFalse(seqScanRows.isEmpty());
+
+      // Without a fix for #12258, this query results in "DocKey(...) not found in indexed table".
+      String indexScanSql =
+          "SELECT * FROM pg_depend"
+              + " WHERE refclassid = 1259"
+              + " AND refobjid >= " + FIRST_YB_OID
+              + " AND refobjid < " + FIRST_BOOTSTRAP_OID;
+      assertTrue(isIndexScan(stmt, indexScanSql, "pg_depend_reference_index"));
+      List<Row> indexScanRows = getRowList(stmt, indexScanSql);
+      Collections.sort(indexScanRows);
+
+      assertEquals(seqScanRows, indexScanRows);
+    }
+  }
+
   /** Returns the deep copy with referenced OIDs replaced with entity names. */
   private List<Row> copyWithResolvedOids(
       String tableName,
diff --git a/managed/build.sbt b/managed/build.sbt
index 8772e54e65..e8bbdc231c 100644
--- a/managed/build.sbt
+++ b/managed/build.sbt
@@ -178,7 +178,8 @@ libraryDependencies ++= Seq(
   "org.unix4j" % "unix4j-command" % "0.6",
   "com.github.dikhan" % "pagerduty-client" % "3.1.2",
   "com.bettercloud" % "vault-java-driver" % "5.1.0",
-  "org.apache.directory.api" % "api-all" % "2.1.0"
+  "org.apache.directory.api" % "api-all" % "2.1.0",
+  "org.apache.commons" % "commons-text" % "1.9"
 )
 // Clear default resolvers.
 appResolvers := None
diff --git a/managed/devops/bin/cluster_health.py b/managed/devops/bin/cluster_health.py
index c467b916d3..ce40765ab5 100755
--- a/managed/devops/bin/cluster_health.py
+++ b/managed/devops/bin/cluster_health.py
@@ -99,6 +99,13 @@ class Entry:
         self.metric_value = None
         self.has_error = None
         self.has_warning = None
+        self.ignore_result = False
+
+    def ignore_check(self):
+        self.has_error = False
+        self.has_warning = False
+        self.ignore_result = True
+        return self
 
     def fill_and_return_entry(self, details, has_error=False):
         return self.fill_and_return_entry(details, has_error, None)
@@ -375,8 +382,11 @@ class NodeChecker():
             return e.fill_and_return_entry([output], True)
 
         if output == 'File not found':
-            return e.fill_and_return_entry(
-                ["Certificate file {} not found".format(cert_path)], fail_if_not_found)
+            if fail_if_not_found:
+                return e.fill_and_return_entry(
+                    ["Certificate file {} not found".format(cert_path)], True)
+            else:
+                return e.ignore_check()
 
         try:
             ssl_date_fmt = r'%b %d %H:%M:%S %Y %Z'
@@ -470,7 +480,7 @@ class NodeChecker():
                                                  )
 
     def check_client_ca_certificate_expiration(self):
-        return self.check_certificate_expiration("Client",
+        return self.check_certificate_expiration("Client CA",
                                                  self.get_client_ca_certificate_path(),
                                                  False  # fail_if_not_found
                                                  )
@@ -987,8 +997,8 @@ class CheckCoordinator:
 
         entries = []
         for check in self.checks:
-            # TODO: we probably do not need to set a timeout, since SSH has one...
-            entries.append(check.result.get())
+            if not check.entry.ignore_result:
+                entries.append(check.entry)
         return entries
 
 
diff --git a/managed/devops/bin/node_client_utils.py b/managed/devops/bin/node_client_utils.py
index 8448b413a7..598976047b 100644
--- a/managed/devops/bin/node_client_utils.py
+++ b/managed/devops/bin/node_client_utils.py
@@ -1,8 +1,14 @@
 import os
 import paramiko
 import subprocess
+import time
 
 YB_USERNAME = 'yugabyte'
+# Let's set some timeout to our commands.
+# If 10 minutes will not be enough for something - will have to pass command timeout as an argument.
+# Just having timeout in shell script, which we're running on the node,
+# does not seem to always help - as ssh client connection itself or command results read can hang.
+COMMAND_TIMEOUT_SEC = 600
 
 
 class KubernetesClient:
@@ -40,7 +46,8 @@ class SshParamikoClient:
         self.client = paramiko.SSHClient()
         self.client.set_missing_host_key_policy(paramiko.MissingHostKeyPolicy())
         self.client.connect(self.ip, self.port, username=YB_USERNAME,
-                            key_filename=self.key_filename, timeout=10)
+                            key_filename=self.key_filename, timeout=10,
+                            banner_timeout=20, auth_timeout=20)
 
     def close_connection(self):
         self.client.close()
@@ -61,10 +68,24 @@ class SshParamikoClient:
             command = cmd
         else:
             command = ' '.join(cmd)
-        stdin, stdout, stderr = self.client.exec_command(command)
+        stdin, stdout, stderr = self.client.exec_command(command, timeout=COMMAND_TIMEOUT_SEC)
         return_code = stdout.channel.recv_exit_status()
         if return_code != 0:
-            error = stderr.read().decode()
-            raise RuntimeError('Command returned error code {}: {}'.format(command, error))
-        output = stdout.read().decode()
+            error = self.read_output(stderr)
+            raise RuntimeError('Command \'{}\' returned error code {}: {}'
+                               .format(command, return_code, error))
+        output = self.read_output(stdout)
         return output
+
+    # We saw this script hang. The only place which can hang in theory is ssh command execution
+    # and reading it's results.
+    # Applied one of described workaround from this issue:
+    # https://github.com/paramiko/paramiko/issues/109
+    def read_output(self, stream):
+        end_time = time.time() + COMMAND_TIMEOUT_SEC
+        while not stream.channel.eof_received:
+            time.sleep(1)
+            if time.time() > end_time:
+                stream.channel.close()
+            break
+        return stream.read().decode()
diff --git a/managed/devops/bin/yb_platform_backup.sh b/managed/devops/bin/yb_platform_backup.sh
index 2c752a06b9..813038db76 100755
--- a/managed/devops/bin/yb_platform_backup.sh
+++ b/managed/devops/bin/yb_platform_backup.sh
@@ -158,7 +158,7 @@ create_backup() {
   prometheus_host="${9}"
   k8s_namespace="${10}"
   k8s_pod="${11}"
-  exclude_releases_flag=""
+  include_releases_flag="**/releases/**"
 
   mkdir -p "${output_path}"
 
@@ -201,16 +201,13 @@ create_backup() {
   fi
 
   if [[ "$exclude_releases" = true ]]; then
-    exclude_releases_flag="--exclude release*"
+    include_releases_flag=""
   fi
 
-  exclude_dirs="--exclude postgres* --exclude devops --exclude yugaware/lib \
-  --exclude yugaware/logs --exclude yugaware/README.md --exclude yugaware/bin \
-  --exclude yugaware/conf --exclude backup_*.tgz --exclude helm --exclude prometheusv2"
-
   modify_service yb-platform stop
 
-  tar_name="${output_path}/backup_${now}.tgz"
+  tar_name="${output_path}/backup_${now}.tar"
+  tgz_name="${output_path}/backup_${now}.tgz"
   db_backup_path="${data_dir}/${PLATFORM_DUMP_FNAME}"
   trap 'delete_postgres_backup ${db_backup_path}' RETURN
   create_postgres_backup "${db_backup_path}" "${db_username}" "${db_host}" "${db_port}" "${verbose}"
@@ -228,13 +225,21 @@ create_backup() {
     run_sudo_cmd "rm -rf ${PROMETHEUS_DATA_DIR}/snapshots/${snapshot_dir}"
   fi
   echo "Creating platform backup package..."
+  cd ${data_dir}
   if [[ "${verbose}" = true ]]; then
-    tar ${exclude_releases_flag} ${exclude_dirs} -czvf "${tar_name}" -C "${data_dir}" .
+    find . \( -path "**/data/certs/**" -o -path "**/data/keys/**" -o -path "**/data/provision/**" \
+              -o -path "**/${PLATFORM_DUMP_FNAME}" -o -path "**/${PROMETHEUS_SNAPSHOT_DIR}/**" \
+              -o -path "${include_releases_flag}" \) -exec tar -rvf "${tar_name}" {} +
   else
-    tar ${exclude_releases_flag} ${exclude_dirs} -czf "${tar_name}" -C "${data_dir}" .
+    find . \( -path "**/data/certs/**" -o -path "**/data/keys/**" -o -path "**/data/provision/**" \
+              -o -path "**/${PLATFORM_DUMP_FNAME}" -o -path "**/${PROMETHEUS_SNAPSHOT_DIR}/**" \
+              -o -path "${include_releases_flag}" \) -exec tar -rf "${tar_name}" {} +
   fi
 
-  echo "Finished creating backup ${tar_name}"
+  gzip -9 < ${tar_name} > ${tgz_name}
+  cleanup "${tar_name}"
+
+  echo "Finished creating backup ${tgz_name}"
   modify_service yb-platform restart
 }
 
diff --git a/managed/devops/opscli/ybops/cloud/ybcloud.py b/managed/devops/opscli/ybops/cloud/ybcloud.py
index 4836dadaaf..ddebb1925c 100644
--- a/managed/devops/opscli/ybops/cloud/ybcloud.py
+++ b/managed/devops/opscli/ybops/cloud/ybcloud.py
@@ -11,6 +11,7 @@
 import argparse
 import json
 import logging
+import sys
 
 from ybops.cloud.aws.cloud import AwsCloud
 from ybops.cloud.gcp.cloud import GcpCloud
@@ -44,7 +45,32 @@ class YbCloud(AbstractCommandParser):
                                  default="INFO",
                                  choices=("INFO", "DEBUG", "WARNING", "ERROR"))
 
+    def exception_hook(self, except_type, except_value, tb):
+        """Handler for uncaught exception to dump well-formed error messages to stdout.
+        """
+        try:
+            cause_tb = tb
+            while cause_tb.tb_next:
+                cause_tb = cause_tb.tb_next
+            filename = cause_tb.tb_frame.f_code.co_filename
+            name = cause_tb.tb_frame.f_code.co_name
+            line_no = cause_tb.tb_lineno
+            output = {
+                "type": "{}.{}".format(except_type.__module__, except_type.__name__),
+                "message": str(except_value),
+                "file": filename,
+                "name": name,
+                "line": line_no
+            }
+            # Write to stdout with markers so that other messages do not interfere.
+            print("<yb-python-error>{}</yb-python-error>".format(json.dumps(output)))
+        except Exception as e:
+            logging.error("Error processing exception. Error: ".format(str(e)))
+        # Propagate the exception.
+        sys.__excepthook__(except_type, except_value, tb)
+
     def run(self):
+        sys.excepthook = self.exception_hook
         self.register(argparse.ArgumentParser())
         self.options = self.parser.parse_args()
 
diff --git a/managed/devops/replicated.yml b/managed/devops/replicated.yml
index 8b4ef5637e..fdc3f2d9b5 100644
--- a/managed/devops/replicated.yml
+++ b/managed/devops/replicated.yml
@@ -245,7 +245,6 @@ components:
                   params:
                     # Enable priority regex on all tables
                     # URL encoding done by prometheus
-                    max_tables_metrics_breakdowns: ["0"]
                     priority_regex:
                       - "rocksdb_(number_db_(next|seek)\
                       |block_cache_(add|single_touch_add|multi_touch_add)\
@@ -333,6 +332,7 @@ components:
             contents: |
               include classpath("application.common.conf")
               play.crypto.secret="{{repl ConfigOption "app_secret"}}"
+              play.http.forwarded.trustedProxies = ["172.17.0.1"]
               play.i18n.langs = [ "en" ]
               pidfile.path = "/dev/null"
 
@@ -431,10 +431,11 @@ components:
                   server_name  {{repl ConfigOption "hostname" }};
                 {{repl end}}
 
-                proxy_http_version 1.1;
-                proxy_set_header X-Real-IP  $remote_addr;
-                proxy_set_header X-Forwarded-For $remote_addr;
-                proxy_set_header Host $host;
+                  proxy_http_version 1.1;
+                  proxy_set_header X-Real-IP  $remote_addr;
+                  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
+                  proxy_set_header X-Forwarded-Proto $scheme;
+                  proxy_set_header Host $host;
 
                 location / {
                   proxy_pass http://{{repl HostPrivateIpAddress "app" "yugabyte/yugaware" }}:{{repl ContainerExposedPort "app" "yugabyte/yugaware" "9000" }};
diff --git a/managed/devops/roles/provision-cluster-server/tasks/main.yml b/managed/devops/roles/provision-cluster-server/tasks/main.yml
index 35aabd03db..1b74eeb2fa 100644
--- a/managed/devops/roles/provision-cluster-server/tasks/main.yml
+++ b/managed/devops/roles/provision-cluster-server/tasks/main.yml
@@ -106,6 +106,7 @@
     state: directory
     owner: "{{ user_name }}"
     group: "{{ user_name }}"
+    recurse: yes
   with_items: "{{ mount_points.split(',') }}"
 
 - include_tasks: "setup-cgroup.yml"
diff --git a/managed/devops/sudo_whitelist.txt b/managed/devops/sudo_whitelist.txt
new file mode 100644
index 0000000000..136cd07546
--- /dev/null
+++ b/managed/devops/sudo_whitelist.txt
@@ -0,0 +1,60 @@
+yugabyte ALL = (root) NOPASSWD: /bin/chown -R yugabyte?yugabyte /opt/yugabyte, \
+/bin/chown -R yugabyte?yugabyte /home/yugabyte, \
+/bin/chown * /home/yugabyte/bin/yb-server-ctl.sh, \
+/bin/mkdir -p /opt/yugabyte/certs, \
+/bin/rm -rf /opt/yugabyte, \
+/bin/sh -c * /usr/bin/python*, \
+/bin/sh -c * /usr/bin/env python*, \
+/bin/sh -c * /usr/bin/env python *, \
+/bin/sh -c * /usr/bin/python *, \
+/bin/sh -c /usr/bin/env python *, \
+/bin/sh -c * --version, \
+/bin/sh -c * which python2 2> /dev/null || which python3 2> /dev/null, \
+/bin/systemctl daemon-reload, \
+/bin/systemctl disable prometheus, \
+/bin/systemctl disable rh-nginx118-nginx, \
+/bin/systemctl disable yb-platform, \
+/bin/systemctl disble rh-postgresql10-postgresql, \
+/bin/systemctl enable prometheus, \
+/bin/systemctl enable rh-nginx118-nginx, \
+/bin/systemctl enable rh-postgresql10-postgresql, \
+/bin/systemctl enable yb-platform, \
+/bin/systemctl restart prometheus, \
+/bin/systemctl restart rh-nginx118-nginx, \
+/bin/systemctl restart rh-postgresql10-postgresql, \
+/bin/systemctl restart yb-platform, \
+/bin/systemctl start prometheus, \
+/bin/systemctl start rh-nginx118-nginx, \
+/bin/systemctl start rh-postgresql10-postgresql, \
+/bin/systemctl start yb-platform, \
+/bin/systemctl status prometheus, \
+/bin/systemctl status rh-nginx118-nginx, \
+/bin/systemctl status rh-postgresql10-postgresql, \
+/bin/systemctl status yb-platform, \
+/bin/systemctl stop prometheus, \
+/bin/systemctl stop rh-nginx118-nginx, \
+/bin/systemctl stop rh-postgresql10-postgresql, \
+/bin/systemctl stop yb-platform, \
+/sbin/service prometheus start, \
+/sbin/service prometheus status, \
+/sbin/service prometheus stop, \
+/sbin/service rh-nginx118-nginx start, \
+/sbin/service rh-nginx118-nginx status, \
+/sbin/service rh-nginx118-nginx stop, \
+/sbin/service rh-postgresql10-postgresql start, \
+/sbin/service rh-postgresql10-postgresql status, \
+/sbin/service rh-postgresql10-postgresql stop, \
+/sbin/service yb-platform start, \
+/sbin/service yb-platform status, \
+/sbin/service yb-platform stop, \
+/usr/bin/chown * /prometheus*, \
+/usr/bin/cp * /opt/yugabyte*, \
+/usr/bin/cp * /prometheus*, \
+/usr/bin/mv * /opt/yugabyte*, \
+/usr/bin/mv * /prometheus*, \
+/usr/bin/rm * /opt/yugabyte*, \
+/usr/bin/rm * /prometheus*, \
+/bin/sh -c * test -e /usr/local/*, \
+/usr/bin/cp * /usr/local/*, \
+/usr/bin/test -w *, \
+/usr/bin/df *
diff --git a/managed/devops/yb_release_manifest.json b/managed/devops/yb_release_manifest.json
index 195400c905..63863bbee6 100644
--- a/managed/devops/yb_release_manifest.json
+++ b/managed/devops/yb_release_manifest.json
@@ -12,7 +12,8 @@
     "yb-server-ctl.yml",
     "yb-server-provision.yml",
     "send_sudo_pass.yml",
-    "remote_role.yml"
+    "remote_role.yml",
+    "sudo_whitelist.txt"
   ],
   "bin": [
     "bin/cluster_health.py",
diff --git a/managed/src/main/java/AppInit.java b/managed/src/main/java/AppInit.java
index da40a9c4e6..15136ee1b5 100644
--- a/managed/src/main/java/AppInit.java
+++ b/managed/src/main/java/AppInit.java
@@ -2,9 +2,9 @@
 
 import com.google.inject.Inject;
 import com.google.inject.Singleton;
-import com.typesafe.config.Config;
 import com.yugabyte.yw.cloud.AWSInitializer;
 import com.yugabyte.yw.commissioner.CallHome;
+import com.yugabyte.yw.commissioner.HealthChecker;
 import com.yugabyte.yw.commissioner.SetUniverseKey;
 import com.yugabyte.yw.commissioner.TaskGarbageCollector;
 import com.yugabyte.yw.common.CertificateHelper;
@@ -18,7 +18,6 @@ import com.yugabyte.yw.common.alerts.AlertConfigurationWriter;
 import com.yugabyte.yw.common.alerts.AlertDestinationService;
 import com.yugabyte.yw.common.alerts.AlertsGarbageCollector;
 import com.yugabyte.yw.common.alerts.QueryAlerts;
-import com.yugabyte.yw.common.config.impl.SettableRuntimeConfigFactory;
 import com.yugabyte.yw.common.ha.PlatformReplicationManager;
 import com.yugabyte.yw.common.metrics.PlatformMetricsProcessor;
 import com.yugabyte.yw.models.Customer;
@@ -61,10 +60,10 @@ public class AppInit {
       PlatformMetricsProcessor platformMetricsProcessor,
       Scheduler scheduler,
       CallHome callHome,
-      SettableRuntimeConfigFactory sConfigFactory,
-      Config config)
+      HealthChecker healthChecker)
       throws ReflectiveOperationException {
     Logger.info("Yugaware Application has started");
+
     Configuration appConfig = application.configuration();
     String mode = appConfig.getString("yb.mode", "PLATFORM");
 
@@ -152,6 +151,7 @@ public class AppInit {
       scheduler.start();
       callHome.start();
       queryAlerts.start();
+      healthChecker.initialize();
 
       // Add checksums for all certificates that don't have a checksum.
       CertificateHelper.createChecksums();
diff --git a/managed/src/main/java/Module.java b/managed/src/main/java/Module.java
index 8b48bcd7fa..9ea019fdf7 100644
--- a/managed/src/main/java/Module.java
+++ b/managed/src/main/java/Module.java
@@ -5,8 +5,6 @@ import com.google.inject.Provides;
 import com.yugabyte.yw.cloud.AWSInitializer;
 import com.yugabyte.yw.cloud.aws.AWSCloudModule;
 import com.yugabyte.yw.commissioner.CallHome;
-import com.yugabyte.yw.common.GFlagsValidation;
-import com.yugabyte.yw.common.metrics.PlatformMetricsProcessor;
 import com.yugabyte.yw.commissioner.DefaultExecutorServiceProvider;
 import com.yugabyte.yw.commissioner.ExecutorServiceProvider;
 import com.yugabyte.yw.commissioner.HealthChecker;
@@ -18,11 +16,11 @@ import com.yugabyte.yw.common.AlertManager;
 import com.yugabyte.yw.common.ConfigHelper;
 import com.yugabyte.yw.common.CustomerTaskManager;
 import com.yugabyte.yw.common.ExtraMigrationManager;
+import com.yugabyte.yw.common.GFlagsValidation;
 import com.yugabyte.yw.common.HealthManager;
 import com.yugabyte.yw.common.KubernetesManager;
 import com.yugabyte.yw.common.NetworkManager;
 import com.yugabyte.yw.common.NodeManager;
-import com.yugabyte.yw.common.PlatformInstanceClientFactory;
 import com.yugabyte.yw.common.ReleaseManager;
 import com.yugabyte.yw.common.ShellProcessHandler;
 import com.yugabyte.yw.common.SwamperHelper;
@@ -35,6 +33,7 @@ import com.yugabyte.yw.common.alerts.AlertsGarbageCollector;
 import com.yugabyte.yw.common.alerts.QueryAlerts;
 import com.yugabyte.yw.common.config.RuntimeConfigFactory;
 import com.yugabyte.yw.common.config.impl.SettableRuntimeConfigFactory;
+import com.yugabyte.yw.common.ha.PlatformInstanceClientFactory;
 import com.yugabyte.yw.common.ha.PlatformReplicationHelper;
 import com.yugabyte.yw.common.ha.PlatformReplicationManager;
 import com.yugabyte.yw.common.kms.EncryptionAtRestManager;
@@ -47,9 +46,11 @@ import com.yugabyte.yw.controllers.PlatformHttpActionAdapter;
 import com.yugabyte.yw.metrics.MetricQueryHelper;
 import com.yugabyte.yw.queries.QueryHelper;
 import com.yugabyte.yw.scheduler.Scheduler;
+import javax.persistence.PersistenceException;
 import lombok.extern.slf4j.Slf4j;
 import org.pac4j.core.client.Clients;
 import org.pac4j.core.config.Config;
+import org.pac4j.core.http.url.DefaultUrlResolver;
 import org.pac4j.oidc.client.OidcClient;
 import org.pac4j.oidc.config.OidcConfiguration;
 import org.pac4j.oidc.profile.OidcProfile;
@@ -59,8 +60,6 @@ import org.pac4j.play.store.PlaySessionStore;
 import play.Configuration;
 import play.Environment;
 
-import javax.persistence.PersistenceException;
-
 /**
  * This class is a Guice module that tells Guice to bind different types
  *
@@ -136,7 +135,9 @@ public class Module extends AbstractModule {
       bind(TaskExecutor.class).asEagerSingleton();
 
       final CallbackController callbackController = new CallbackController();
-      callbackController.setDefaultUrl(config.getString("yb.url", ""));
+      // TODO(sbapat): Check whats the use case for setting default url because '/' is anyway
+      //  used when we do not call setDefaultUrl here.
+      callbackController.setDefaultUrl(config.getString("yb.security.oidcDefaultRedirectUrl"));
       bind(CallbackController.class).toInstance(callbackController);
     }
   }
@@ -147,39 +148,40 @@ public class Module extends AbstractModule {
     final OidcConfiguration oidcConfiguration = new OidcConfiguration();
 
     try {
-      if (runtimeConfigFactory.globalRuntimeConf().getString("yb.security.type").equals("OIDC")) {
-        oidcConfiguration.setClientId(
-            runtimeConfigFactory.globalRuntimeConf().getString("yb.security.clientID"));
-        oidcConfiguration.setSecret(
-            runtimeConfigFactory.globalRuntimeConf().getString("yb.security.secret"));
-        oidcConfiguration.setScope(
-            runtimeConfigFactory.globalRuntimeConf().getString("yb.security.oidcScope"));
-        oidcConfiguration.setDiscoveryURI(
-            runtimeConfigFactory.globalRuntimeConf().getString("yb.security.discoveryURI"));
-        oidcConfiguration.setMaxClockSkew(3600);
-        oidcConfiguration.setResponseType("code");
+      final com.typesafe.config.Config config = runtimeConfigFactory.globalRuntimeConf();
+      if (buildOidcClientFromAppConfig(oidcConfiguration, config)) {
         return new OidcClient<>(oidcConfiguration);
       }
     } catch (PersistenceException e) {
+      // TODO(sbapat): This should not be needed.
       log.debug("Defaulting to static configuration since runtime configuration is not available.");
-      if (config.getString("yb.security.type", "").equals("OIDC")) {
-        oidcConfiguration.setClientId(config.getString("yb.security.clientID", ""));
-        oidcConfiguration.setSecret(config.getString("yb.security.secret", ""));
-        oidcConfiguration.setScope(config.getString("yb.security.oidcScope", ""));
-        oidcConfiguration.setDiscoveryURI(config.getString("yb.security.discoveryURI", ""));
-        oidcConfiguration.setMaxClockSkew(3600);
-        oidcConfiguration.setResponseType("code");
+      com.typesafe.config.Config config1 = this.config.underlying();
+      if (buildOidcClientFromAppConfig(oidcConfiguration, config1)) {
         return new OidcClient<>(oidcConfiguration);
       }
     }
     return new OidcClient<>(oidcConfiguration);
   }
 
+  private boolean buildOidcClientFromAppConfig(
+      OidcConfiguration oidcConfiguration, com.typesafe.config.Config config) {
+    if (config.getString("yb.security.type").equals("OIDC")) {
+      oidcConfiguration.setClientId(config.getString("yb.security.clientID"));
+      oidcConfiguration.setSecret(config.getString("yb.security.secret"));
+      oidcConfiguration.setScope(config.getString("yb.security.oidcScope"));
+      oidcConfiguration.setDiscoveryURI(config.getString("yb.security.discoveryURI"));
+      oidcConfiguration.setMaxClockSkew(3600);
+      oidcConfiguration.setResponseType("code");
+      return true;
+    }
+    return false;
+  }
+
   @Provides
-  protected Config provideConfig(OidcClient<OidcProfile, OidcConfiguration> oidcClient) {
+  protected Config providePac4jConfig(OidcClient<OidcProfile, OidcConfiguration> oidcClient) {
     final Clients clients =
-        new Clients(
-            String.format("%s/api/v1/callback", config.getString("yb.url", "")), oidcClient);
+        new Clients(config.getString("yb.security.oidcCallbackUrl"), oidcClient);
+    clients.setUrlResolver(new DefaultUrlResolver(true));
     final Config config = new Config(clients);
     config.setHttpActionAdapter(new PlatformHttpActionAdapter());
     return config;
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/AbstractTaskBase.java b/managed/src/main/java/com/yugabyte/yw/commissioner/AbstractTaskBase.java
index 273917aa52..c4a84276fe 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/AbstractTaskBase.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/AbstractTaskBase.java
@@ -2,8 +2,6 @@
 
 package com.yugabyte.yw.commissioner;
 
-import static com.yugabyte.yw.common.ShellResponse.ERROR_CODE_EXECUTION_CANCELLED;
-import static com.yugabyte.yw.common.ShellResponse.ERROR_CODE_SUCCESS;
 import com.fasterxml.jackson.databind.JsonNode;
 import com.google.common.util.concurrent.MoreExecutors;
 import com.google.common.util.concurrent.ThreadFactoryBuilder;
@@ -27,7 +25,6 @@ import com.yugabyte.yw.models.helpers.NodeDetails;
 import com.yugabyte.yw.models.helpers.NodeStatus;
 import java.time.Duration;
 import java.util.UUID;
-import java.util.concurrent.CancellationException;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.ThreadFactory;
 import java.util.concurrent.TimeUnit;
@@ -130,16 +127,6 @@ public abstract class AbstractTaskBase implements ITask {
     this.userTaskUUID = userTaskUUID;
   }
 
-  /** @param response : ShellResponse object */
-  public void processShellResponse(ShellResponse response) {
-    if (response.code == ERROR_CODE_EXECUTION_CANCELLED) {
-      throw new CancellationException((response.message != null) ? response.message : "error");
-    }
-    if (response.code != ERROR_CODE_SUCCESS) {
-      throw new RuntimeException((response.message != null) ? response.message : "error");
-    }
-  }
-
   /**
    * We would try to parse the shell response message as JSON and return JsonNode
    *
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/CallHome.java b/managed/src/main/java/com/yugabyte/yw/commissioner/CallHome.java
index 244ecef2a0..506581224f 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/CallHome.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/CallHome.java
@@ -23,14 +23,14 @@ public class CallHome {
 
   private final ExecutionContext executionContext;
 
-  private CallHomeManager callHomeManager;
+  private final CallHomeManager callHomeManager;
 
   private final Environment environment;
 
   // Interval at which to send callhome diagnostics in minutes
-  private final int YB_CALLHOME_INTERVAL = 60;
+  private static final int YB_CALLHOME_INTERVAL = 60;
 
-  private AtomicBoolean running = new AtomicBoolean(false);
+  private final AtomicBoolean running = new AtomicBoolean(false);
 
   @Inject
   public CallHome(
@@ -56,7 +56,7 @@ public class CallHome {
         .schedule(
             Duration.create(0, TimeUnit.MINUTES), // initialDelay
             Duration.create(YB_CALLHOME_INTERVAL, TimeUnit.MINUTES), // interval
-            () -> scheduleRunner(),
+            this::scheduleRunner,
             this.executionContext);
   }
 
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/HealthChecker.java b/managed/src/main/java/com/yugabyte/yw/commissioner/HealthChecker.java
index 72eba3a6ee..3a1cf42a96 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/HealthChecker.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/HealthChecker.java
@@ -48,7 +48,6 @@ import com.yugabyte.yw.models.MetricSourceKey;
 import com.yugabyte.yw.models.Provider;
 import com.yugabyte.yw.models.Universe;
 import com.yugabyte.yw.models.filters.MetricFilter;
-import com.yugabyte.yw.models.helpers.KnownAlertLabels;
 import com.yugabyte.yw.models.helpers.NodeDetails;
 import com.yugabyte.yw.models.helpers.PlatformMetrics;
 import com.yugabyte.yw.models.helpers.TaskType;
@@ -190,11 +189,9 @@ public class HealthChecker {
     this.lifecycle = lifecycle;
     this.healthMetrics = healthMetrics;
     this.executor = executorService;
-
-    this.initialize();
   }
 
-  private void initialize() {
+  public void initialize() {
     log.info("Scheduling health checker every " + this.healthCheckIntervalMs() + " ms");
     this.actorSystem
         .scheduler()
@@ -242,7 +239,7 @@ public class HealthChecker {
    * @param sendMailAlways Force the email sending.
    * @param reportOnlyErrors Include only errors into the report.
    * @param onlyMetrics Don't send email, only metrics collection.
-   * @return
+   * @return true if success
    */
   private boolean processResults(
       Customer c,
@@ -259,8 +256,7 @@ public class HealthChecker {
       healthJSON = Util.convertStringToJson(response);
     } catch (Exception e) {
       log.warn("Failed to convert health check response to JSON " + e.getMessage());
-      setHealthCheckFailedMetric(
-          c, u, "Error converting health check response to JSON: " + e.getMessage());
+      setHealthCheckFailedMetric(c, u);
       return false;
     }
 
@@ -351,9 +347,8 @@ public class HealthChecker {
             buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_NODE_METRICS_STATUS, u));
       } catch (Exception e) {
         log.warn("Failed to convert health check response to prometheus metrics", e);
-        metricService.setStatusMetric(
-            buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_NODE_METRICS_STATUS, u),
-            "Error converting health check response to prometheus metrics: " + e.getMessage());
+        metricService.setFailureStatusMetric(
+            buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_NODE_METRICS_STATUS, u));
       }
 
       if (!onlyMetrics
@@ -394,9 +389,8 @@ public class HealthChecker {
         emailHelper.sendEmail(c, subject, emailDestinations, smtpData, contentMap);
       } catch (MessagingException e) {
         log.warn("Health check had the following errors during mailing: " + e.getMessage());
-        metricService.setStatusMetric(
-            buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_NOTIFICATION_STATUS, u),
-            "Error sending Health check email: " + e.getMessage());
+        metricService.setFailureStatusMetric(
+            buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_NOTIFICATION_STATUS, u));
         return false;
       }
     }
@@ -561,10 +555,7 @@ public class HealthChecker {
                     universeName);
               } catch (Exception e) {
                 log.error("Error running health check for universe: {}", universeName, e);
-                setHealthCheckFailedMetric(
-                    params.customer,
-                    params.universe,
-                    "Error running health check: " + e.getMessage());
+                setHealthCheckFailedMetric(params.customer, params.universe);
               }
             },
             this.executor);
@@ -604,8 +595,7 @@ public class HealthChecker {
     UniverseDefinitionTaskParams details = params.universe.getUniverseDetails();
     if (details == null) {
       log.warn("Skipping universe " + params.universe.name + " due to invalid details json...");
-      setHealthCheckFailedMetric(
-          params.customer, params.universe, "Health check skipped due to invalid details json.");
+      setHealthCheckFailedMetric(params.customer, params.universe);
       return;
     }
     if (details.universePaused) {
@@ -652,8 +642,7 @@ public class HealthChecker {
                 + " due to invalid provider "
                 + cluster.userIntent.provider);
         invalidUniverseData = true;
-        setHealthCheckFailedMetric(
-            params.customer, params.universe, "Health check skipped due to invalid provider data.");
+        setHealthCheckFailedMetric(params.customer, params.universe);
 
         break;
       }
@@ -670,8 +659,7 @@ public class HealthChecker {
         if (!providerCode.equals(CloudType.kubernetes.toString())) {
           log.warn("Skipping universe " + params.universe.name + " due to invalid access key...");
           invalidUniverseData = true;
-          setHealthCheckFailedMetric(
-              params.customer, params.universe, "Health check skipped due to invalid access key.");
+          setHealthCheckFailedMetric(params.customer, params.universe);
 
           break;
         }
@@ -726,12 +714,7 @@ public class HealthChecker {
         log.warn(
             String.format(
                 "Universe %s has unprovisioned node %s.", params.universe.name, nd.nodeName));
-        setHealthCheckFailedMetric(
-            params.customer,
-            params.universe,
-            String.format(
-                "Can't run health check for the universe due to unprovisioned node%s.",
-                nd.nodeName == null ? "" : " " + nd.nodeName));
+        setHealthCheckFailedMetric(params.customer, params.universe);
         break;
       }
 
@@ -742,10 +725,7 @@ public class HealthChecker {
             String.format(
                 "Universe %s has node %s with invalid placement %s",
                 params.universe.name, nd.nodeName, nd.placementUuid));
-        String alertText =
-            String.format(
-                "Universe has node %s with invalid placement %s.", nd.nodeName, nd.placementUuid);
-        setHealthCheckFailedMetric(params.customer, params.universe, alertText);
+        setHealthCheckFailedMetric(params.customer, params.universe);
 
         break;
       }
@@ -833,21 +813,15 @@ public class HealthChecker {
           response.message,
           response.code,
           durationMs);
-      String alertText =
-          String.format(
-              "Health check script got error: %s code (%d) [ %d ms ]",
-              response.message, response.code, durationMs);
-      setHealthCheckFailedMetric(params.customer, params.universe, alertText);
+      setHealthCheckFailedMetric(params.customer, params.universe);
     }
   }
 
-  private void setHealthCheckFailedMetric(Customer customer, Universe universe, String message) {
+  private void setHealthCheckFailedMetric(Customer customer, Universe universe) {
     // Remove old metrics and create only health check failed.
     MetricFilter toClean = metricSourceKeysFilter(customer, universe, HEALTH_CHECK_METRICS);
     Metric healthCheckFailed =
-        buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_STATUS, universe)
-            .setLabel(KnownAlertLabels.ERROR_MESSAGE, message)
-            .setValue(0.0);
+        buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_STATUS, universe).setValue(0.0);
     metricService.cleanAndSave(Collections.singletonList(healthCheckFailed), toClean);
   }
 
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/SetUniverseKey.java b/managed/src/main/java/com/yugabyte/yw/commissioner/SetUniverseKey.java
index 86adff21ed..c838c3a0c1 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/SetUniverseKey.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/SetUniverseKey.java
@@ -47,7 +47,7 @@ public class SetUniverseKey {
 
   private final YBClientService ybService;
 
-  private final int YB_SET_UNIVERSE_KEY_INTERVAL = 2;
+  private static final int YB_SET_UNIVERSE_KEY_INTERVAL = 2;
 
   @Inject
   public SetUniverseKey(
@@ -126,7 +126,7 @@ public class SetUniverseKey {
         byte[] keyRef = Base64.getDecoder().decode(activeKey.uuid.keyRef);
         byte[] keyVal = keyManager.getUniverseKey(u.universeUUID, activeKey.configUuid, keyRef);
         Arrays.stream(u.getMasterAddresses().split(","))
-            .map(addrString -> HostAndPort.fromString(addrString))
+            .map(HostAndPort::fromString)
             .forEach(addr -> setKeyInMaster(u, addr, keyRef, keyVal));
       }
     } catch (Exception e) {
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/TaskExecutor.java b/managed/src/main/java/com/yugabyte/yw/commissioner/TaskExecutor.java
index 8ae0d18a75..5ee4ede95d 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/TaskExecutor.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/TaskExecutor.java
@@ -754,6 +754,7 @@ public class TaskExecutor {
         updateTaskDetailsOnError(TaskInfo.State.Failure, e);
         Throwables.propagate(e);
       } finally {
+        log.debug("Completed task {}", task.getName());
         taskCompletionTime = Instant.now();
         writeTaskStateMetric(taskType, taskStartTime, taskCompletionTime, getTaskState());
         publishAfterTask(t);
@@ -1010,7 +1011,7 @@ public class TaskExecutor {
               log.error("Ignoring error for " + subTaskGroup.toString(), e);
             } else {
               // Postpone throwing this error later when all the subgroups are done.
-              throw new RuntimeException(subTaskGroup.toString() + " failed.");
+              throw new RuntimeException(subTaskGroup.toString() + " failed.", e);
             }
             anyRe = e;
           }
@@ -1020,7 +1021,7 @@ public class TaskExecutor {
         subTaskGroups.clear();
       }
       if (anyRe != null) {
-        throw new RuntimeException("One or more SubTaskGroups failed while running.");
+        throw new RuntimeException("One or more SubTaskGroups failed while running.", anyRe);
       }
     }
 
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/AddNodeToUniverse.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/AddNodeToUniverse.java
index e254481a8f..35564cd87e 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/AddNodeToUniverse.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/AddNodeToUniverse.java
@@ -10,6 +10,7 @@
 
 package com.yugabyte.yw.commissioner.tasks;
 
+import static com.google.api.client.util.Preconditions.checkState;
 import static com.yugabyte.yw.common.Util.areMastersUnderReplicated;
 
 import com.yugabyte.yw.commissioner.BaseTaskDependencies;
@@ -25,13 +26,11 @@ import com.yugabyte.yw.models.NodeInstance;
 import com.yugabyte.yw.models.Universe;
 import com.yugabyte.yw.models.helpers.NodeDetails;
 import com.yugabyte.yw.models.helpers.NodeDetails.NodeState;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Optional;
+import java.util.Set;
 import java.util.UUID;
 import javax.inject.Inject;
 import lombok.extern.slf4j.Slf4j;
@@ -54,11 +53,10 @@ public class AddNodeToUniverse extends UniverseDefinitionTaskBase {
   @Override
   public void run() {
     log.info(
-        "Started {} task for node {} in univ uuid={}",
+        "Started {} task for node {} in universe {}",
         getName(),
         taskParams().nodeName,
         taskParams().universeUUID);
-    NodeDetails currentNode = null;
     String errorString = null;
 
     try {
@@ -66,9 +64,10 @@ public class AddNodeToUniverse extends UniverseDefinitionTaskBase {
       // Update the DB to prevent other changes from happening.
       Universe universe = lockUniverseForUpdate(taskParams().expectedUniverseVersion);
 
-      currentNode = universe.getNode(taskParams().nodeName);
+      final NodeDetails currentNode = universe.getNode(taskParams().nodeName);
       if (currentNode == null) {
-        String msg = "No node " + taskParams().nodeName + " in universe " + universe.name;
+        String msg =
+            String.format("No node %s in universe %s", taskParams().nodeName, universe.name);
         log.error(msg);
         throw new RuntimeException(msg);
       }
@@ -79,142 +78,156 @@ public class AddNodeToUniverse extends UniverseDefinitionTaskBase {
 
       Cluster cluster = taskParams().getClusterByUuid(currentNode.placementUuid);
       UserIntent userIntent = cluster.userIntent;
-      Collection<NodeDetails> node = Collections.singletonList(currentNode);
-
       boolean wasDecommissioned = currentNode.state == NodeState.Decommissioned;
+
       // For onprem universes, allocate an available node
       // from the provider's node_instance table.
-      if (wasDecommissioned && cluster.userIntent.providerType.equals(CloudType.onprem)) {
-        Map<UUID, List<String>> onpremAzToNodes = new HashMap<UUID, List<String>>();
-        List<String> nodeNameList = new ArrayList<>();
-        nodeNameList.add(currentNode.nodeName);
-        onpremAzToNodes.put(currentNode.azUuid, nodeNameList);
-        String instanceType = currentNode.cloudInfo.instance_type;
-
-        Map<String, NodeInstance> nodeMap = NodeInstance.pickNodes(onpremAzToNodes, instanceType);
-        currentNode.nodeUuid = nodeMap.get(currentNode.nodeName).getNodeUuid();
+      if (wasDecommissioned && userIntent.providerType.equals(CloudType.onprem)) {
+        Optional<NodeInstance> nodeInstance = NodeInstance.maybeGetByName(currentNode.nodeName);
+        if (nodeInstance.isPresent()) {
+          // Illegal state if it is unused because both node name and in-use fields are updated
+          // together.
+          checkState(nodeInstance.get().isInUse(), "Node name is set but the node is not in use");
+        } else {
+          // Reserve a node if it is not assigned yet, and persist the universe details and node
+          // reservation in transaction so that universe is aware of the reservation.
+          Map<UUID, List<String>> onpremAzToNodes =
+              Collections.singletonMap(
+                  currentNode.azUuid, Collections.singletonList(currentNode.nodeName));
+          universe =
+              saveUniverseDetails(
+                  u -> {
+                    NodeDetails node = u.getNode(taskParams().nodeName);
+                    Map<String, NodeInstance> nodeMap =
+                        NodeInstance.pickNodes(
+                            onpremAzToNodes, currentNode.cloudInfo.instance_type);
+                    node.nodeUuid = nodeMap.get(currentNode.nodeName).getNodeUuid();
+                    currentNode.nodeUuid = node.nodeUuid;
+                    // This needs to be set because DB fetch of this universe later can override the
+                    // field as the universe details object is transient and not tracked by DB.
+                    u.setUniverseDetails(u.getUniverseDetails());
+                    // Perform preflight check. If it fails, the node must not be in use,
+                    // otherwise running it second time can succeed. This check must be
+                    // performed only when a new node is picked as Add after Remove can
+                    // leave processes that require sudo access.
+                    String preflightStatus =
+                        performPreflightCheck(
+                            cluster,
+                            currentNode,
+                            CertificateHelper.isRootCARequired(taskParams())
+                                ? taskParams().rootCA
+                                : null,
+                            CertificateHelper.isClientRootCARequired(taskParams())
+                                ? taskParams().clientRootCA
+                                : null);
+                    if (preflightStatus != null) {
+                      throw new RuntimeException(
+                          String.format(
+                              "Node %s (%s) failed preflight check. Error: %s",
+                              node.getNodeName(), node.getNodeUuid(), preflightStatus));
+                    }
+                  });
+        }
       }
+      Set<NodeDetails> nodeSet = Collections.singleton(currentNode);
+      // Update Node State to being added.
+      createSetNodeStateTask(currentNode, NodeState.Adding)
+          .setSubTaskGroupType(SubTaskGroupType.StartingNode);
 
-      String preflightStatus = null;
-      // Perform preflight check for onprem cluster
-      if (cluster.userIntent.providerType == CloudType.onprem) {
-        preflightStatus =
-            performPreflightCheck(
-                cluster,
-                currentNode,
-                CertificateHelper.isRootCARequired(taskParams()) ? taskParams().rootCA : null,
-                CertificateHelper.isClientRootCARequired(taskParams())
-                    ? taskParams().clientRootCA
-                    : null);
-      }
+      // First spawn an instance for Decommissioned node.
+      if (wasDecommissioned) {
+        createCreateServerTasks(nodeSet).setSubTaskGroupType(SubTaskGroupType.Provisioning);
 
-      if (preflightStatus != null) {
-        Map<String, String> failedNodes =
-            Collections.singletonMap(currentNode.nodeName, preflightStatus);
-        createFailedPrecheckTask(failedNodes, true)
-            .setSubTaskGroupType(SubTaskGroupType.PreflightChecks);
-        errorString = "Preflight checks failed.";
-      } else {
-        // Update Node State to being added.
-        createSetNodeStateTask(currentNode, NodeState.Adding)
-            .setSubTaskGroupType(SubTaskGroupType.StartingNode);
+        createServerInfoTasks(nodeSet).setSubTaskGroupType(SubTaskGroupType.Provisioning);
 
-        // First spawn an instance for Decommissioned node.
-        if (wasDecommissioned) {
-          createCreateServerTasks(node).setSubTaskGroupType(SubTaskGroupType.Provisioning);
+        createSetupServerTasks(nodeSet).setSubTaskGroupType(SubTaskGroupType.Provisioning);
+      }
 
-          createServerInfoTasks(node).setSubTaskGroupType(SubTaskGroupType.Provisioning);
+      // Re-install software.
+      // TODO: Remove the need for version for existing instance, NodeManger needs changes.
+      createConfigureServerTasks(nodeSet, true /* isShell */)
+          .setSubTaskGroupType(SubTaskGroupType.InstallingSoftware);
 
-          createSetupServerTasks(node).setSubTaskGroupType(SubTaskGroupType.Provisioning);
-        }
+      // All necessary nodes are created. Data moving will coming soon.
+      createSetNodeStateTasks(nodeSet, NodeDetails.NodeState.ToJoinCluster)
+          .setSubTaskGroupType(SubTaskGroupType.Provisioning);
 
-        // Re-install software.
-        // TODO: Remove the need for version for existing instance, NodeManger needs changes.
-        createConfigureServerTasks(node, true /* isShell */)
-            .setSubTaskGroupType(SubTaskGroupType.InstallingSoftware);
+      // Bring up any masters, as needed.
+      boolean masterAdded = false;
+      if (areMastersUnderReplicated(currentNode, universe)) {
+        log.info(
+            "Bringing up master for under replicated universe {} ({})",
+            universe.universeUUID,
+            universe.name);
 
-        // All necessary nodes are created. Data moving will coming soon.
-        createSetNodeStateTasks(node, NodeDetails.NodeState.ToJoinCluster)
-            .setSubTaskGroupType(SubTaskGroupType.Provisioning);
+        // Set gflags for master.
+        createGFlagsOverrideTasks(nodeSet, ServerType.MASTER, true /* isShell */);
 
-        // Bring up any masters, as needed.
-        boolean masterAdded = false;
-        if (areMastersUnderReplicated(currentNode, universe)) {
-          log.info(
-              "Bringing up master for under replicated universe {} ({})",
-              universe.universeUUID,
-              universe.name);
+        // Start a shell master process.
+        createStartMasterTasks(nodeSet).setSubTaskGroupType(SubTaskGroupType.StartingNodeProcesses);
 
-          // Set gflags for master.
-          createGFlagsOverrideTasks(node, ServerType.MASTER, true /* isShell */);
+        // Mark node as a master in YW DB.
+        // Do this last so that master addresses does not pick up current node.
+        createUpdateNodeProcessTask(taskParams().nodeName, ServerType.MASTER, true)
+            .setSubTaskGroupType(SubTaskGroupType.StartingNodeProcesses);
 
-          // Start a shell master process.
-          createStartMasterTasks(node).setSubTaskGroupType(SubTaskGroupType.StartingNodeProcesses);
+        // Wait for master to be responsive.
+        createWaitForServersTasks(nodeSet, ServerType.MASTER)
+            .setSubTaskGroupType(SubTaskGroupType.ConfigureUniverse);
 
-          // Mark node as a master in YW DB.
-          // Do this last so that master addresses does not pick up current node.
-          createUpdateNodeProcessTask(taskParams().nodeName, ServerType.MASTER, true)
-              .setSubTaskGroupType(SubTaskGroupType.StartingNodeProcesses);
+        // Add it into the master quorum.
+        createChangeConfigTask(currentNode, true, SubTaskGroupType.WaitForDataMigration);
 
-          // Wait for master to be responsive.
-          createWaitForServersTasks(node, ServerType.MASTER)
-              .setSubTaskGroupType(SubTaskGroupType.ConfigureUniverse);
+        masterAdded = true;
+      }
 
-          // Add it into the master quorum.
-          createChangeConfigTask(currentNode, true, SubTaskGroupType.WaitForDataMigration);
+      // Set gflags for the tserver.
+      createGFlagsOverrideTasks(nodeSet, ServerType.TSERVER);
 
-          masterAdded = true;
-        }
+      // Add the tserver process start task.
+      createTServerTaskForNode(currentNode, "start")
+          .setSubTaskGroupType(SubTaskGroupType.StartingNodeProcesses);
 
-        // Set gflags for the tserver.
-        createGFlagsOverrideTasks(node, ServerType.TSERVER);
+      // Mark the node as tserver in the YW DB.
+      createUpdateNodeProcessTask(taskParams().nodeName, ServerType.TSERVER, true)
+          .setSubTaskGroupType(SubTaskGroupType.StartingNodeProcesses);
 
-        // Add the tserver process start task.
-        createTServerTaskForNode(currentNode, "start")
-            .setSubTaskGroupType(SubTaskGroupType.StartingNodeProcesses);
+      // Wait for new tablet servers to be responsive.
+      createWaitForServersTasks(nodeSet, ServerType.TSERVER)
+          .setSubTaskGroupType(SubTaskGroupType.ConfigureUniverse);
 
-        // Mark the node as tserver in the YW DB.
-        createUpdateNodeProcessTask(taskParams().nodeName, ServerType.TSERVER, true)
-            .setSubTaskGroupType(SubTaskGroupType.StartingNodeProcesses);
+      // Update the swamper target file.
+      createSwamperTargetUpdateTask(false /* removeFile */);
 
-        // Wait for new tablet servers to be responsive.
-        createWaitForServersTasks(node, ServerType.TSERVER)
+      // Clear the host from master's blacklist.
+      if (currentNode.state == NodeState.Removed) {
+        createModifyBlackListTask(nodeSet, false /* isAdd */, false /* isLeaderBlacklist */)
             .setSubTaskGroupType(SubTaskGroupType.ConfigureUniverse);
+      }
 
-        // Update the swamper target file.
-        createSwamperTargetUpdateTask(false /* removeFile */);
-
-        // Clear the host from master's blacklist.
-        if (currentNode.state == NodeState.Removed) {
-          createModifyBlackListTask(
-                  Arrays.asList(currentNode), false /* isAdd */, false /* isLeaderBlacklist */)
-              .setSubTaskGroupType(SubTaskGroupType.ConfigureUniverse);
-        }
-
-        // Wait for load to balance.
-        createWaitForLoadBalanceTask().setSubTaskGroupType(SubTaskGroupType.WaitForDataMigration);
+      // Wait for load to balance.
+      createWaitForLoadBalanceTask().setSubTaskGroupType(SubTaskGroupType.WaitForDataMigration);
 
+      if (masterAdded) {
         // Update all tserver conf files with new master information.
-        if (masterAdded) {
-          createMasterInfoUpdateTask(universe, currentNode);
-        }
-
-        // Update node state to live.
-        createSetNodeStateTask(currentNode, NodeState.Live)
-            .setSubTaskGroupType(SubTaskGroupType.StartingNode);
+        createMasterInfoUpdateTask(universe, currentNode);
+      }
 
-        // Update the DNS entry for this universe.
-        createDnsManipulationTask(DnsManager.DnsCommandType.Edit, false, userIntent)
-            .setSubTaskGroupType(SubTaskGroupType.StartingNode);
+      // Update node state to live.
+      createSetNodeStateTask(currentNode, NodeState.Live)
+          .setSubTaskGroupType(SubTaskGroupType.StartingNode);
 
-        // Mark universe task state to success.
-        createMarkUniverseUpdateSuccessTasks().setSubTaskGroupType(SubTaskGroupType.StartingNode);
-      }
+      // Update the DNS entry for this universe.
+      createDnsManipulationTask(DnsManager.DnsCommandType.Edit, false, userIntent)
+          .setSubTaskGroupType(SubTaskGroupType.StartingNode);
 
+      // Mark universe task state to success.
+      createMarkUniverseUpdateSuccessTasks().setSubTaskGroupType(SubTaskGroupType.StartingNode);
       // Run all the tasks.
       getRunnableTask().runSubTasks();
     } catch (Throwable t) {
       log.error("Error executing task {} with error='{}'.", getName(), t.getMessage(), t);
+      errorString = t.getMessage();
       throw t;
     } finally {
       // Mark the update of the universe as done. This will allow future updates to the universe.
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/BackupUniverse.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/BackupUniverse.java
index a24ef996ce..d9d9f3b165 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/BackupUniverse.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/BackupUniverse.java
@@ -183,8 +183,8 @@ public class BackupUniverse extends UniverseTaskBase {
         log.error("Error executing task {} with error='{}'.", getName(), t.getMessage(), t);
         if (taskParams().actionType == ActionType.CREATE) {
           BACKUP_FAILURE_COUNTER.labels(metricLabelsBuilder.getPrometheusValues()).inc();
-          metricService.setStatusMetric(
-              buildMetricTemplate(PlatformMetrics.CREATE_BACKUP_STATUS, universe), t.getMessage());
+          metricService.setFailureStatusMetric(
+              buildMetricTemplate(PlatformMetrics.CREATE_BACKUP_STATUS, universe));
         }
       } finally {
         // Run an unlock in case the task failed before getting to the unlock. It is okay if it
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/CreateBackup.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/CreateBackup.java
index f1cc006fb8..2afd7f8d4d 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/CreateBackup.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/CreateBackup.java
@@ -268,8 +268,8 @@ public class CreateBackup extends UniverseTaskBase {
       try {
         log.error("Error executing task {} with error='{}'.", getName(), t.getMessage(), t);
         BACKUP_FAILURE_COUNTER.labels(metricLabelsBuilder.getPrometheusValues()).inc();
-        metricService.setStatusMetric(
-            buildMetricTemplate(PlatformMetrics.CREATE_BACKUP_STATUS, universe), t.getMessage());
+        metricService.setFailureStatusMetric(
+            buildMetricTemplate(PlatformMetrics.CREATE_BACKUP_STATUS, universe));
       } finally {
         // Run an unlock in case the task failed before getting to the unlock. It is okay if it
         // errors out.
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/MultiTableBackup.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/MultiTableBackup.java
index 5621947615..65944e0a2a 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/MultiTableBackup.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/MultiTableBackup.java
@@ -319,8 +319,8 @@ public class MultiTableBackup extends UniverseTaskBase {
 
       if (params().actionType == ActionType.CREATE) {
         BACKUP_FAILURE_COUNTER.labels(metricLabelsBuilder.getPrometheusValues()).inc();
-        metricService.setStatusMetric(
-            buildMetricTemplate(PlatformMetrics.CREATE_BACKUP_STATUS, universe), t.getMessage());
+        metricService.setFailureStatusMetric(
+            buildMetricTemplate(PlatformMetrics.CREATE_BACKUP_STATUS, universe));
       }
       // Run an unlock in case the task failed before getting to the unlock. It is okay if it
       // errors out.
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/PauseUniverse.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/PauseUniverse.java
index 9cd17df102..65c72b57e6 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/PauseUniverse.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/PauseUniverse.java
@@ -104,6 +104,7 @@ public class PauseUniverse extends UniverseTaskBase {
             u.setUniverseDetails(universeDetails);
           });
 
+      metricService.markSourceInactive(params().customerUUID, params().universeUUID);
     } catch (Throwable t) {
       log.error("Error executing task {} with error='{}'.", getName(), t.getMessage(), t);
       throw t;
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/ReleaseInstanceFromUniverse.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/ReleaseInstanceFromUniverse.java
index 97d72baf3c..064defa098 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/ReleaseInstanceFromUniverse.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/ReleaseInstanceFromUniverse.java
@@ -16,16 +16,15 @@ import com.yugabyte.yw.commissioner.UserTaskDetails.SubTaskGroupType;
 import com.yugabyte.yw.commissioner.tasks.params.NodeTaskParams;
 import com.yugabyte.yw.common.DnsManager;
 import com.yugabyte.yw.common.NodeActionType;
+import com.yugabyte.yw.common.Util;
 import com.yugabyte.yw.forms.UniverseDefinitionTaskParams.UserIntent;
 import com.yugabyte.yw.models.Universe;
 import com.yugabyte.yw.models.helpers.NodeDetails;
 import com.yugabyte.yw.models.helpers.NodeDetails.NodeState;
-import java.util.Arrays;
 import java.util.Collection;
-import java.util.HashSet;
+import java.util.Collections;
 import javax.inject.Inject;
 import lombok.extern.slf4j.Slf4j;
-import org.apache.commons.lang3.StringUtils;
 
 // Allows the removal of the instance from a universe. That node is already not part of the
 // universe and is in Removed state.
@@ -74,20 +73,23 @@ public class ReleaseInstanceFromUniverse extends UniverseTaskBase {
 
       taskParams().azUuid = currentNode.azUuid;
       taskParams().placementUuid = currentNode.placementUuid;
-
+      taskParams().nodeUuid = currentNode.nodeUuid;
+      Collection<NodeDetails> currentNodeDetails = Collections.singleton(currentNode);
       // Wait for Master Leader before doing Master operations, like blacklisting.
       createWaitForMasterLeaderTask().setSubTaskGroupType(SubTaskGroupType.ReleasingInstance);
-      // Create a task for removal of this server from blacklist on master leader.
-      createModifyBlackListTask(
-              Arrays.asList(currentNode), false /* isAdd */, false /* isLeaderBlacklist */)
-          .setSubTaskGroupType(SubTaskGroupType.ReleasingInstance);
-
+      // If the node fails in Adding state during ADD action, IP may not be available.
+      // Check to make sure that the node IP is available.
+      if (Util.getNodeIp(universe, currentNode) != null) {
+        // Create a task for removal of this server from blacklist on master leader.
+        createModifyBlackListTask(
+                currentNodeDetails, false /* isAdd */, false /* isLeaderBlacklist */)
+            .setSubTaskGroupType(SubTaskGroupType.ReleasingInstance);
+      }
       UserIntent userIntent =
           universe.getUniverseDetails().getClusterByUuid(currentNode.placementUuid).userIntent;
-      boolean isOnprem = userIntent.providerType.equals(CloudType.onprem);
-      if (instanceExists(taskParams()) || isOnprem) {
-        Collection<NodeDetails> currentNodeDetails = new HashSet<>(Arrays.asList(currentNode));
-        if (isOnprem) {
+      // Method instanceExists also checks for on-prem.
+      if (instanceExists(taskParams())) {
+        if (userIntent.providerType == CloudType.onprem) {
           // Stop master and tservers.
           createStopServerTasks(currentNodeDetails, "master", true /* isForceDelete */)
               .setSubTaskGroupType(SubTaskGroupType.StoppingNodeProcesses);
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/RemoveNodeFromUniverse.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/RemoveNodeFromUniverse.java
index 98915eefd5..c168fa713a 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/RemoveNodeFromUniverse.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/RemoveNodeFromUniverse.java
@@ -83,15 +83,7 @@ public class RemoveNodeFromUniverse extends UniverseTaskBase {
       createSetNodeStateTask(currentNode, NodeState.Removing)
           .setSubTaskGroupType(SubTaskGroupType.RemovingNode);
 
-      boolean instanceAlive = false;
-      try {
-        instanceAlive = instanceExists(taskParams());
-      } catch (Exception e) {
-        log.info(
-            "Instance {} in universe {} not found, assuming dead",
-            taskParams().nodeName,
-            universe.name);
-      }
+      boolean instanceAlive = instanceExists(taskParams());
 
       if (instanceAlive) {
         // Remove the master on this node from master quorum and update its state from YW DB,
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/ResumeUniverse.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/ResumeUniverse.java
index 2b91101e61..6586b761a6 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/ResumeUniverse.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/ResumeUniverse.java
@@ -86,6 +86,7 @@ public class ResumeUniverse extends UniverseTaskBase {
             u.setUniverseDetails(universeDetails);
           });
 
+      metricService.markSourceActive(params().customerUUID, params().universeUUID);
     } catch (Throwable t) {
       log.error("Error executing task {} with error='{}'.", getName(), t.getMessage(), t);
       throw t;
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/UniverseTaskBase.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/UniverseTaskBase.java
index 46f4c1fc9d..5ae686677a 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/UniverseTaskBase.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/UniverseTaskBase.java
@@ -1053,7 +1053,8 @@ public abstract class UniverseTaskBase extends AbstractTaskBase {
 
   /** Create a task to create write/read test table wor write/read metric and alert. */
   public SubTaskGroup createReadWriteTestTableTask(int numPartitions, boolean ifNotExist) {
-    SubTaskGroup subTaskGroup = getTaskExecutor().createSubTaskGroup("CreateReadWriteTestTable");
+    SubTaskGroup subTaskGroup =
+        getTaskExecutor().createSubTaskGroup("CreateReadWriteTestTable", executor);
 
     CreateTable task = createTask(CreateTable.class);
 
@@ -1651,7 +1652,7 @@ public abstract class UniverseTaskBase extends AbstractTaskBase {
       Collection<NodeDetails> addNodes,
       Collection<NodeDetails> removeNodes,
       boolean isLeaderBlacklist) {
-    SubTaskGroup subTaskGroup = getTaskExecutor().createSubTaskGroup("ModifyBlackList");
+    SubTaskGroup subTaskGroup = getTaskExecutor().createSubTaskGroup("ModifyBlackList", executor);
     ModifyBlackList.Params params = new ModifyBlackList.Params();
     params.universeUUID = taskParams().universeUUID;
     params.addNodes = addNodes;
@@ -1713,9 +1714,9 @@ public abstract class UniverseTaskBase extends AbstractTaskBase {
   public Optional<Boolean> instanceExists(
       NodeTaskParams taskParams, Map<String, String> expectedTags) {
     NodeManager nodeManager = Play.current().injector().instanceOf(NodeManager.class);
-    ShellResponse response = nodeManager.nodeCommand(NodeManager.NodeCommandType.List, taskParams);
-    processShellResponse(response);
-    if (response == null || Strings.isNullOrEmpty(response.message)) {
+    ShellResponse response =
+        nodeManager.nodeCommand(NodeManager.NodeCommandType.List, taskParams).processErrors();
+    if (Strings.isNullOrEmpty(response.message)) {
       // Instance does not exist.
       return Optional.empty();
     }
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/UpdateDiskSize.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/UpdateDiskSize.java
index e57171f761..025a24bdc5 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/UpdateDiskSize.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/UpdateDiskSize.java
@@ -43,11 +43,13 @@ public class UpdateDiskSize extends UniverseDefinitionTaskBase {
 
       // Update the universe DB with the update to be performed and set the 'updateInProgress' flag
       // to prevent other updates from happening.
-      Universe universe = lockUniverseForUpdate(taskParams().expectedUniverseVersion);
-
-      // Update the user intent.
-      universe = writeUserIntentToUniverse();
-      updateOnPremNodeUuids(universe);
+      Universe universe =
+          lockUniverseForUpdate(
+              taskParams().expectedUniverseVersion,
+              u -> {
+                // Set the task param data to universe in-memory.
+                setUserIntentToUniverse(u, taskParams(), false);
+              });
 
       Cluster primaryCluster = universe.getUniverseDetails().getPrimaryCluster();
 
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleClusterServerCtl.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleClusterServerCtl.java
index 281c969dd5..75ae66e21d 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleClusterServerCtl.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleClusterServerCtl.java
@@ -13,7 +13,6 @@ package com.yugabyte.yw.commissioner.tasks.subtasks;
 import com.yugabyte.yw.commissioner.BaseTaskDependencies;
 import com.yugabyte.yw.commissioner.tasks.params.NodeTaskParams;
 import com.yugabyte.yw.common.NodeManager;
-import com.yugabyte.yw.common.ShellResponse;
 import com.yugabyte.yw.models.Universe;
 import java.time.Duration;
 import javax.inject.Inject;
@@ -62,9 +61,9 @@ public class AnsibleClusterServerCtl extends NodeTaskBase {
       Universe universe = Universe.getOrBadRequest(taskParams().universeUUID);
       taskParams().useSystemd =
           universe.getUniverseDetails().getPrimaryCluster().userIntent.useSystemd;
-      ShellResponse response =
-          getNodeManager().nodeCommand(NodeManager.NodeCommandType.Control, taskParams());
-      processShellResponse(response);
+      getNodeManager()
+          .nodeCommand(NodeManager.NodeCommandType.Control, taskParams())
+          .processErrors();
     } catch (Exception e) {
       if (!taskParams().isForceDelete) {
         throw e;
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleConfigureServers.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleConfigureServers.java
index d1dcda1226..d229aaab23 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleConfigureServers.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleConfigureServers.java
@@ -94,8 +94,9 @@ public class AnsibleConfigureServers extends NodeTaskBase {
         universe_temp.getUniverseDetails().getPrimaryCluster().userIntent.useSystemd;
     // Execute the ansible command.
     ShellResponse response =
-        getNodeManager().nodeCommand(NodeManager.NodeCommandType.Configure, taskParams());
-    processShellResponse(response);
+        getNodeManager()
+            .nodeCommand(NodeManager.NodeCommandType.Configure, taskParams())
+            .processErrors();
 
     if (taskParams().type == UpgradeTaskParams.UpgradeTaskType.Everything
         && !taskParams().updateMasterAddrsOnly) {
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleCreateServer.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleCreateServer.java
index faad903dc5..a90a3f0dd2 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleCreateServer.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleCreateServer.java
@@ -14,7 +14,6 @@ import com.yugabyte.yw.commissioner.BaseTaskDependencies;
 import com.yugabyte.yw.commissioner.Common;
 import com.yugabyte.yw.commissioner.tasks.params.NodeTaskParams;
 import com.yugabyte.yw.common.NodeManager;
-import com.yugabyte.yw.common.ShellResponse;
 import com.yugabyte.yw.models.AccessKey;
 import com.yugabyte.yw.models.Provider;
 import com.yugabyte.yw.models.helpers.NodeDetails.NodeState;
@@ -75,9 +74,9 @@ public class AnsibleCreateServer extends NodeTaskBase {
       log.info("Skipping creation of already existing instance {}", taskParams().nodeName);
     } else {
       //   Execute the ansible command.
-      ShellResponse response =
-          getNodeManager().nodeCommand(NodeManager.NodeCommandType.Create, taskParams());
-      processShellResponse(response);
+      getNodeManager()
+          .nodeCommand(NodeManager.NodeCommandType.Create, taskParams())
+          .processErrors();
       setNodeStatus(NodeStatus.builder().nodeState(NodeState.InstanceCreated).build());
     }
   }
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleDestroyServer.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleDestroyServer.java
index 0abcb4bfcf..3cee120f09 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleDestroyServer.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleDestroyServer.java
@@ -14,7 +14,6 @@ import com.yugabyte.yw.commissioner.BaseTaskDependencies;
 import com.yugabyte.yw.commissioner.Common;
 import com.yugabyte.yw.commissioner.tasks.params.NodeTaskParams;
 import com.yugabyte.yw.common.NodeManager;
-import com.yugabyte.yw.common.ShellResponse;
 import com.yugabyte.yw.forms.UniverseDefinitionTaskParams;
 import com.yugabyte.yw.forms.UniverseDefinitionTaskParams.UserIntent;
 import com.yugabyte.yw.models.NodeInstance;
@@ -73,9 +72,9 @@ public class AnsibleDestroyServer extends NodeTaskBase {
   public void run() {
     // Execute the ansible command.
     try {
-      ShellResponse response =
-          getNodeManager().nodeCommand(NodeManager.NodeCommandType.Destroy, taskParams());
-      processShellResponse(response);
+      getNodeManager()
+          .nodeCommand(NodeManager.NodeCommandType.Destroy, taskParams())
+          .processErrors();
     } catch (Exception e) {
       if (!taskParams().isForceDelete) {
         throw e;
@@ -96,10 +95,9 @@ public class AnsibleDestroyServer extends NodeTaskBase {
     if (taskParams().deleteRootVolumes
         && !userIntent.providerType.equals(Common.CloudType.onprem)) {
       try {
-        ShellResponse response =
-            getNodeManager()
-                .nodeCommand(NodeManager.NodeCommandType.Delete_Root_Volumes, taskParams());
-        processShellResponse(response);
+        getNodeManager()
+            .nodeCommand(NodeManager.NodeCommandType.Delete_Root_Volumes, taskParams())
+            .processErrors();
       } catch (Exception e) {
         if (!taskParams().isForceDelete) {
           throw e;
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleSetupServer.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleSetupServer.java
index 1787cee179..38d0db5028 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleSetupServer.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleSetupServer.java
@@ -74,9 +74,9 @@ public class AnsibleSetupServer extends NodeTaskBase {
       log.info("Skipping ansible provision.");
     } else {
       // Execute the ansible command.
-      ShellResponse response =
-          getNodeManager().nodeCommand(NodeManager.NodeCommandType.Provision, taskParams());
-      processShellResponse(response);
+      getNodeManager()
+          .nodeCommand(NodeManager.NodeCommandType.Provision, taskParams())
+          .processErrors();
       setNodeStatus(NodeStatus.builder().nodeState(NodeState.ServerSetup).build());
     }
   }
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleUpdateNodeInfo.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleUpdateNodeInfo.java
index 6d6d5d463a..e47d14fc3a 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleUpdateNodeInfo.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/AnsibleUpdateNodeInfo.java
@@ -41,8 +41,9 @@ public class AnsibleUpdateNodeInfo extends NodeTaskBase {
   public void run() {
     // Create the process to fetch information about the node from the cloud provider.
     ShellResponse response =
-        getNodeManager().nodeCommand(NodeManager.NodeCommandType.List, taskParams());
-    processShellResponse(response);
+        getNodeManager()
+            .nodeCommand(NodeManager.NodeCommandType.List, taskParams())
+            .processErrors();
 
     NodeTaskParams taskParams = taskParams();
     log.info(
@@ -54,7 +55,7 @@ public class AnsibleUpdateNodeInfo extends NodeTaskBase {
     if (Strings.isNullOrEmpty(response.message)) {
       String msg =
           String.format(
-              "Node % in universe %s is not found.", taskParams.nodeName, taskParams.universeUUID);
+              "Node %s in universe %s is not found.", taskParams.nodeName, taskParams.universeUUID);
       log.error(msg);
       throw new RuntimeException(msg);
     }
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/BackupTable.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/BackupTable.java
index c8fceda9af..f54a46610f 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/BackupTable.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/BackupTable.java
@@ -58,8 +58,7 @@ public class BackupTable extends AbstractTaskBase {
         if (taskParams().backupList != null) {
           for (BackupTableParams backupParams : taskParams().backupList) {
             backupParams.backupUuid = taskParams().backupUuid;
-            ShellResponse response = tableManager.createBackup(backupParams);
-            processShellResponse(response);
+            ShellResponse response = tableManager.createBackup(backupParams).processErrors();
             JsonNode jsonNode = null;
             try {
               jsonNode = Json.parse(response.message);
@@ -77,7 +76,7 @@ public class BackupTable extends AbstractTaskBase {
 
           backup.transitionState(Backup.BackupState.Completed);
         } else {
-          ShellResponse response = tableManager.createBackup(taskParams());
+          ShellResponse response = tableManager.createBackup(taskParams()).processErrors();
           JsonNode jsonNode = null;
           try {
             jsonNode = Json.parse(response.message);
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/BackupTableYb.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/BackupTableYb.java
index 0491027468..51bfae16a2 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/BackupTableYb.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/BackupTableYb.java
@@ -14,18 +14,9 @@ import com.fasterxml.jackson.databind.JsonNode;
 import com.yugabyte.yw.commissioner.AbstractTaskBase;
 import com.yugabyte.yw.commissioner.BaseTaskDependencies;
 import com.yugabyte.yw.common.ShellResponse;
-import com.yugabyte.yw.common.TableManagerYb;
 import com.yugabyte.yw.forms.BackupTableParams;
 import com.yugabyte.yw.models.Backup;
 import com.yugabyte.yw.models.Universe;
-import play.libs.Json;
-
-import com.fasterxml.jackson.databind.ObjectWriter;
-import com.fasterxml.jackson.databind.ObjectMapper;
-
-import javax.inject.Inject;
-
-import java.util.List;
 import java.util.Map;
 import javax.inject.Inject;
 import lombok.extern.slf4j.Slf4j;
@@ -56,8 +47,7 @@ public class BackupTableYb extends AbstractTaskBase {
         if (config.isEmpty() || config.getOrDefault(Universe.TAKE_BACKUPS, "true").equals("true")) {
           if (taskParams().backupList != null) {
             for (BackupTableParams backupParams : taskParams().backupList) {
-              ShellResponse response = tableManagerYb.createBackup(backupParams);
-              processShellResponse(response);
+              ShellResponse response = tableManagerYb.createBackup(backupParams).processErrors();
               JsonNode jsonNode = null;
               try {
                 jsonNode = Json.parse(response.message);
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/BulkImport.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/BulkImport.java
index fe2de6cd59..ccc285f333 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/BulkImport.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/BulkImport.java
@@ -29,6 +29,6 @@ public class BulkImport extends AbstractTaskBase {
   @Override
   public void run() {
     // Execute the ansible command and log its result.
-    processShellResponse(tableManager.bulkImport(taskParams()));
+    tableManager.bulkImport(taskParams()).processErrors();
   }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ChangeInstanceType.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ChangeInstanceType.java
index 418a224e6e..c51127ead3 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ChangeInstanceType.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ChangeInstanceType.java
@@ -41,9 +41,8 @@ public class ChangeInstanceType extends NodeTaskBase {
             .instance_type,
         taskParams().instanceType);
 
-    ShellResponse response =
-        getNodeManager()
-            .nodeCommand(NodeManager.NodeCommandType.Change_Instance_Type, taskParams());
-    processShellResponse(response);
+    getNodeManager()
+        .nodeCommand(NodeManager.NodeCommandType.Change_Instance_Type, taskParams())
+        .processErrors();
   }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/CreateRootVolumes.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/CreateRootVolumes.java
index 0889d2da11..4f71111bc5 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/CreateRootVolumes.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/CreateRootVolumes.java
@@ -32,8 +32,9 @@ public class CreateRootVolumes extends NodeTaskBase {
   @Override
   public void run() {
     ShellResponse response =
-        getNodeManager().nodeCommand(NodeManager.NodeCommandType.Create_Root_Volumes, taskParams());
-    processShellResponse(response);
+        getNodeManager()
+            .nodeCommand(NodeManager.NodeCommandType.Create_Root_Volumes, taskParams())
+            .processErrors();
     JsonNode parsedResponse = parseShellResponseAsJson(response);
     List<String> disks = Json.fromJson(parsedResponse, CopyOnWriteArrayList.class);
     taskParams().bootDisksPerZone.putIfAbsent(taskParams().azUuid, disks);
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/CreateTable.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/CreateTable.java
index 2be611e339..df71e63a9d 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/CreateTable.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/CreateTable.java
@@ -12,7 +12,6 @@ package com.yugabyte.yw.commissioner.tasks.subtasks;
 
 import com.datastax.driver.core.Cluster;
 import com.datastax.driver.core.Session;
-import com.google.api.client.util.Throwables;
 import com.yugabyte.yw.commissioner.AbstractTaskBase;
 import com.yugabyte.yw.commissioner.BaseTaskDependencies;
 import com.yugabyte.yw.common.NodeUniverseManager;
@@ -30,6 +29,7 @@ import java.time.Duration;
 import java.time.Instant;
 import java.util.List;
 import java.util.Random;
+import java.util.concurrent.TimeUnit;
 import java.util.regex.Pattern;
 import java.util.stream.Collectors;
 import javax.inject.Inject;
@@ -44,8 +44,9 @@ public class CreateTable extends AbstractTaskBase {
 
   private static final Pattern YSQLSH_CREATE_TABLE_SUCCESS =
       Pattern.compile("Command output:.*CREATE TABLE", Pattern.DOTALL);
-  private static final int RETRY_DELAY_SEC = 5;
-  private static final int MAX_TIMEOUT_SEC = 60;
+  private static final long RETRY_DELAY_SEC = 30;
+  private static final long MIN_RETRY_COUNT = 3;
+  private static final long TOTAL_ATTEMPTS_DURATION_SEC = TimeUnit.MINUTES.toSeconds(10);
 
   // To use for the Cassandra client
   private Cluster cassandraCluster;
@@ -105,8 +106,8 @@ public class CreateTable extends AbstractTaskBase {
     boolean tableCreated = false;
     Random random = new Random();
     int attempt = 0;
-    Instant timeout = Instant.now().plusSeconds(MAX_TIMEOUT_SEC);
-    while (Instant.now().isBefore(timeout) || attempt < 2) {
+    Instant timeout = Instant.now().plusSeconds(TOTAL_ATTEMPTS_DURATION_SEC);
+    while (Instant.now().isBefore(timeout) || attempt < MIN_RETRY_COUNT) {
       NodeDetails randomTServer = tserverLiveNodes.get(random.nextInt(tserverLiveNodes.size()));
       ShellResponse response =
           nodeUniverseManager.runYsqlCommand(
@@ -119,7 +120,7 @@ public class CreateTable extends AbstractTaskBase {
             randomTServer.nodeName,
             response.code,
             response.message);
-        waitFor(Duration.ofMillis(RETRY_DELAY_SEC));
+        waitFor(Duration.ofSeconds(RETRY_DELAY_SEC));
       } else {
         tableCreated = true;
         break;
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/DeleteBackup.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/DeleteBackup.java
index fe75537a53..1d1207c5c0 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/DeleteBackup.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/DeleteBackup.java
@@ -89,8 +89,7 @@ public class DeleteBackup extends AbstractTaskBase {
 
   private boolean deleteBackup(BackupTableParams backupTableParams) {
     backupTableParams.actionType = BackupTableParams.ActionType.DELETE;
-    ShellResponse response = tableManager.deleteBackup(backupTableParams);
-    processShellResponse(response);
+    ShellResponse response = tableManager.deleteBackup(backupTableParams).processErrors();
     JsonNode jsonNode = null;
     try {
       jsonNode = Json.parse(response.message);
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/InstanceActions.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/InstanceActions.java
index fbe7883e68..b007d2cc9b 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/InstanceActions.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/InstanceActions.java
@@ -13,7 +13,6 @@ package com.yugabyte.yw.commissioner.tasks.subtasks;
 import com.yugabyte.yw.commissioner.BaseTaskDependencies;
 import com.yugabyte.yw.commissioner.tasks.params.NodeTaskParams;
 import com.yugabyte.yw.common.NodeManager;
-import com.yugabyte.yw.common.ShellResponse;
 import javax.inject.Inject;
 import lombok.extern.slf4j.Slf4j;
 
@@ -45,7 +44,6 @@ public class InstanceActions extends NodeTaskBase {
         taskParams().type.toString(),
         taskParams().nodeName);
 
-    ShellResponse response = getNodeManager().nodeCommand(taskParams().type, taskParams());
-    processShellResponse(response);
+    getNodeManager().nodeCommand(taskParams().type, taskParams()).processErrors();
   }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/KubernetesCommandExecutor.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/KubernetesCommandExecutor.java
index 15b1875216..846bb2966c 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/KubernetesCommandExecutor.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/KubernetesCommandExecutor.java
@@ -219,7 +219,7 @@ public class KubernetesCommandExecutor extends UniverseTaskBase {
       if (response.code != 0 && flag) {
         response = getPodError(config);
       }
-      processShellResponse(response);
+      response.processErrors();
     }
   }
 
@@ -227,8 +227,9 @@ public class KubernetesCommandExecutor extends UniverseTaskBase {
     ShellResponse response = new ShellResponse();
     response.code = -1;
     ShellResponse podResponse =
-        kubernetesManager.getPodInfos(config, taskParams().nodePrefix, taskParams().namespace);
-    processShellResponse(response);
+        kubernetesManager
+            .getPodInfos(config, taskParams().nodePrefix, taskParams().namespace)
+            .processErrors();
     JsonNode podInfos = parseShellResponseAsJson(podResponse);
     boolean flag = false;
     for (JsonNode podInfo : podInfos.path("items")) {
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ManipulateDnsRecordTask.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ManipulateDnsRecordTask.java
index c2eec222dc..41de507a62 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ManipulateDnsRecordTask.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ManipulateDnsRecordTask.java
@@ -14,7 +14,6 @@ import com.google.inject.Inject;
 import com.yugabyte.yw.commissioner.BaseTaskDependencies;
 import com.yugabyte.yw.commissioner.tasks.UniverseTaskBase;
 import com.yugabyte.yw.common.DnsManager;
-import com.yugabyte.yw.common.ShellResponse;
 import com.yugabyte.yw.forms.UniverseTaskParams;
 import com.yugabyte.yw.models.Universe;
 import com.yugabyte.yw.models.helpers.NodeDetails;
@@ -56,14 +55,14 @@ public class ManipulateDnsRecordTask extends UniverseTaskBase {
       String nodeIpCsv =
           tserverNodes.stream().map(nd -> nd.cloudInfo.private_ip).collect(Collectors.joining(","));
       // Create the process to fetch information about the node from the cloud provider.
-      ShellResponse response =
-          dnsManager.manipulateDnsRecord(
+      dnsManager
+          .manipulateDnsRecord(
               taskParams().type,
               taskParams().providerUUID,
               taskParams().hostedZoneId,
               taskParams().domainNamePrefix,
-              nodeIpCsv);
-      processShellResponse(response);
+              nodeIpCsv)
+          .processErrors();
     } catch (Exception e) {
       if (taskParams().type != DnsManager.DnsCommandType.Delete || !taskParams().isForceDelete) {
         throw e;
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/PauseServer.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/PauseServer.java
index 3089034827..d455c628fb 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/PauseServer.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/PauseServer.java
@@ -13,7 +13,6 @@ package com.yugabyte.yw.commissioner.tasks.subtasks;
 import com.yugabyte.yw.commissioner.BaseTaskDependencies;
 import com.yugabyte.yw.commissioner.tasks.params.NodeTaskParams;
 import com.yugabyte.yw.common.NodeManager;
-import com.yugabyte.yw.common.ShellResponse;
 import com.yugabyte.yw.models.Universe;
 import com.yugabyte.yw.models.helpers.NodeDetails;
 import javax.inject.Inject;
@@ -51,9 +50,7 @@ public class PauseServer extends NodeTaskBase {
       // Update the node state as stopping also can not set the node state to stopped
       // as it will be not reachable.
       setNodeState(NodeDetails.NodeState.Stopping);
-      ShellResponse response =
-          getNodeManager().nodeCommand(NodeManager.NodeCommandType.Pause, taskParams());
-      processShellResponse(response);
+      getNodeManager().nodeCommand(NodeManager.NodeCommandType.Pause, taskParams()).processErrors();
       pauseUniverse(taskParams().nodeName);
       setNodeState(NodeDetails.NodeState.Stopped);
     } catch (Exception e) {
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/PrecheckNodeDetached.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/PrecheckNodeDetached.java
index 248a20056d..66ea770a53 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/PrecheckNodeDetached.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/PrecheckNodeDetached.java
@@ -46,6 +46,6 @@ public class PrecheckNodeDetached extends AbstractTaskBase {
         }
       }
     }
-    processShellResponse(response);
+    response.processErrors();
   }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/RemoveUniverseEntry.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/RemoveUniverseEntry.java
index 42ab7d552d..fb735abb98 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/RemoveUniverseEntry.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/RemoveUniverseEntry.java
@@ -40,6 +40,6 @@ public class RemoveUniverseEntry extends UniverseTaskBase {
         taskParams().customerUUID,
         AlertConfiguration.TargetType.UNIVERSE,
         taskParams().universeUUID);
-    metricService.handleSourceRemoval(taskParams().customerUUID, taskParams().universeUUID);
+    metricService.markSourceRemoved(taskParams().customerUUID, taskParams().universeUUID);
   }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ReplaceRootVolume.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ReplaceRootVolume.java
index 9e303d1d23..f0d067d94d 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ReplaceRootVolume.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ReplaceRootVolume.java
@@ -41,8 +41,8 @@ public class ReplaceRootVolume extends NodeTaskBase {
 
     // this won't be saved in taskDetails!
     taskParams().replacementDisk = bootDisks.remove(0);
-    ShellResponse response =
-        getNodeManager().nodeCommand(NodeManager.NodeCommandType.Replace_Root_Volume, taskParams());
-    processShellResponse(response);
+    getNodeManager()
+        .nodeCommand(NodeManager.NodeCommandType.Replace_Root_Volume, taskParams())
+        .processErrors();
   }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ResumeServer.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ResumeServer.java
index 9e316a8113..9435ae8e25 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ResumeServer.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/ResumeServer.java
@@ -49,9 +49,9 @@ public class ResumeServer extends NodeTaskBase {
   @Override
   public void run() {
     try {
-      ShellResponse response =
-          getNodeManager().nodeCommand(NodeManager.NodeCommandType.Resume, taskParams());
-      processShellResponse(response);
+      getNodeManager()
+          .nodeCommand(NodeManager.NodeCommandType.Resume, taskParams())
+          .processErrors();
       setNodeState(NodeDetails.NodeState.Live);
       resumeUniverse(taskParams().nodeName);
     } catch (Exception e) {
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/RunExternalScript.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/RunExternalScript.java
index 802fd777b1..8a5c563446 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/RunExternalScript.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/RunExternalScript.java
@@ -9,7 +9,6 @@ import com.google.inject.Inject;
 import com.yugabyte.yw.commissioner.AbstractTaskBase;
 import com.yugabyte.yw.commissioner.BaseTaskDependencies;
 import com.yugabyte.yw.common.ShellProcessHandler;
-import com.yugabyte.yw.common.ShellResponse;
 import com.yugabyte.yw.common.Util;
 import com.yugabyte.yw.common.config.impl.RuntimeConfig;
 import com.yugabyte.yw.common.config.impl.SettableRuntimeConfigFactory;
@@ -129,9 +128,7 @@ public class RunExternalScript extends AbstractTaskBase {
       String description = String.join(" ", commandList);
 
       // Execute the command.
-      ShellResponse shellResponse =
-          shellProcessHandler.run(commandList, new HashMap<>(), description);
-      processShellResponse(shellResponse);
+      shellProcessHandler.run(commandList, new HashMap<>(), description).processErrors();
     } catch (Exception e) {
       log.error("Error executing task {}, error='{}'", getName(), e.getMessage(), e);
       throw new RuntimeException(e);
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/RunYsqlUpgrade.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/RunYsqlUpgrade.java
index 00b09a9960..f9d2fa0491 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/RunYsqlUpgrade.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/RunYsqlUpgrade.java
@@ -102,7 +102,7 @@ public class RunYsqlUpgrade extends AbstractTaskBase {
                 leaderMasterNode, universe, "upgrade_ysql", timeout);
 
         if (numAttempts == MAX_ATTEMPTS) {
-          processShellResponse(response);
+          response.processErrors();
         } else {
           if (response.code == ERROR_CODE_SUCCESS) {
             log.info("Successfully performed YSQL upgrade");
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/UpdateMountedDisks.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/UpdateMountedDisks.java
index 6d5276dcc9..51fa595976 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/UpdateMountedDisks.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/UpdateMountedDisks.java
@@ -6,7 +6,6 @@ import com.google.inject.Inject;
 import com.yugabyte.yw.commissioner.BaseTaskDependencies;
 import com.yugabyte.yw.commissioner.tasks.params.NodeTaskParams;
 import com.yugabyte.yw.common.NodeManager;
-import com.yugabyte.yw.common.ShellResponse;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -29,9 +28,8 @@ public class UpdateMountedDisks extends NodeTaskBase {
   public void run() {
     LOG.info("Running UpdateMountedDisksTask against node {}", taskParams().nodeName);
 
-    ShellResponse response =
-        getNodeManager()
-            .nodeCommand(NodeManager.NodeCommandType.Update_Mounted_Disks, taskParams());
-    processShellResponse(response);
+    getNodeManager()
+        .nodeCommand(NodeManager.NodeCommandType.Update_Mounted_Disks, taskParams())
+        .processErrors();
   }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/WaitForServer.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/WaitForServer.java
index 2db33c8d1d..938c14ff25 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/WaitForServer.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/subtasks/WaitForServer.java
@@ -12,6 +12,7 @@ package com.yugabyte.yw.commissioner.tasks.subtasks;
 
 import com.google.common.net.HostAndPort;
 import com.yugabyte.yw.commissioner.BaseTaskDependencies;
+import com.yugabyte.yw.commissioner.tasks.UniverseDefinitionTaskBase.ServerType;
 import com.yugabyte.yw.commissioner.tasks.params.ServerSubTaskParams;
 import javax.inject.Inject;
 import lombok.extern.slf4j.Slf4j;
@@ -46,8 +47,13 @@ public class WaitForServer extends ServerSubTaskBase {
     try {
       HostAndPort hp = getHostPort();
       client = getClient();
-
-      ret = client.waitForServer(hp, taskParams().serverWaitTimeoutMs);
+      if (taskParams().serverType == ServerType.MASTER) {
+        // This first calls waitForServer followed by availability check of master UUID.
+        // Check for master UUID retries until timeout.
+        ret = client.waitForMaster(hp, taskParams().serverWaitTimeoutMs);
+      } else {
+        ret = client.waitForServer(hp, taskParams().serverWaitTimeoutMs);
+      }
     } catch (Exception e) {
       log.error("{} hit error : {}", getName(), e.getMessage());
       throw new RuntimeException(e);
diff --git a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/upgrade/SoftwareUpgrade.java b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/upgrade/SoftwareUpgrade.java
index 3f84e41981..78374e1887 100644
--- a/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/upgrade/SoftwareUpgrade.java
+++ b/managed/src/main/java/com/yugabyte/yw/commissioner/tasks/upgrade/SoftwareUpgrade.java
@@ -74,7 +74,8 @@ public class SoftwareUpgrade extends UpgradeTaskBase {
             "AnsibleConfigureServers (%s) for: %s",
             SubTaskGroupType.DownloadingSoftware, taskParams().nodePrefix);
 
-    SubTaskGroup downloadTaskGroup = getTaskExecutor().createSubTaskGroup(subGroupDescription);
+    SubTaskGroup downloadTaskGroup =
+        getTaskExecutor().createSubTaskGroup(subGroupDescription, executor);
     for (NodeDetails node : nodes) {
       downloadTaskGroup.addSubTask(
           getAnsibleConfigureServerTask(
diff --git a/managed/src/main/java/com/yugabyte/yw/common/AlertManager.java b/managed/src/main/java/com/yugabyte/yw/common/AlertManager.java
index d9a53ababd..eee466d6d5 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/AlertManager.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/AlertManager.java
@@ -40,10 +40,11 @@ import com.yugabyte.yw.models.helpers.KnownAlertLabels;
 import com.yugabyte.yw.models.helpers.PlatformMetrics;
 import java.time.temporal.ChronoUnit;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Date;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Map.Entry;
 import java.util.Set;
 import java.util.UUID;
 import java.util.stream.Collectors;
@@ -278,9 +279,8 @@ public class AlertManager {
         log.warn(
             "Unable to notify about alert {}, there is no default destination specified.",
             alert.getUuid());
-        metricService.setStatusMetric(
-            MetricService.buildMetricTemplate(PlatformMetrics.ALERT_MANAGER_STATUS, customer),
-            "Unable to notify about alert(s), there is no default destination specified.");
+        metricService.setFailureStatusMetric(
+            MetricService.buildMetricTemplate(PlatformMetrics.ALERT_MANAGER_STATUS, customer));
         return new SendNotificationResult(
             SendNotificationStatus.FAILED_TO_RESCHEDULE, "No default destination configured");
       } else {
@@ -298,10 +298,8 @@ public class AlertManager {
         && ((AlertChannelEmailParams) channels.get(0).getParams()).isDefaultRecipients()
         && CollectionUtils.isEmpty(emailHelper.getDestinations(customer.getUuid()))) {
 
-      metricService.setStatusMetric(
-          MetricService.buildMetricTemplate(PlatformMetrics.ALERT_MANAGER_STATUS, customer),
-          "Unable to notify about alert(s) using default destination, "
-              + "there are no recipients configured in the customer's profile.");
+      metricService.setFailureStatusMetric(
+          MetricService.buildMetricTemplate(PlatformMetrics.ALERT_MANAGER_STATUS, customer));
       return new SendNotificationResult(
           SendNotificationStatus.FAILED_TO_RESCHEDULE,
           "No recipients configured in Health settings");
@@ -323,6 +321,7 @@ public class AlertManager {
       tempAlert.setState(stateToNotify);
     }
 
+    Map<String, String> perChannelStatus = new HashMap<>();
     for (AlertChannel channel : channels) {
       try {
         alertChannelService.validate(channel);
@@ -331,7 +330,8 @@ public class AlertManager {
         if (report.failuresByChannel(channel.getUuid()) == 0) {
           log.warn(String.format("Channel %s skipped: %s", channel.getUuid(), e.getMessage()), e);
         }
-        handleChannelSendError(channel, report, "Misconfigured alert channel: " + e.getMessage());
+        perChannelStatus.put(channel.getName(), "Misconfigured alert channel");
+        handleChannelSendError(channel, report);
         continue;
       }
 
@@ -340,45 +340,50 @@ public class AlertManager {
             channelsManager.get(AlertUtils.getJsonTypeName(channel.getParams()));
         handler.sendNotification(customer, tempAlert, channel);
         atLeastOneSucceeded = true;
+        perChannelStatus.put(channel.getName(), "Alert sent successfully");
         setOkChannelStatusMetric(PlatformMetrics.ALERT_MANAGER_CHANNEL_STATUS, channel);
       } catch (PlatformServiceException e) {
         if (report.failuresByChannel(channel.getUuid()) == 0) {
           log.error(e.getMessage(), e);
         }
-        handleChannelSendError(channel, report, e.getMessage());
+        perChannelStatus.put(channel.getName(), e.getMessage());
+        handleChannelSendError(channel, report);
       } catch (Exception e) {
         if (report.failuresByChannel(channel.getUuid()) == 0) {
           log.error(e.getMessage(), e);
         }
-        handleChannelSendError(channel, report, "Error sending notification: " + e.getMessage());
+        perChannelStatus.put(channel.getName(), "Error sending notification: " + e.getMessage());
+        handleChannelSendError(channel, report);
       }
     }
 
+    String resultMessage =
+        "Result: "
+            + perChannelStatus
+                .entrySet()
+                .stream()
+                .sorted(Entry.comparingByKey())
+                .map(e -> e.getKey() + " - " + e.getValue())
+                .collect(Collectors.joining("; "));
     return atLeastOneSucceeded
-        ? new SendNotificationResult(SendNotificationStatus.SUCCEEDED, "Alert sent successfully")
-        : new SendNotificationResult(
-            SendNotificationStatus.FAILED_TO_RESCHEDULE, "All notification channels failed");
+        ? new SendNotificationResult(SendNotificationStatus.SUCCEEDED, resultMessage)
+        : new SendNotificationResult(SendNotificationStatus.FAILED_TO_RESCHEDULE, resultMessage);
   }
 
-  private void handleChannelSendError(
-      AlertChannel channel, AlertNotificationReport report, String alertMessage) {
+  private void handleChannelSendError(AlertChannel channel, AlertNotificationReport report) {
     report.failChannel(channel.getUuid());
-    setChannelStatusMetric(PlatformMetrics.ALERT_MANAGER_CHANNEL_STATUS, channel, alertMessage);
+    setChannelStatusMetric(PlatformMetrics.ALERT_MANAGER_CHANNEL_STATUS, channel, false);
   }
 
   @VisibleForTesting
   void setOkChannelStatusMetric(PlatformMetrics metric, AlertChannel channel) {
-    setChannelStatusMetric(metric, channel, StringUtils.EMPTY);
+    setChannelStatusMetric(metric, channel, true);
   }
 
   @VisibleForTesting
-  void setChannelStatusMetric(PlatformMetrics metric, AlertChannel channel, String message) {
-    boolean isSuccess = StringUtils.isEmpty(message);
+  void setChannelStatusMetric(PlatformMetrics metric, AlertChannel channel, boolean isSuccess) {
     Metric statusMetric = buildMetricTemplate(metric, channel).setValue(isSuccess ? 1.0 : 0.0);
-    if (!isSuccess) {
-      statusMetric.setLabel(KnownAlertLabels.ERROR_MESSAGE, message);
-    }
-    metricService.cleanAndSave(Collections.singletonList(statusMetric));
+    metricService.save(statusMetric);
   }
 
   private Metric buildMetricTemplate(PlatformMetrics metric, AlertChannel channel) {
diff --git a/managed/src/main/java/com/yugabyte/yw/common/AlertTemplate.java b/managed/src/main/java/com/yugabyte/yw/common/AlertTemplate.java
index 00b67ee690..10fde5b83b 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/AlertTemplate.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/AlertTemplate.java
@@ -8,6 +8,7 @@ import static com.yugabyte.yw.models.common.Unit.COUNT;
 import static com.yugabyte.yw.models.common.Unit.DAY;
 import static com.yugabyte.yw.models.common.Unit.MILLISECOND;
 import static com.yugabyte.yw.models.common.Unit.PERCENT;
+import static com.yugabyte.yw.models.common.Unit.SECOND;
 import static com.yugabyte.yw.models.common.Unit.STATUS;
 
 import com.google.common.collect.ImmutableMap;
@@ -43,7 +44,7 @@ public enum AlertTemplate {
       "Average replication lag for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }} ms."
           + " Current value is {{ $value | printf \\\"%.0f\\\" }} ms",
-      15,
+      0,
       EnumSet.noneOf(DefinitionSettings.class),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -60,7 +61,7 @@ public enum AlertTemplate {
       "Max clock skew for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }} ms."
           + " Current value is {{ $value | printf \\\"%.0f\\\" }} ms",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -87,7 +88,7 @@ public enum AlertTemplate {
       "Average memory usage for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }}%."
           + " Current value is {{ $value | printf \\\"%.0f\\\" }}%",
-      15,
+      0,
       EnumSet.noneOf(DefinitionSettings.class),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -98,55 +99,50 @@ public enum AlertTemplate {
   HEALTH_CHECK_ERROR(
       "Health Check Error",
       "Failed to perform health check",
-      "ybp_health_check_status{universe_uuid = \"__universeUuid__\"} {{ query_condition }} 1",
-      "Failed to perform health check for universe '{{ $labels.source_name }}':"
-          + " {{ $labels.error_message }}",
-      15,
+      "last_over_time(ybp_health_check_status{universe_uuid = \"__universeUuid__\"}[1d])"
+          + " {{ query_condition }} 1",
+      "Failed to perform health check for universe '{{ $labels.source_name }}'"
+          + " - check YB Platform logs for details or contact YB support team",
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
-      ThresholdSettings.builder().statusThreshold(SEVERE).build(),
-      TestAlertSettings.builder()
-          .label(KnownAlertLabels.ERROR_MESSAGE, "Some error occurred")
-          .build()),
+      ThresholdSettings.builder().statusThreshold(SEVERE).build()),
 
   HEALTH_CHECK_NOTIFICATION_ERROR(
       "Health Check Notification Error",
       "Failed to perform health check notification",
-      "ybp_health_check_notification_status{universe_uuid = \"__universeUuid__\"}"
+      "last_over_time(ybp_health_check_notification_status"
+          + "{universe_uuid = \"__universeUuid__\"}[1d])"
           + " {{ query_condition }} 1",
-      "Failed to perform health check notification for universe '{{ $labels.source_name }}':"
-          + " {{ $labels.error_message }}",
-      15,
+      "Failed to perform health check notification for universe '{{ $labels.source_name }}'"
+          + " - check Health notification settings and YB Platform logs for details"
+          + " or contact YB support team",
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
-      ThresholdSettings.builder().statusThreshold(SEVERE).build(),
-      TestAlertSettings.builder()
-          .label(KnownAlertLabels.ERROR_MESSAGE, "Some error occurred")
-          .build()),
+      ThresholdSettings.builder().statusThreshold(SEVERE).build()),
 
   BACKUP_FAILURE(
       "Backup Failure",
       "Last universe backup creation task failed",
-      "ybp_create_backup_status{universe_uuid = \"__universeUuid__\"}" + " {{ query_condition }} 1",
-      "Last backup task for universe '{{ $labels.source_name }}' failed:"
-          + " {{ $labels.error_message }}",
-      15,
+      "last_over_time(ybp_create_backup_status{universe_uuid = \"__universeUuid__\"}[1d])"
+          + " {{ query_condition }} 1",
+      "Last backup task for universe '{{ $labels.source_name }}' failed"
+          + " - check backup task result for more details",
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
-      ThresholdSettings.builder().statusThreshold(SEVERE).build(),
-      TestAlertSettings.builder()
-          .label(KnownAlertLabels.ERROR_MESSAGE, "Some error occurred")
-          .build()),
+      ThresholdSettings.builder().statusThreshold(SEVERE).build()),
 
   BACKUP_SCHEDULE_FAILURE(
       "Backup Schedule Failure",
       "Last attempt to run scheduled backup failed due to other backup"
           + " or universe operation in progress",
-      "ybp_schedule_backup_status{universe_uuid = \"__universeUuid__\"}"
+      "last_over_time(ybp_schedule_backup_status{universe_uuid = \"__universeUuid__\"}[1d])"
           + " {{ query_condition }} 1",
       "Last attempt to run scheduled backup for universe '{{ $labels.source_name }}'"
           + " failed due to other backup or universe operation is in progress.",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder().statusThreshold(SEVERE).build()),
@@ -158,7 +154,7 @@ public enum AlertTemplate {
           + " {{ query_condition }} {{ query_threshold }}",
       "{{ $value | printf \\\"%.0f\\\" }} node(s) has inactive cronjob"
           + " for universe '{{ $labels.source_name }}'.",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -171,60 +167,49 @@ public enum AlertTemplate {
   ALERT_QUERY_FAILED(
       "Alert Query Failed",
       "Failed to query alerts from Prometheus",
-      "ybp_alert_query_status {{ query_condition }} 1",
-      "Last alert query for customer '{{ $labels.source_name }}' failed:"
-          + " {{ $labels.error_message }}",
-      15,
+      "last_over_time(ybp_alert_query_status[1d]) {{ query_condition }} 1",
+      "Last alert query for customer '{{ $labels.source_name }}' failed"
+          + " - check YB Platform logs for details or contact YB support team",
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.PLATFORM,
-      ThresholdSettings.builder().statusThreshold(SEVERE).build(),
-      TestAlertSettings.builder()
-          .label(KnownAlertLabels.ERROR_MESSAGE, "Some error occurred")
-          .build()),
+      ThresholdSettings.builder().statusThreshold(SEVERE).build()),
 
   ALERT_CONFIG_WRITING_FAILED(
       "Alert Rules Sync Failed",
       "Failed to sync alerting rules to Prometheus",
-      "ybp_alert_config_writer_status {{ query_condition }} 1",
-      "Last alert rules sync for customer '{{ $labels.source_name }}' failed:"
-          + " {{ $labels.error_message }}",
-      15,
+      "last_over_time(ybp_alert_config_writer_status[1d]) {{ query_condition }} 1",
+      "Last alert rules sync for customer '{{ $labels.source_name }}' failed"
+          + " - check YB Platform logs for details or contact YB support team",
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.PLATFORM,
-      ThresholdSettings.builder().statusThreshold(SEVERE).build(),
-      TestAlertSettings.builder()
-          .label(KnownAlertLabels.ERROR_MESSAGE, "Some error occurred")
-          .build()),
+      ThresholdSettings.builder().statusThreshold(SEVERE).build()),
 
   ALERT_NOTIFICATION_ERROR(
       "Alert Notification Failed",
       "Failed to send alert notifications",
-      "ybp_alert_manager_status{customer_uuid = \"__customerUuid__\"}" + " {{ query_condition }} 1",
+      "last_over_time(ybp_alert_manager_status{customer_uuid = \"__customerUuid__\"}[1d])"
+          + " {{ query_condition }} 1",
       "Last attempt to send alert notifications for customer '{{ $labels.source_name }}'"
-          + " failed: {{ $labels.error_message }}",
-      15,
+          + " failed - check YB Platform logs for details or contact YB support team",
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.PLATFORM,
-      ThresholdSettings.builder().statusThreshold(SEVERE).build(),
-      TestAlertSettings.builder()
-          .label(KnownAlertLabels.ERROR_MESSAGE, "Some error occurred")
-          .build()),
+      ThresholdSettings.builder().statusThreshold(SEVERE).build()),
 
   ALERT_NOTIFICATION_CHANNEL_ERROR(
       "Alert Channel Failed",
       "Failed to send alerts to notification channel",
-      "ybp_alert_manager_channel_status{customer_uuid = \"__customerUuid__\"}"
+      "last_over_time(ybp_alert_manager_channel_status{customer_uuid = \"__customerUuid__\"}[1d])"
           + " {{ query_condition }} 1",
       "Last attempt to send alert notifications to channel '{{ $labels.source_name }}'"
-          + " failed: {{ $labels.error_message }}",
-      15,
+          + " failed - try sending test alert to get more details",
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER, DefinitionSettings.SKIP_TARGET_LABELS),
       TargetType.PLATFORM,
       ThresholdSettings.builder().statusThreshold(SEVERE).build(),
-      TestAlertSettings.builder()
-          .label(KnownAlertLabels.ERROR_MESSAGE, "Some error occurred")
-          .label(KnownAlertLabels.SOURCE_NAME, "Some Channel")
-          .build()),
+      TestAlertSettings.builder().label(KnownAlertLabels.SOURCE_NAME, "Some Channel").build()),
 
   NODE_DOWN(
       "DB node down",
@@ -235,7 +220,7 @@ public enum AlertTemplate {
           + "{{ query_condition }} {{ query_threshold }}",
       "{{ $value | printf \\\"%.0f\\\" }} DB node(s) are down "
           + "for more than 15 minutes for universe '{{ $labels.source_name }}'.",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -254,7 +239,7 @@ public enum AlertTemplate {
       "Universe '{{ $labels.source_name }}'"
           + " DB node is restarted {{ $value | printf \\\"%.0f\\\" }} times"
           + " during last 30 minutes",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -275,7 +260,7 @@ public enum AlertTemplate {
           + "{{ query_condition }} {{ query_threshold }})",
       "Average node CPU usage for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }}% on {{ $value | printf \\\"%.0f\\\" }} node(s).",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -295,7 +280,7 @@ public enum AlertTemplate {
           + "* 100) {{ query_condition }} {{ query_threshold }})",
       "Node disk usage for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }}% on {{ $value | printf \\\"%.0f\\\" }} node(s).",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -312,7 +297,7 @@ public enum AlertTemplate {
           + "{{ query_condition }} {{ query_threshold }})",
       "Node file descriptors usage for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }}% on {{ $value | printf \\\"%.0f\\\" }} node(s).",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -330,7 +315,7 @@ public enum AlertTemplate {
       "More than {{ $labels.threshold }} OOM kills detected"
           + " for universe '{{ $labels.source_name }}'"
           + " on {{ $value | printf \\\"%.0f\\\" }} node(s).",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -371,7 +356,7 @@ public enum AlertTemplate {
           + "{{ query_condition }} {{ query_threshold }}",
       "{{ $value | printf \\\"%.0f\\\" }} DB Master/TServer instance(s) are down "
           + "for more than 15 minutes for universe '{{ $labels.source_name }}'.",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -392,7 +377,7 @@ public enum AlertTemplate {
       "Universe '{{ $labels.source_name }}'"
           + " Master or TServer is restarted {{ $value | printf \\\"%.0f\\\" }} times"
           + " during last 30 minutes",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -415,7 +400,7 @@ public enum AlertTemplate {
           + "{{ query_condition }} {{ query_threshold }}",
       "Fatal logs detected for universe '{{ $labels.source_name }}'"
           + " on {{ $value | printf \\\"%.0f\\\" }} Master/TServer instance(s).",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -441,7 +426,7 @@ public enum AlertTemplate {
           + "{{ query_condition }} {{ query_threshold }}",
       "Error logs detected for universe '{{ $labels.source_name }}'"
           + " on {{ $value | printf \\\"%.0f\\\" }} Master/TServer instance(s).",
-      15,
+      0,
       EnumSet.noneOf(DefinitionSettings.class),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -458,7 +443,7 @@ public enum AlertTemplate {
           + "{{ query_condition }} {{ query_threshold }}",
       "Core files detected for universe '{{ $labels.source_name }}'"
           + " on {{ $value | printf \\\"%.0f\\\" }} TServer instance(s).",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -475,7 +460,7 @@ public enum AlertTemplate {
           + "{{ query_condition }} {{ query_threshold }}",
       "YSQLSH connection failure detected for universe '{{ $labels.source_name }}'"
           + " on {{ $value | printf \\\"%.0f\\\" }} TServer instance(s).",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -492,7 +477,7 @@ public enum AlertTemplate {
           + "{{ query_condition }} {{ query_threshold }}",
       "CQLSH connection failure detected for universe '{{ $labels.source_name }}'"
           + " on {{ $value | printf \\\"%.0f\\\" }} TServer instance(s).",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -508,7 +493,7 @@ public enum AlertTemplate {
           + "{{ query_condition }} {{ query_threshold }}",
       "Redis connection failure detected for universe '{{ $labels.source_name }}'"
           + " on {{ $value | printf \\\"%.0f\\\" }} TServer instance(s).",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -528,7 +513,7 @@ public enum AlertTemplate {
           + "operation_memory_pressure_rejections{node_prefix=\"__nodePrefix__\"}[10m])) "
           + "{{ query_condition }} {{ query_threshold }}",
       "DB memory rejections detected for universe '{{ $labels.source_name }}'.",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -545,7 +530,7 @@ public enum AlertTemplate {
           + "majority_sst_files_rejections{node_prefix=\"__nodePrefix__\"}[10m])) "
           + "{{ query_condition }} {{ query_threshold }}",
       "DB compaction rejections detected for universe '{{ $labels.source_name }}'.",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -564,7 +549,7 @@ public enum AlertTemplate {
           + "rpcs_timed_out_in_queue{node_prefix=\"__nodePrefix__\"}[10m])) "
           + "{{ query_condition }} {{ query_threshold }}",
       "DB queues overflow detected for universe '{{ $labels.source_name }}'.",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -582,7 +567,7 @@ public enum AlertTemplate {
           + " {{ query_condition }} {{ query_threshold }})",
       "Test YSQL write/read operation failed on "
           + "{{ $value | printf \\\"%.0f\\\" }} nodes(s) for universe '{{ $labels.source_name }}'.",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder().statusThreshold(SEVERE).build()),
@@ -595,7 +580,7 @@ public enum AlertTemplate {
           + "{{ query_condition }} {{ query_threshold }})",
       "Node to node CA certificate for universe '{{ $labels.source_name }}'"
           + " will expire in {{ $value | printf \\\"%.0f\\\" }} days.",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -612,7 +597,7 @@ public enum AlertTemplate {
           + "{{ query_condition }} {{ query_threshold }})",
       "Node to node certificate for universe '{{ $labels.source_name }}'"
           + " will expire in {{ $value | printf \\\"%.0f\\\" }} days.",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -629,7 +614,7 @@ public enum AlertTemplate {
           + "{{ query_condition }} {{ query_threshold }})",
       "Client to node CA certificate for universe '{{ $labels.source_name }}'"
           + " will expire in {{ $value | printf \\\"%.0f\\\" }} days.",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -646,7 +631,7 @@ public enum AlertTemplate {
           + "{{ query_condition }} {{ query_threshold }})",
       "Client to node certificate for universe '{{ $labels.source_name }}'"
           + " will expire in {{ $value | printf \\\"%.0f\\\" }} days.",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -669,7 +654,7 @@ public enum AlertTemplate {
       "Average YSQL operations latency for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }} ms."
           + " Current value is {{ $value | printf \\\"%.0f\\\" }} ms",
-      15,
+      0,
       EnumSet.noneOf(DefinitionSettings.class),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -691,7 +676,7 @@ public enum AlertTemplate {
       "Average YCQL operations latency for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }} ms."
           + " Current value is {{ $value | printf \\\"%.0f\\\" }} ms",
-      15,
+      0,
       EnumSet.noneOf(DefinitionSettings.class),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -709,7 +694,7 @@ public enum AlertTemplate {
       "YSQL P99 latency for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }} ms."
           + " Current value is {{ $value | printf \\\"%.0f\\\" }} ms",
-      15,
+      0,
       EnumSet.noneOf(DefinitionSettings.class),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -727,7 +712,7 @@ public enum AlertTemplate {
       "YCQL P99 latency for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }} ms."
           + " Current value is {{ $value | printf \\\"%.0f\\\" }} ms",
-      15,
+      0,
       EnumSet.noneOf(DefinitionSettings.class),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -743,7 +728,7 @@ public enum AlertTemplate {
       "Number of YSQL connections for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }}."
           + " Current value is {{ $value | printf \\\"%.0f\\\" }}",
-      15,
+      0,
       EnumSet.noneOf(DefinitionSettings.class),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -759,7 +744,7 @@ public enum AlertTemplate {
       "Number of YCQL connections for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }}."
           + " Current value is {{ $value | printf \\\"%.0f\\\" }}",
-      15,
+      0,
       EnumSet.noneOf(DefinitionSettings.class),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -775,7 +760,7 @@ public enum AlertTemplate {
       "Number of YEDIS connections for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }}."
           + " Current value is {{ $value | printf \\\"%.0f\\\" }}",
-      15,
+      0,
       EnumSet.noneOf(DefinitionSettings.class),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -793,7 +778,7 @@ public enum AlertTemplate {
       "Maximum throughput for YSQL operations for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }}."
           + " Current value is {{ $value | printf \\\"%.0f\\\" }}",
-      15,
+      0,
       EnumSet.noneOf(DefinitionSettings.class),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -811,7 +796,7 @@ public enum AlertTemplate {
       "Maximum throughput for YCQL operations for universe '{{ $labels.source_name }}'"
           + " is above {{ $labels.threshold }}."
           + " Current value is {{ $value | printf \\\"%.0f\\\" }}",
-      15,
+      0,
       EnumSet.noneOf(DefinitionSettings.class),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -830,6 +815,26 @@ public enum AlertTemplate {
       TargetType.UNIVERSE,
       ThresholdSettings.builder().statusThreshold(SEVERE).build()),
 
+  MASTER_UNDER_REPLICATED(
+      "Under-replicated master",
+      "Master is missing from raft group or has follower lag higher than threshold",
+      "(min_over_time((ybp_universe_replication_factor{node_prefix=\"__nodePrefix__\"}"
+          + " - on(node_prefix) count by(node_prefix) (count by (node_prefix, exported_instance)"
+          + " (follower_lag_ms{export_type=\"master_export\", node_prefix=\"__nodePrefix__\"})))"
+          + "[{{ query_threshold }}s:]) > 0 or (max by(node_prefix) (follower_lag_ms"
+          + "{export_type=\"master_export\", node_prefix=\"__nodePrefix__\"})"
+          + " {{ query_condition }} ({{ query_threshold }} * 1000)))",
+      "Master is missing from raft group or has follower lag higher"
+          + " than {{ $labels.threshold }} seconds for universe '{{ $labels.source_name }}'.",
+      0,
+      EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
+      TargetType.UNIVERSE,
+      ThresholdSettings.builder()
+          .defaultThreshold(SEVERE, "yb.alert.underreplicated_masters_secs_severe")
+          .defaultThresholdUnit(SECOND)
+          .thresholdMinValue(1.0)
+          .build()),
+
   LEADERLESS_TABLETS(
       "Leaderless tablets",
       "Leader is missing for some tablet(s) for more than 5 minutes",
@@ -838,7 +843,7 @@ public enum AlertTemplate {
           + " {{ query_condition }} {{ query_threshold }})",
       "Tablet leader is missing for more than 5 minutes for "
           + "{{ $value | printf \\\"%.0f\\\" }} tablet(s) in universe '{{ $labels.source_name }}'.",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
@@ -855,7 +860,7 @@ public enum AlertTemplate {
           + " {{ query_condition }} {{ query_threshold }})",
       "{{ $value | printf \\\"%.0f\\\" }} tablet(s) remain under-replicated "
           + "for more than 5 minutes in universe '{{ $labels.source_name }}'.",
-      15,
+      0,
       EnumSet.of(DefinitionSettings.CREATE_FOR_NEW_CUSTOMER),
       TargetType.UNIVERSE,
       ThresholdSettings.builder()
diff --git a/managed/src/main/java/com/yugabyte/yw/common/ApiHelper.java b/managed/src/main/java/com/yugabyte/yw/common/ApiHelper.java
index b21f1d6eb6..7182813801 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/ApiHelper.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/ApiHelper.java
@@ -6,6 +6,7 @@ import akka.stream.javadsl.Source;
 import akka.util.ByteString;
 import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.node.ObjectNode;
+import com.google.common.annotations.VisibleForTesting;
 import com.google.inject.Inject;
 import com.google.inject.Singleton;
 import java.net.HttpURLConnection;
@@ -17,6 +18,8 @@ import java.util.Map;
 import java.util.UUID;
 import java.util.concurrent.CompletionStage;
 import java.util.concurrent.ExecutionException;
+import lombok.Getter;
+import lombok.extern.slf4j.Slf4j;
 import play.libs.Json;
 import play.libs.ws.WSClient;
 import play.libs.ws.WSRequest;
@@ -25,11 +28,18 @@ import play.mvc.Http;
 
 /** Helper class API specific stuff */
 @Singleton
+@Slf4j
 public class ApiHelper {
 
   private static final Duration DEFAULT_GET_REQUEST_TIMEOUT = Duration.ofSeconds(10);
 
-  @Inject WSClient wsClient;
+  @Getter(onMethod_ = {@VisibleForTesting})
+  private final WSClient wsClient;
+
+  @Inject
+  public ApiHelper(WSClient wsClient) {
+    this.wsClient = wsClient;
+  }
 
   public boolean postRequest(String url) {
     try {
@@ -124,7 +134,11 @@ public class ApiHelper {
       String jsonString = jsonPromise.toCompletableFuture().get();
       return Json.parse(jsonString);
     } catch (InterruptedException | ExecutionException e) {
+      log.warn("Unexpected exception while parsing response", e);
       return ApiResponse.errorJSON(e.getMessage());
+    } catch (RuntimeException e) {
+      log.warn("Unexpected exception while parsing response", e);
+      throw e;
     }
   }
 
diff --git a/managed/src/main/java/com/yugabyte/yw/common/CustomWsClientFactory.java b/managed/src/main/java/com/yugabyte/yw/common/CustomWsClientFactory.java
new file mode 100644
index 0000000000..6da0cb2217
--- /dev/null
+++ b/managed/src/main/java/com/yugabyte/yw/common/CustomWsClientFactory.java
@@ -0,0 +1,52 @@
+/*
+ * Copyright 2021 YugaByte, Inc. and Contributors
+ *
+ * Licensed under the Polyform Free Trial License 1.0.0 (the "License"); you
+ * may not use this file except in compliance with the License. You
+ * may obtain a copy of the License at
+ *
+ * http://github.com/YugaByte/yugabyte-db/blob/master/licenses/POLYFORM-FREE-TRIAL-LICENSE-1.0.0.txt
+ */
+
+package com.yugabyte.yw.common;
+
+import akka.stream.Materializer;
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+import com.typesafe.config.Config;
+import java.util.concurrent.CompletableFuture;
+import play.Environment;
+import play.inject.ApplicationLifecycle;
+import play.libs.ws.WSClient;
+import play.libs.ws.ahc.AhcWSClient;
+import play.libs.ws.ahc.AhcWSClientConfigFactory;
+
+@Singleton
+public class CustomWsClientFactory {
+
+  private final ApplicationLifecycle lifecycle;
+  private final Materializer materializer;
+  private final Environment environment;
+
+  @Inject
+  public CustomWsClientFactory(
+      ApplicationLifecycle lifecycle, Materializer materializer, Environment environment) {
+    this.lifecycle = lifecycle;
+    this.materializer = materializer;
+    this.environment = environment;
+  }
+
+  public WSClient forCustomConfig(Config customConfig) {
+    AhcWSClient customeWsClient =
+        AhcWSClient.create(
+            AhcWSClientConfigFactory.forConfig(customConfig, environment.classLoader()),
+            null, // no HTTP caching
+            materializer);
+    lifecycle.addStopHook(
+        () -> {
+          customeWsClient.close();
+          return CompletableFuture.completedFuture(null);
+        });
+    return customeWsClient;
+  }
+}
diff --git a/managed/src/main/java/com/yugabyte/yw/common/EmailHelper.java b/managed/src/main/java/com/yugabyte/yw/common/EmailHelper.java
index a0f5eb1e2b..314054d4f2 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/EmailHelper.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/EmailHelper.java
@@ -70,7 +70,7 @@ public class EmailHelper {
       SmtpData smtpData,
       Map<String, String> content)
       throws MessagingException {
-    LOG.info("Sending email: '{}' to '{}'", subject, destinations);
+    LOG.info("Sending email: '{}'", subject);
 
     Session session =
         Session.getInstance(
diff --git a/managed/src/main/java/com/yugabyte/yw/common/NodeManager.java b/managed/src/main/java/com/yugabyte/yw/common/NodeManager.java
index 86d2d9f8d2..8e46a4959c 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/NodeManager.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/NodeManager.java
@@ -173,9 +173,11 @@ public class NodeManager extends DevopsBase {
     }
 
     if (userIntent.providerType.equals(Common.CloudType.onprem)) {
-      NodeInstance node = NodeInstance.getByName(nodeTaskParam.nodeName);
+      // Instance may not be present if it is deleted from NodeInstance table after a release
+      // action. Node UUID is not available in 2.6.
+      Optional<NodeInstance> node = NodeInstance.maybeGetByName(nodeTaskParam.getNodeName());
       command.add("--node_metadata");
-      command.add(node.getDetailsJson());
+      command.add(node.isPresent() ? node.get().getDetailsJson() : "{}");
     }
     return command;
   }
@@ -257,7 +259,8 @@ public class NodeManager extends DevopsBase {
     if ((type == NodeCommandType.Provision
             || type == NodeCommandType.Destroy
             || type == NodeCommandType.Create
-            || type == NodeCommandType.Disk_Update)
+            || type == NodeCommandType.Disk_Update
+            || type == NodeCommandType.Update_Mounted_Disks)
         && keyInfo.sshUser != null) {
       subCommand.add("--ssh_user");
       subCommand.add(keyInfo.sshUser);
@@ -1251,6 +1254,7 @@ public class NodeManager extends DevopsBase {
 
   public ShellResponse nodeCommand(NodeCommandType type, NodeTaskParams nodeTaskParam) {
     Universe universe = Universe.getOrBadRequest(nodeTaskParam.universeUUID);
+    populateNodeUuidFromUniverse(universe, nodeTaskParam);
     List<String> commandArgs = new ArrayList<>();
     UserIntent userIntent = getUserIntentFromParams(nodeTaskParam);
     Path bootScriptFile = null;
@@ -1793,11 +1797,8 @@ public class NodeManager extends DevopsBase {
     return lowMemInstanceTypePrefixes.contains(instanceTypePrefix);
   }
 
-  private void addAdditionalInstanceTags(
-      Universe universe, NodeTaskParams nodeTaskParam, Map<String, String> tags) {
-    Customer customer = Customer.get(universe.customerId);
-    tags.put("customer-uuid", customer.uuid.toString());
-    tags.put("universe-uuid", universe.universeUUID.toString());
+  // Set the nodeUuid in nodeTaskParam if it is not set.
+  private void populateNodeUuidFromUniverse(Universe universe, NodeTaskParams nodeTaskParam) {
     if (nodeTaskParam.nodeUuid == null) {
       NodeDetails nodeDetails = universe.getNode(nodeTaskParam.nodeName);
       if (nodeDetails != null) {
@@ -1805,9 +1806,20 @@ public class NodeManager extends DevopsBase {
       }
     }
     if (nodeTaskParam.nodeUuid == null) {
-      // This is for backward compatibility where node UUID is not set in the Universe.
-      nodeTaskParam.nodeUuid = Util.generateNodeUUID(universe.universeUUID, nodeTaskParam.nodeName);
+      UserIntent userIntent = getUserIntentFromParams(universe, nodeTaskParam);
+      if (!Common.CloudType.onprem.equals(userIntent.providerType)) {
+        // This is for backward compatibility where node UUID is not set in the Universe.
+        nodeTaskParam.nodeUuid =
+            Util.generateNodeUUID(universe.universeUUID, nodeTaskParam.nodeName);
+      }
     }
+  }
+
+  private void addAdditionalInstanceTags(
+      Universe universe, NodeTaskParams nodeTaskParam, Map<String, String> tags) {
+    Customer customer = Customer.get(universe.customerId);
+    tags.put("customer-uuid", customer.uuid.toString());
+    tags.put("universe-uuid", universe.universeUUID.toString());
     tags.put("node-uuid", nodeTaskParam.nodeUuid.toString());
   }
 
diff --git a/managed/src/main/java/com/yugabyte/yw/common/NodeUniverseManager.java b/managed/src/main/java/com/yugabyte/yw/common/NodeUniverseManager.java
index 11474bbe96..e1c295b688 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/NodeUniverseManager.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/NodeUniverseManager.java
@@ -2,6 +2,7 @@ package com.yugabyte.yw.common;
 
 import com.google.inject.Singleton;
 import com.yugabyte.yw.commissioner.Common;
+import com.yugabyte.yw.common.concurrent.KeyLock;
 import com.yugabyte.yw.forms.UniverseDefinitionTaskParams;
 import com.yugabyte.yw.forms.UniverseDefinitionTaskParams.Cluster;
 import com.yugabyte.yw.forms.UniverseDefinitionTaskParams.UserIntent;
@@ -18,35 +19,41 @@ import java.util.concurrent.TimeUnit;
 
 @Singleton
 public class NodeUniverseManager extends DevopsBase {
-  public static final int YSQL_COMMAND_DEFAULT_TIMEOUT_SEC = 20;
+  public static final long YSQL_COMMAND_DEFAULT_TIMEOUT_SEC = TimeUnit.MINUTES.toSeconds(3);
   public static final String NODE_ACTION_SSH_SCRIPT = "bin/run_node_action.py";
   public static final String CERTS_DIR = "/yugabyte-tls-config";
   public static final String K8S_CERTS_DIR = "/opt/certs/yugabyte";
 
+  private final KeyLock<UUID> universeLock = new KeyLock<>();
+
   @Override
   protected String getCommandType() {
     return null;
   }
 
-  public synchronized ShellResponse downloadNodeLogs(
+  public ShellResponse downloadNodeLogs(
       NodeDetails node, Universe universe, String targetLocalFile) {
-    List<String> actionArgs = new ArrayList<>();
-    actionArgs.add("--yb_home_dir");
-    actionArgs.add(getYbHomeDir(node, universe));
-    actionArgs.add("--target_local_file");
-    actionArgs.add(targetLocalFile);
-    return executeNodeAction(UniverseNodeAction.DOWNLOAD_LOGS, universe, node, actionArgs);
+    universeLock.acquireLock(universe.getUniverseUUID());
+    try {
+      List<String> actionArgs = new ArrayList<>();
+      actionArgs.add("--yb_home_dir");
+      actionArgs.add(getYbHomeDir(node, universe));
+      actionArgs.add("--target_local_file");
+      actionArgs.add(targetLocalFile);
+      return executeNodeAction(UniverseNodeAction.DOWNLOAD_LOGS, universe, node, actionArgs);
+    } finally {
+      universeLock.releaseLock(universe.getUniverseUUID());
+    }
   }
 
-  public synchronized ShellResponse runCommand(
-      NodeDetails node, Universe universe, String command) {
+  public ShellResponse runCommand(NodeDetails node, Universe universe, String command) {
     List<String> actionArgs = new ArrayList<>();
     actionArgs.add("--command");
     actionArgs.add(command);
     return executeNodeAction(UniverseNodeAction.RUN_COMMAND, universe, node, actionArgs);
   }
 
-  public synchronized ShellResponse runYbAdminCommand(
+  public ShellResponse runYbAdminCommand(
       NodeDetails node, Universe universe, String ybAdminCommand, long timeoutSec) {
     List<String> command = new ArrayList<>();
     command.add("/usr/bin/timeout");
@@ -65,13 +72,13 @@ public class NodeUniverseManager extends DevopsBase {
     return runCommand(node, universe, String.join(" ", command));
   }
 
-  public synchronized ShellResponse runYsqlCommand(
+  public ShellResponse runYsqlCommand(
       NodeDetails node, Universe universe, String dbName, String ysqlCommand) {
     return runYsqlCommand(node, universe, dbName, ysqlCommand, YSQL_COMMAND_DEFAULT_TIMEOUT_SEC);
   }
 
-  public synchronized ShellResponse runYsqlCommand(
-      NodeDetails node, Universe universe, String dbName, String ysqlCommand, int timeoutSec) {
+  public ShellResponse runYsqlCommand(
+      NodeDetails node, Universe universe, String dbName, String ysqlCommand, long timeoutSec) {
     List<String> command = new ArrayList<>();
     command.add("timeout");
     command.add(String.valueOf(timeoutSec));
diff --git a/managed/src/main/java/com/yugabyte/yw/common/PlatformInstanceClientFactory.java b/managed/src/main/java/com/yugabyte/yw/common/PlatformInstanceClientFactory.java
deleted file mode 100644
index bbab0bfd22..0000000000
--- a/managed/src/main/java/com/yugabyte/yw/common/PlatformInstanceClientFactory.java
+++ /dev/null
@@ -1,23 +0,0 @@
-/*
- * Copyright 2021 YugaByte, Inc. and Contributors
- *
- * Licensed under the Polyform Free Trial License 1.0.0 (the "License"); you
- * may not use this file except in compliance with the License. You
- * may obtain a copy of the License at
- *
- * https://github.com/YugaByte/yugabyte-db/blob/master/licenses/POLYFORM-FREE-TRIAL-LICENSE-1.0.0.txt
- */
-
-package com.yugabyte.yw.common;
-
-import com.google.inject.Inject;
-import com.google.inject.Singleton;
-
-@Singleton
-public class PlatformInstanceClientFactory {
-  @Inject ApiHelper apiHelper;
-
-  public PlatformInstanceClient getClient(String clusterKey, String remoteAddress) {
-    return new PlatformInstanceClient(this.apiHelper, clusterKey, remoteAddress);
-  }
-}
diff --git a/managed/src/main/java/com/yugabyte/yw/common/ReleaseManager.java b/managed/src/main/java/com/yugabyte/yw/common/ReleaseManager.java
index eedd8acc26..e8331fa24c 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/ReleaseManager.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/ReleaseManager.java
@@ -18,8 +18,10 @@ import java.nio.file.PathMatcher;
 import java.nio.file.Paths;
 import java.util.ArrayList;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 import java.util.function.Predicate;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
@@ -196,14 +198,25 @@ public class ReleaseManager {
 
   public Map<String, String> getReleaseFiles(String releasesPath, Predicate<Path> fileFilter) {
     Map<String, String> fileMap = new HashMap<>();
+    Set<String> duplicateKeys = new HashSet<>();
     try {
-      fileMap =
-          Files.walk(Paths.get(releasesPath))
-              .filter(fileFilter)
-              .collect(
-                  Collectors.toMap(
-                      p -> p.getName(p.getNameCount() - 2).toString(),
-                      p -> p.toAbsolutePath().toString()));
+      Files.walk(Paths.get(releasesPath))
+          .filter(fileFilter)
+          .forEach(
+              p -> {
+                String key = p.getName(p.getNameCount() - 2).toString();
+                String value = p.toAbsolutePath().toString();
+                if (!fileMap.containsKey(key)) {
+                  fileMap.put(key, value);
+                } else if (!duplicateKeys.contains(key)) {
+                  LOG.warn(
+                      String.format(
+                          "Skipping %s - it contains multiple releases of same architecture type",
+                          key));
+                  duplicateKeys.add(key);
+                }
+              });
+      duplicateKeys.forEach(k -> fileMap.remove(k));
     } catch (IOException e) {
       LOG.error(e.getMessage());
     }
diff --git a/managed/src/main/java/com/yugabyte/yw/common/ShellProcessHandler.java b/managed/src/main/java/com/yugabyte/yw/common/ShellProcessHandler.java
index eee69da8d4..ef66b5bbe4 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/ShellProcessHandler.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/ShellProcessHandler.java
@@ -14,6 +14,8 @@ import static com.yugabyte.yw.common.ShellResponse.ERROR_CODE_EXECUTION_CANCELLE
 import static com.yugabyte.yw.common.ShellResponse.ERROR_CODE_GENERIC_ERROR;
 import static com.yugabyte.yw.common.ShellResponse.ERROR_CODE_SUCCESS;
 
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Joiner;
 import com.google.inject.Inject;
 import com.yugabyte.yw.common.config.RuntimeConfigFactory;
@@ -31,11 +33,14 @@ import java.util.UUID;
 import java.util.concurrent.TimeUnit;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
+import java.util.stream.Collectors;
 import javax.inject.Singleton;
+import org.apache.commons.text.StringSubstitutor;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.slf4j.Marker;
 import org.slf4j.MarkerFactory;
+import play.libs.Json;
 
 @Singleton
 public class ShellProcessHandler {
@@ -51,9 +56,11 @@ public class ShellProcessHandler {
   static final Pattern ANSIBLE_FAIL_PAT =
       Pattern.compile(
           "(ybops.common.exceptions.YBOpsRuntimeError: Runtime error: "
-              + "Playbook run.* )with args.* (failed with.*) ");
+              + "Playbook run.* )with args.* (failed with.*? [0-9]+)");
   static final Pattern ANSIBLE_FAILED_TASK_PAT =
       Pattern.compile("TASK.*?fatal.*?FAILED.*", Pattern.DOTALL);
+  static final Pattern PYTHON_ERROR_PAT =
+      Pattern.compile("(<yb-python-error>)(.*?)(</yb-python-error>)", Pattern.DOTALL);
   static final String ANSIBLE_IGNORING = "ignoring";
   static final String COMMAND_OUTPUT_LOGS_DELETE = "yb.logs.cmdOutputDelete";
   static final String YB_LOGS_MAX_MSG_SIZE = "yb.logs.max_msg_size";
@@ -150,51 +157,24 @@ public class ShellProcessHandler {
         if (logCmdOutput) {
           LOG.debug("Proc stdout for '{}' :", response.description);
         }
-        StringBuilder processOutput = new StringBuilder();
-        Marker fileOnly = MarkerFactory.getMarker("fileOnly");
-        Marker consoleOnly = MarkerFactory.getMarker("consoleOnly");
-
-        outputStream
-            .lines()
-            .forEach(
-                line -> {
-                  processOutput.append(line).append("\n");
-                  if (logCmdOutput) {
-                    LOG.debug(fileOnly, line);
-                  }
-                });
-
-        if (logCmdOutput && cloudLoggingEnabled && processOutput.length() > 0) {
-          LOG.debug(consoleOnly, processOutput.toString());
-        }
-
-        if (logCmdOutput) {
-          LOG.debug("Proc stderr for '{}' :", response.description);
+        String processOutput = getOutputLines(outputStream, logCmdOutput);
+        String processError = getOutputLines(errorStream, logCmdOutput);
+        try {
+          response.code = process.exitValue();
+        } catch (IllegalThreadStateException itse) {
+          response.code = ERROR_CODE_GENERIC_ERROR;
+          LOG.warn(
+              "Expected process to be shut down, marking this process as failed '{}'",
+              response.description,
+              itse);
         }
-        StringBuilder processError = new StringBuilder();
-        errorStream
-            .lines()
-            .forEach(
-                line -> {
-                  processError.append(line).append("\n");
-                  if (logCmdOutput) {
-                    LOG.debug(fileOnly, line);
-                  }
-                });
-
-        if (logCmdOutput && cloudLoggingEnabled && processError.length() > 0) {
-          LOG.debug(consoleOnly, processError.toString());
+        response.message = (response.code == ERROR_CODE_SUCCESS) ? processOutput : processError;
+        String specificErrMsg = getAnsibleErrMsg(response.code, processOutput, processError);
+        if (specificErrMsg == null) {
+          specificErrMsg = getPythonErrMsg(response.code, processOutput);
         }
-
-        response.code = process.exitValue();
-        response.message =
-            (response.code == ERROR_CODE_SUCCESS)
-                ? processOutput.toString().trim()
-                : processError.toString().trim();
-        String ansibleErrMsg =
-            getAnsibleErrMsg(response.code, processOutput.toString(), processError.toString());
-        if (ansibleErrMsg != null) {
-          response.message = ansibleErrMsg;
+        if (specificErrMsg != null) {
+          response.message = specificErrMsg;
         }
       }
     } catch (IOException | InterruptedException e) {
@@ -241,6 +221,26 @@ public class ShellProcessHandler {
     return response;
   }
 
+  private String getOutputLines(BufferedReader reader, boolean logOutput) {
+    Marker fileMarker = MarkerFactory.getMarker("fileOnly");
+    Marker consoleMarker = MarkerFactory.getMarker("consoleOnly");
+    String lines =
+        reader
+            .lines()
+            .peek(
+                line -> {
+                  if (logOutput) {
+                    LOG.debug(fileMarker, line);
+                  }
+                })
+            .collect(Collectors.joining("\n"))
+            .trim();
+    if (logOutput && cloudLoggingEnabled && lines.length() > 0) {
+      LOG.debug(consoleMarker, lines);
+    }
+    return lines;
+  }
+
   private long getMaxLogMsgSize() {
     return appConfig.getBytes(YB_LOGS_MAX_MSG_SIZE);
   }
@@ -338,4 +338,25 @@ public class ShellProcessHandler {
     }
     return result;
   }
+
+  @VisibleForTesting
+  static String getPythonErrMsg(int code, String stdout) {
+    if (stdout == null || code == ERROR_CODE_SUCCESS) return null;
+
+    try {
+      Matcher matcher = PYTHON_ERROR_PAT.matcher(stdout);
+      if (matcher.find()) {
+        Map<String, String> values =
+            Json.mapper()
+                .readValue(matcher.group(2).trim(), new TypeReference<Map<String, String>>() {});
+        StringSubstitutor substitutor =
+            new StringSubstitutor(values).setEnableUndefinedVariableException(true);
+        // Flexible template to add more fields or change format.
+        return substitutor.replace("${type}: ${message}");
+      }
+    } catch (Exception e) {
+      LOG.error("Error occurred in processing command output", e);
+    }
+    return null;
+  }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/common/ShellResponse.java b/managed/src/main/java/com/yugabyte/yw/common/ShellResponse.java
index e79bfdfc5a..f05a95b57c 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/ShellResponse.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/ShellResponse.java
@@ -2,10 +2,13 @@
 package com.yugabyte.yw.common;
 
 import java.util.List;
+import java.util.concurrent.CancellationException;
 import lombok.Data;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.commons.lang3.StringUtils;
 
 @Data
+@Slf4j
 public class ShellResponse {
   // Some known error codes for shell process.
   public static final int ERROR_CODE_SUCCESS = 0;
@@ -31,4 +34,29 @@ public class ShellResponse {
   public boolean isSuccess() {
     return code == ERROR_CODE_SUCCESS;
   }
+
+  // Call this method to process or validate the exit code if required.
+  public ShellResponse processErrors() {
+    return processErrors(null);
+  }
+
+  // Call this method to process or validate the exit code with custom error message if required.
+  public ShellResponse processErrors(String errorMessage) {
+    if (code != ERROR_CODE_SUCCESS) {
+      String formatted = StringUtils.isBlank(errorMessage) ? "Error occurred" : errorMessage;
+      try {
+        switch (code) {
+          case ERROR_CODE_EXECUTION_CANCELLED:
+            formatted = String.format("%s. Command is cancelled.", formatted);
+            throw new CancellationException(formatted);
+          default:
+            formatted = String.format("%s. Output: %s", formatted, message);
+            throw new RuntimeException(formatted);
+        }
+      } finally {
+        log.error("{}, {}", formatted, toString());
+      }
+    }
+    return this;
+  }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/common/Util.java b/managed/src/main/java/com/yugabyte/yw/common/Util.java
index b3cd9ce7ac..0969017d59 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/Util.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/Util.java
@@ -10,7 +10,6 @@ import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.node.ArrayNode;
 import com.google.common.annotations.VisibleForTesting;
-import com.yugabyte.yw.commissioner.tasks.UniverseTaskBase;
 import com.yugabyte.yw.common.config.impl.RuntimeConfig;
 import com.yugabyte.yw.forms.UniverseDefinitionTaskParams;
 import com.yugabyte.yw.forms.UniverseDefinitionTaskParams.Cluster;
@@ -23,8 +22,8 @@ import java.io.File;
 import java.io.FileInputStream;
 import java.io.FileNotFoundException;
 import java.io.FileWriter;
-import java.io.InputStream;
 import java.io.IOException;
+import java.io.InputStream;
 import java.math.BigDecimal;
 import java.net.InetAddress;
 import java.net.InetSocketAddress;
@@ -47,8 +46,8 @@ import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.UUID;
 import java.util.TimeZone;
+import java.util.UUID;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
diff --git a/managed/src/main/java/com/yugabyte/yw/common/alerts/AlertConfigurationWriter.java b/managed/src/main/java/com/yugabyte/yw/common/alerts/AlertConfigurationWriter.java
index 1950667107..dbac442111 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/alerts/AlertConfigurationWriter.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/alerts/AlertConfigurationWriter.java
@@ -227,9 +227,8 @@ public class AlertConfigurationWriter {
       metricService.setOkStatusMetric(
           buildMetricTemplate(PlatformMetrics.ALERT_MAINTENANCE_WINDOW_PROCESSOR_STATUS));
     } catch (Exception e) {
-      metricService.setStatusMetric(
-          buildMetricTemplate(PlatformMetrics.ALERT_MAINTENANCE_WINDOW_PROCESSOR_STATUS),
-          "Error processing maintenance windows: " + e.getMessage());
+      metricService.setFailureStatusMetric(
+          buildMetricTemplate(PlatformMetrics.ALERT_MAINTENANCE_WINDOW_PROCESSOR_STATUS));
       log.error("Error processing maintenance windows:", e);
     }
   }
@@ -271,9 +270,8 @@ public class AlertConfigurationWriter {
       metricService.setOkStatusMetric(
           buildMetricTemplate(PlatformMetrics.ALERT_CONFIG_WRITER_STATUS));
     } catch (Exception e) {
-      metricService.setStatusMetric(
-          buildMetricTemplate(PlatformMetrics.ALERT_CONFIG_WRITER_STATUS),
-          "Error syncing alert definition configs " + e.getMessage());
+      metricService.setFailureStatusMetric(
+          buildMetricTemplate(PlatformMetrics.ALERT_CONFIG_WRITER_STATUS));
       log.error("Error syncing alert definition configs", e);
     }
   }
diff --git a/managed/src/main/java/com/yugabyte/yw/common/alerts/QueryAlerts.java b/managed/src/main/java/com/yugabyte/yw/common/alerts/QueryAlerts.java
index c1ee36a524..56415a344b 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/alerts/QueryAlerts.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/alerts/QueryAlerts.java
@@ -129,9 +129,8 @@ public class QueryAlerts {
         resolveAlerts(activeAlertsUuids);
         metricService.setOkStatusMetric(buildMetricTemplate(PlatformMetrics.ALERT_QUERY_STATUS));
       } catch (Exception e) {
-        metricService.setStatusMetric(
-            buildMetricTemplate(PlatformMetrics.ALERT_QUERY_STATUS),
-            "Error querying for alerts: " + e.getMessage());
+        metricService.setFailureStatusMetric(
+            buildMetricTemplate(PlatformMetrics.ALERT_QUERY_STATUS));
         log.error("Error querying for alerts", e);
       }
       alertManager.sendNotifications();
diff --git a/managed/src/main/java/com/yugabyte/yw/common/alerts/SmtpData.java b/managed/src/main/java/com/yugabyte/yw/common/alerts/SmtpData.java
index 585468bc7d..377247303e 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/alerts/SmtpData.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/alerts/SmtpData.java
@@ -59,8 +59,7 @@ public class SmtpData {
         + smtpServer
         + ", smtpPort="
         + smtpPort
-        + ", emailFrom="
-        + emailFrom
+        + ", emailFrom=REDACTED"
         + ", smtpUsername="
         + smtpUsername
         + ", useSSL="
diff --git a/managed/src/main/java/com/yugabyte/yw/common/config/impl/DelegatingConfig.java b/managed/src/main/java/com/yugabyte/yw/common/config/impl/DelegatingConfig.java
index fac708d485..1b67303b61 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/config/impl/DelegatingConfig.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/config/impl/DelegatingConfig.java
@@ -11,6 +11,7 @@
 package com.yugabyte.yw.common.config.impl;
 
 import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
 import com.typesafe.config.ConfigList;
 import com.typesafe.config.ConfigMemorySize;
 import com.typesafe.config.ConfigMergeable;
@@ -65,6 +66,11 @@ public class DelegatingConfig implements Config {
     delegate.set(delegate().withValue(path, newValue));
   }
 
+  protected void setObjInternal(String path, String strObj) {
+    ConfigValue configValue = ConfigFactory.parseString(path + "=" + strObj).getValue(path);
+    delegate.set(delegate().withValue(path, configValue));
+  }
+
   protected void deleteValueInternal(String path) {
     delegate.set(delegate().withoutPath(path));
   }
@@ -129,10 +135,9 @@ public class DelegatingConfig implements Config {
     throw new UnsupportedOperationException();
   }
 
-  @Deprecated
   @Override
   public ConfigValue getValue(String path) {
-    throw new UnsupportedOperationException();
+    return delegate().getValue(path);
   }
 
   @Deprecated
@@ -365,4 +370,9 @@ public class DelegatingConfig implements Config {
   public List<Duration> getDurationList(String path) {
     return delegate().getDurationList(path);
   }
+
+  @Override
+  public String toString() {
+    return SettableRuntimeConfigFactory.toRedactedString(delegate());
+  }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/common/config/impl/RuntimeConfig.java b/managed/src/main/java/com/yugabyte/yw/common/config/impl/RuntimeConfig.java
index ad0169503f..273267a5c8 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/config/impl/RuntimeConfig.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/config/impl/RuntimeConfig.java
@@ -37,40 +37,56 @@ public class RuntimeConfig<M extends Model> extends DelegatingConfig {
   }
 
   RuntimeConfig(M scope, Config config) {
-    super(config);
+    super(config.resolve());
     this.scope = scope;
   }
 
   /**
-   * @return modify single path in underlying scoped config in the database and return modified
-   *     config view.
+   * @return modify single leaf path as any ref in underlying scoped config in the database and
+   *     return modified config view.
    */
-  public RuntimeConfig<M> setValue(String path, String value) {
+  public RuntimeConfig<M> setValue(String path, String strValue) {
+    return setValueOrObj(path, strValue, false);
+  }
+
+  /**
+   * @return modify single leaf path as HOCON string in underlying scoped config in the database and
+   *     return modified config view.
+   */
+  public RuntimeConfig<M> setObject(String path, String strValue) {
+    return setValueOrObj(path, strValue, true);
+  }
+
+  private RuntimeConfig<M> setValueOrObj(String path, String strValue, boolean isObject) {
     if (scope == null) {
-      RuntimeConfigEntry.upsertGlobal(path, value);
+      RuntimeConfigEntry.upsertGlobal(path, strValue);
     } else if (scope instanceof Customer) {
-      RuntimeConfigEntry.upsert((Customer) scope, path, value);
+      RuntimeConfigEntry.upsert((Customer) scope, path, strValue);
     } else if (scope instanceof Universe) {
-      RuntimeConfigEntry.upsert((Universe) scope, path, value);
+      RuntimeConfigEntry.upsert((Universe) scope, path, strValue);
     } else if (scope instanceof Provider) {
-      RuntimeConfigEntry.upsert((Provider) scope, path, value);
+      RuntimeConfigEntry.upsert((Provider) scope, path, strValue);
     } else {
       throw new UnsupportedOperationException("Unsupported Scope: " + scope);
     }
-    super.setValueInternal(path, ConfigValueFactory.fromAnyRef(value));
+    if (isObject) {
+      super.setObjInternal(path, strValue);
+    } else {
+      super.setValueInternal(path, ConfigValueFactory.fromAnyRef(strValue));
+    }
     LOG.trace("After setValue {}", delegate());
     return this;
   }
 
   public RuntimeConfig<M> deleteEntry(String path) {
     if (scope == null) {
-      RuntimeConfigEntry.get(GLOBAL_SCOPE_UUID, path).delete();
+      RuntimeConfigEntry.getOrBadRequest(GLOBAL_SCOPE_UUID, path).delete();
     } else if (scope instanceof Customer) {
-      RuntimeConfigEntry.get(((Customer) scope).uuid, path).delete();
+      RuntimeConfigEntry.getOrBadRequest(((Customer) scope).uuid, path).delete();
     } else if (scope instanceof Universe) {
-      RuntimeConfigEntry.get(((Universe) scope).universeUUID, path).delete();
+      RuntimeConfigEntry.getOrBadRequest(((Universe) scope).universeUUID, path).delete();
     } else if (scope instanceof Provider) {
-      RuntimeConfigEntry.get(((Provider) scope).uuid, path).delete();
+      RuntimeConfigEntry.getOrBadRequest(((Provider) scope).uuid, path).delete();
     } else {
       throw new UnsupportedOperationException("Unsupported Scope: " + scope);
     }
diff --git a/managed/src/main/java/com/yugabyte/yw/common/config/impl/SettableRuntimeConfigFactory.java b/managed/src/main/java/com/yugabyte/yw/common/config/impl/SettableRuntimeConfigFactory.java
index d8647c307e..aeb7024b1e 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/config/impl/SettableRuntimeConfigFactory.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/config/impl/SettableRuntimeConfigFactory.java
@@ -1,5 +1,5 @@
 /*
- * Copyright 2021 YugaByte, Inc. and Contributors
+ * Copyright 2022 YugaByte, Inc. and Contributors
  *
  * Licensed under the Polyform Free Trial License 1.0.0 (the "License"); you
  * may not use this file except in compliance with the License. You
@@ -15,7 +15,9 @@ import static com.yugabyte.yw.models.ScopedRuntimeConfig.GLOBAL_SCOPE_UUID;
 import com.google.common.annotations.VisibleForTesting;
 import com.typesafe.config.Config;
 import com.typesafe.config.ConfigFactory;
+import com.typesafe.config.ConfigParseOptions;
 import com.yugabyte.yw.common.config.RuntimeConfigFactory;
+import com.yugabyte.yw.common.ybflyway.YBFlywayInit;
 import com.yugabyte.yw.models.Customer;
 import com.yugabyte.yw.models.Provider;
 import com.yugabyte.yw.models.RuntimeConfigEntry;
@@ -23,57 +25,81 @@ import com.yugabyte.yw.models.Universe;
 import io.ebean.Model;
 import java.util.Map;
 import java.util.UUID;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
 import javax.inject.Inject;
 import javax.inject.Singleton;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import play.db.ebean.EbeanDynamicEvolutions;
+import play.libs.Json;
 
 /** Factory to create RuntimeConfig for various scopes */
 @Singleton
 public class SettableRuntimeConfigFactory implements RuntimeConfigFactory {
   private static final Logger LOG = LoggerFactory.getLogger(SettableRuntimeConfigFactory.class);
 
+  // For example, anything just email, or ending with .email, _email or -email matches.
+  private static final Pattern SENSITIVE_CONFIG_NAME_PAT =
+      Pattern.compile("(^|\\.|[_\\-])(email|password|server)$");
+
+  @VisibleForTesting
+  static final String RUNTIME_CONFIG_INCLUDED_OBJECTS = "runtime_config.included_objects";
+
   private final Config appConfig;
 
+  // We need to do this because appConfig is preResolved by playFramework
+  // So setting references to global or universe scoped config in reference.conf or application.conf
+  // wont resolve to unexpected.
+  // This helps us avoid unnecessary migrations of config keys.
+  private static final Config UNRESOLVED_STATIC_CONFIG =
+      ConfigFactory.parseString(
+          "\n" + "yb {\n" + "  upgrade.vmImage = ${yb.cloud.enabled}\n" + "}\n");
+
   @Inject
   public SettableRuntimeConfigFactory(
-      Config appConfig, EbeanDynamicEvolutions ebeanDynamicEvolutions) {
+      Config appConfig, EbeanDynamicEvolutions ebeanDynamicEvolutions, YBFlywayInit ybFlywayInit) {
     this.appConfig = appConfig;
   }
 
   /** @return A RuntimeConfig instance for a given scope */
   @Override
   public RuntimeConfig<Customer> forCustomer(Customer customer) {
-    Config config =
-        getConfigForScope(customer.uuid, "Scoped Config (" + customer + ")")
-            .withFallback(globalConfig());
+    RuntimeConfig<Customer> config =
+        new RuntimeConfig<>(
+            customer,
+            getConfigForScope(customer.uuid, "Scoped Config (" + customer + ")")
+                .withFallback(globalConfig()));
     LOG.trace("forCustomer {}: {}", customer.uuid, config);
-    return new RuntimeConfig<>(customer, config);
+    return config;
   }
 
   /** @return A RuntimeConfig instance for a given scope */
   @Override
   public RuntimeConfig<Universe> forUniverse(Universe universe) {
     Customer customer = Customer.get(universe.customerId);
-    Config config =
-        getConfigForScope(universe.universeUUID, "Scoped Config (" + universe + ")")
-            .withFallback(getConfigForScope(customer.uuid, "Scoped Config (" + customer + ")"))
-            .withFallback(globalConfig());
+    RuntimeConfig<Universe> config =
+        new RuntimeConfig<>(
+            universe,
+            getConfigForScope(universe.universeUUID, "Scoped Config (" + universe + ")")
+                .withFallback(getConfigForScope(customer.uuid, "Scoped Config (" + customer + ")"))
+                .withFallback(globalConfig()));
     LOG.trace("forUniverse {}: {}", universe.universeUUID, config);
-    return new RuntimeConfig<>(universe, config);
+    return config;
   }
 
   /** @return A RuntimeConfig instance for a given scope */
   @Override
   public RuntimeConfig<Provider> forProvider(Provider provider) {
     Customer customer = Customer.get(provider.customerUUID);
-    Config config =
-        getConfigForScope(provider.uuid, "Scoped Config (" + provider + ")")
-            .withFallback(getConfigForScope(customer.uuid, "Scoped Config (" + customer + ")"))
-            .withFallback(globalConfig());
+    RuntimeConfig<Provider> config =
+        new RuntimeConfig<>(
+            provider,
+            getConfigForScope(provider.uuid, "Scoped Config (" + provider + ")")
+                .withFallback(getConfigForScope(customer.uuid, "Scoped Config (" + customer + ")"))
+                .withFallback(globalConfig()));
     LOG.trace("forProvider {}: {}", provider.uuid, config);
-    return new RuntimeConfig<>(provider, config);
+    return config;
   }
 
   /** @return A RuntimeConfig instance for a GLOBAL_SCOPE */
@@ -90,16 +116,60 @@ public class SettableRuntimeConfigFactory implements RuntimeConfigFactory {
   private Config globalConfig() {
     Config config =
         getConfigForScope(GLOBAL_SCOPE_UUID, "Global Runtime Config (" + GLOBAL_SCOPE_UUID + ")")
+            .withFallback(UNRESOLVED_STATIC_CONFIG)
             .withFallback(appConfig);
-    LOG.trace("globalConfig : {}", config);
+    if (LOG.isTraceEnabled()) {
+      LOG.trace("globalConfig : {}", toRedactedString(config));
+    }
     return config;
   }
 
   @VisibleForTesting
   Config getConfigForScope(UUID scope, String description) {
     Map<String, String> values = RuntimeConfigEntry.getAsMapForScope(scope);
-    Config config = ConfigFactory.parseMap(values, description);
-    LOG.trace("Read from DB for {}: {}", description, config);
+    return toConfig(description, values);
+  }
+
+  private Config toConfig(String description, Map<String, String> values) {
+    String confStr = toConfigString(values);
+    Config config =
+        ConfigFactory.parseString(
+            confStr, ConfigParseOptions.defaults().setOriginDescription(description));
+
+    LOG.trace("Read from DB for {}: {}", description, toRedactedString(config));
     return config;
   }
+
+  private String toConfigString(Map<String, String> values) {
+    return values
+        .entrySet()
+        .stream()
+        .map(entry -> entry.getKey() + "=" + maybeQuote(entry))
+        .collect(Collectors.joining("\n"));
+  }
+
+  private String maybeQuote(Map.Entry<String, String> entry) {
+    final boolean isObject =
+        appConfig.getStringList(RUNTIME_CONFIG_INCLUDED_OBJECTS).contains(entry.getKey());
+    if (isObject || entry.getValue().startsWith("\"")) {
+      // No need to escape
+      return entry.getValue();
+    }
+    return Json.stringify(Json.toJson(entry.getValue()));
+  }
+
+  @VisibleForTesting
+  static String toRedactedString(Config config) {
+    return config
+        .entrySet()
+        .stream()
+        .map(
+            entry -> {
+              if (SENSITIVE_CONFIG_NAME_PAT.matcher(entry.getKey()).find()) {
+                return entry.getKey() + "=REDACTED";
+              }
+              return entry.getKey() + "=" + entry.getValue();
+            })
+        .collect(Collectors.joining(", "));
+  }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/common/PlatformInstanceClient.java b/managed/src/main/java/com/yugabyte/yw/common/ha/PlatformInstanceClient.java
similarity index 93%
rename from managed/src/main/java/com/yugabyte/yw/common/PlatformInstanceClient.java
rename to managed/src/main/java/com/yugabyte/yw/common/ha/PlatformInstanceClient.java
index 2d117ec30c..3556385d16 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/PlatformInstanceClient.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/ha/PlatformInstanceClient.java
@@ -1,15 +1,14 @@
 /*
- * Copyright 2021 YugaByte, Inc. and Contributors
+ * Copyright 2022 YugaByte, Inc. and Contributors
  *
  * Licensed under the Polyform Free Trial License 1.0.0 (the "License"); you
  * may not use this file except in compliance with the License. You
  * may obtain a copy of the License at
  *
- * https://github.com/YugaByte/yugabyte-db/blob/master/licenses/POLYFORM-FREE-TRIAL-LICENSE-1.0.0
- * .txt
+ * http://github.com/YugaByte/yugabyte-db/blob/master/licenses/POLYFORM-FREE-TRIAL-LICENSE-1.0.0.txt
  */
 
-package com.yugabyte.yw.common;
+package com.yugabyte.yw.common.ha;
 
 import static scala.compat.java8.JFunction.func;
 
@@ -18,7 +17,9 @@ import akka.stream.javadsl.Source;
 import akka.util.ByteString;
 import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.node.ObjectNode;
+import com.google.common.annotations.VisibleForTesting;
 import com.google.common.collect.ImmutableMap;
+import com.yugabyte.yw.common.ApiHelper;
 import com.yugabyte.yw.controllers.HAAuthenticator;
 import com.yugabyte.yw.controllers.ReverseInternalHAController;
 import com.yugabyte.yw.models.HighAvailabilityConfig;
@@ -26,6 +27,7 @@ import java.io.File;
 import java.util.List;
 import java.util.Map;
 import java.util.stream.Collectors;
+import lombok.Getter;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import play.libs.Json;
@@ -37,6 +39,7 @@ public class PlatformInstanceClient {
 
   private static final Logger LOG = LoggerFactory.getLogger(PlatformInstanceClient.class);
 
+  @Getter(onMethod_ = {@VisibleForTesting})
   private final ApiHelper apiHelper;
 
   private final String remoteAddress;
diff --git a/managed/src/main/java/com/yugabyte/yw/common/ha/PlatformInstanceClientFactory.java b/managed/src/main/java/com/yugabyte/yw/common/ha/PlatformInstanceClientFactory.java
new file mode 100644
index 0000000000..0b2bb4c2f4
--- /dev/null
+++ b/managed/src/main/java/com/yugabyte/yw/common/ha/PlatformInstanceClientFactory.java
@@ -0,0 +1,80 @@
+/*
+ * Copyright 2022 YugaByte, Inc. and Contributors
+ *
+ * Licensed under the Polyform Free Trial License 1.0.0 (the "License"); you
+ * may not use this file except in compliance with the License. You
+ * may obtain a copy of the License at
+ *
+ * http://github.com/YugaByte/yugabyte-db/blob/master/licenses/POLYFORM-FREE-TRIAL-LICENSE-1.0.0.txt
+ */
+
+package com.yugabyte.yw.common.ha;
+
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
+import com.typesafe.config.ConfigRenderOptions;
+import com.typesafe.config.ConfigValue;
+import com.yugabyte.yw.common.ApiHelper;
+import com.yugabyte.yw.common.CustomWsClientFactory;
+import com.yugabyte.yw.common.config.RuntimeConfigFactory;
+import java.io.IOException;
+import lombok.extern.slf4j.Slf4j;
+import play.libs.ws.WSClient;
+
+@Singleton
+@Slf4j
+public class PlatformInstanceClientFactory {
+
+  private final CustomWsClientFactory customWsClientFactory;
+  private final RuntimeConfigFactory runtimeConfigFactory;
+  private final ApiHelper fallbackApiHelper;
+  private WSClient customWsClient = null;
+
+  @Inject
+  public PlatformInstanceClientFactory(
+      ApiHelper defaultApiHelper,
+      CustomWsClientFactory customWsClientFactory,
+      RuntimeConfigFactory runtimeConfigFactory) {
+    this.fallbackApiHelper = defaultApiHelper;
+    this.customWsClientFactory = customWsClientFactory;
+    this.runtimeConfigFactory = runtimeConfigFactory;
+  }
+
+  public synchronized void refreshWsClient(String haWsConfigPath) {
+    ConfigValue haWsOverrides = runtimeConfigFactory.globalRuntimeConf().getValue(haWsConfigPath);
+    log.info(
+        "Creating ws client with config override: {}",
+        haWsOverrides.render(ConfigRenderOptions.concise()));
+    Config customWsConfig =
+        ConfigFactory.empty()
+            .withValue("play.ws", haWsOverrides)
+            .withFallback(runtimeConfigFactory.staticApplicationConf())
+            .withOnlyPath("play.ws");
+    // Enable trace level logging to debug actual config value being resolved:
+    log.trace("Creating ws client with config: {}", customWsConfig.root().render());
+    closePreviousClient(customWsClient);
+    customWsClient = customWsClientFactory.forCustomConfig(customWsConfig);
+  }
+
+  private void closePreviousClient(WSClient previousWsClient) {
+    if (previousWsClient != null) {
+      try {
+        previousWsClient.close();
+      } catch (IOException e) {
+        log.warn("Exception while closing wsClient. Ignored", e);
+      }
+    }
+  }
+
+  public PlatformInstanceClient getClient(String clusterKey, String remoteAddress) {
+    if (customWsClient == null) {
+      log.info("Using fallbackApiHelper");
+      return new PlatformInstanceClient(fallbackApiHelper, clusterKey, remoteAddress);
+    } else {
+      log.info("Using customApiHelper");
+      return new PlatformInstanceClient(new ApiHelper(customWsClient), clusterKey, remoteAddress);
+    }
+  }
+}
diff --git a/managed/src/main/java/com/yugabyte/yw/common/ha/PlatformReplicationHelper.java b/managed/src/main/java/com/yugabyte/yw/common/ha/PlatformReplicationHelper.java
index 5321ed0003..7fb176454a 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/ha/PlatformReplicationHelper.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/ha/PlatformReplicationHelper.java
@@ -16,8 +16,6 @@ import com.google.common.annotations.VisibleForTesting;
 import com.google.inject.Inject;
 import com.google.inject.Singleton;
 import com.yugabyte.yw.common.ApiHelper;
-import com.yugabyte.yw.common.PlatformInstanceClient;
-import com.yugabyte.yw.common.PlatformInstanceClientFactory;
 import com.yugabyte.yw.common.ShellProcessHandler;
 import com.yugabyte.yw.common.ShellResponse;
 import com.yugabyte.yw.common.Util;
diff --git a/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricLabelsBuilder.java b/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricLabelsBuilder.java
index c08c947467..b581ed981a 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricLabelsBuilder.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricLabelsBuilder.java
@@ -9,15 +9,15 @@
  */
 package com.yugabyte.yw.common.metrics;
 
+import com.yugabyte.yw.models.AlertChannel;
 import com.yugabyte.yw.models.AlertDefinitionLabel;
 import com.yugabyte.yw.models.AlertLabel;
-import com.yugabyte.yw.models.AlertChannel;
 import com.yugabyte.yw.models.Customer;
-import com.yugabyte.yw.models.MetricLabel;
 import com.yugabyte.yw.models.Universe;
 import com.yugabyte.yw.models.helpers.KnownAlertLabels;
-import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
 import java.util.stream.Collectors;
 
 public class MetricLabelsBuilder {
@@ -29,59 +29,61 @@ public class MetricLabelsBuilder {
     KnownAlertLabels.SOURCE_TYPE.labelName()
   };
 
-  private final List<MetricLabel> labels = new ArrayList<>();
+  private final Map<String, String> labels = new HashMap<>();
 
   public static MetricLabelsBuilder create() {
     return new MetricLabelsBuilder();
   }
 
   public MetricLabelsBuilder appendUniverse(Universe universe) {
-    labels.add(new MetricLabel(KnownAlertLabels.UNIVERSE_UUID, universe.universeUUID.toString()));
-    labels.add(new MetricLabel(KnownAlertLabels.UNIVERSE_NAME, universe.name));
+    labels.put(KnownAlertLabels.UNIVERSE_UUID.labelName(), universe.universeUUID.toString());
+    labels.put(KnownAlertLabels.UNIVERSE_NAME.labelName(), universe.name);
     return this;
   }
 
   public MetricLabelsBuilder appendSource(Universe universe) {
     appendUniverse(universe);
-    labels.add(new MetricLabel(KnownAlertLabels.SOURCE_UUID, universe.universeUUID.toString()));
-    labels.add(new MetricLabel(KnownAlertLabels.SOURCE_NAME, universe.name));
-    labels.add(new MetricLabel(KnownAlertLabels.SOURCE_TYPE, "universe"));
+    labels.put(KnownAlertLabels.SOURCE_UUID.labelName(), universe.universeUUID.toString());
+    labels.put(KnownAlertLabels.SOURCE_NAME.labelName(), universe.name);
+    labels.put(KnownAlertLabels.SOURCE_TYPE.labelName(), "universe");
     return this;
   }
 
   public MetricLabelsBuilder appendSource(Customer customer) {
-    labels.add(new MetricLabel(KnownAlertLabels.SOURCE_UUID, customer.getUuid().toString()));
-    labels.add(new MetricLabel(KnownAlertLabels.SOURCE_NAME, customer.name));
-    labels.add(new MetricLabel(KnownAlertLabels.SOURCE_TYPE, "customer"));
+    labels.put(KnownAlertLabels.SOURCE_UUID.labelName(), customer.getUuid().toString());
+    labels.put(KnownAlertLabels.SOURCE_NAME.labelName(), customer.name);
+    labels.put(KnownAlertLabels.SOURCE_TYPE.labelName(), "customer");
     return this;
   }
 
   public MetricLabelsBuilder appendSource(AlertChannel channel) {
-    labels.add(new MetricLabel(KnownAlertLabels.SOURCE_UUID, channel.getUuid().toString()));
-    labels.add(new MetricLabel(KnownAlertLabels.SOURCE_NAME, channel.getName()));
-    labels.add(new MetricLabel(KnownAlertLabels.SOURCE_TYPE, "alert channel"));
+    labels.put(KnownAlertLabels.SOURCE_UUID.labelName(), channel.getUuid().toString());
+    labels.put(KnownAlertLabels.SOURCE_NAME.labelName(), channel.getName());
+    labels.put(KnownAlertLabels.SOURCE_TYPE.labelName(), "alert channel");
     return this;
   }
 
   public List<AlertDefinitionLabel> getDefinitionLabels() {
     return labels
+        .entrySet()
         .stream()
-        .map(label -> new AlertDefinitionLabel(label.getName(), label.getValue()))
+        .map(label -> new AlertDefinitionLabel(label.getKey(), label.getValue()))
         .collect(Collectors.toList());
   }
 
   public List<AlertLabel> getAlertLabels() {
     return labels
+        .entrySet()
         .stream()
-        .map(label -> new AlertLabel(label.getName(), label.getValue()))
+        .map(label -> new AlertLabel(label.getKey(), label.getValue()))
         .collect(Collectors.toList());
   }
 
-  public List<MetricLabel> getMetricLabels() {
+  public Map<String, String> getMetricLabels() {
     return labels;
   }
 
   public String[] getPrometheusValues() {
-    return labels.stream().map(MetricLabel::getValue).toArray(String[]::new);
+    return labels.values().toArray(new String[0]);
   }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricSaveGroup.java b/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricSaveGroup.java
index 856dd20e9d..5dd2f4892c 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricSaveGroup.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricSaveGroup.java
@@ -20,5 +20,5 @@ import lombok.Value;
 @Builder
 public class MetricSaveGroup {
   @Singular List<Metric> metrics;
-  @Singular List<MetricFilter> cleanMetricFilters;
+  MetricFilter cleanMetricFilter;
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricService.java b/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricService.java
index 752fad342b..8da7648e15 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricService.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricService.java
@@ -9,83 +9,38 @@
  */
 package com.yugabyte.yw.common.metrics;
 
-import static com.yugabyte.yw.models.Metric.createQueryByFilter;
-import static com.yugabyte.yw.models.helpers.CommonUtils.getDurationSeconds;
 import static com.yugabyte.yw.models.helpers.CommonUtils.nowPlusWithoutMillis;
-import static com.yugabyte.yw.models.helpers.EntityOperation.CREATE;
-import static com.yugabyte.yw.models.helpers.EntityOperation.UPDATE;
 import static play.mvc.Http.Status.BAD_REQUEST;
 
-import com.google.common.collect.ImmutableList;
 import com.yugabyte.yw.common.PlatformServiceException;
-import com.yugabyte.yw.common.concurrent.MultiKeyLock;
 import com.yugabyte.yw.models.Customer;
 import com.yugabyte.yw.models.Metric;
 import com.yugabyte.yw.models.MetricKey;
-import com.yugabyte.yw.models.MetricLabel;
-import com.yugabyte.yw.models.MetricSourceKey;
 import com.yugabyte.yw.models.Universe;
 import com.yugabyte.yw.models.filters.MetricFilter;
 import com.yugabyte.yw.models.filters.MetricFilter.MetricFilterBuilder;
-import com.yugabyte.yw.models.helpers.CommonUtils;
-import com.yugabyte.yw.models.helpers.EntityOperation;
-import com.yugabyte.yw.models.helpers.KnownAlertLabels;
+import com.yugabyte.yw.models.helpers.MetricSourceState;
 import com.yugabyte.yw.models.helpers.PlatformMetrics;
-import io.ebean.annotation.Transactional;
-import io.prometheus.client.CollectorRegistry;
-import io.prometheus.client.Counter;
 import java.time.temporal.ChronoUnit;
-import java.util.Collection;
+import java.util.ArrayList;
 import java.util.Collections;
-import java.util.Comparator;
-import java.util.Date;
 import java.util.List;
-import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 import java.util.UUID;
 import java.util.concurrent.TimeUnit;
-import java.util.function.Function;
 import java.util.stream.Collectors;
-import java.util.stream.Stream;
 import javax.inject.Inject;
 import javax.inject.Singleton;
 import lombok.extern.slf4j.Slf4j;
-import org.apache.commons.collections.CollectionUtils;
 import org.apache.commons.lang3.StringUtils;
 
 @Singleton
 @Slf4j
 public class MetricService {
   public static final long DEFAULT_METRIC_EXPIRY_SEC = TimeUnit.DAYS.toSeconds(10);
-  public static final double EXPIRY_TIME_UPDATE_COEFFICIENT = 1.1;
   public static final double STATUS_OK = 1D;
   public static final double STATUS_NOT_OK = 0D;
 
-  private static final Comparator<MetricSourceKey> METRIC_SOURCE_KEY_COMPARATOR =
-      Comparator.comparing(MetricSourceKey::getName)
-          .thenComparing(MetricSourceKey::getCustomerUuid)
-          .thenComparing(MetricSourceKey::getSourceUuid);
-  private static final Comparator<MetricKey> METRIC_KEY_COMPARATOR =
-      Comparator.comparing(MetricKey::getSourceKey, METRIC_SOURCE_KEY_COMPARATOR)
-          .thenComparing(
-              MetricKey::getSourceLabels, Comparator.nullsFirst(Comparator.naturalOrder()));
-  private final MultiKeyLock<MetricKey> metricKeyLock = new MultiKeyLock<>(METRIC_KEY_COMPARATOR);
-
-  // Counter names
-  private static final String METRIC_UPDATE_COUNT = "ybp_metric_update_count";
-  private static final String METRIC_DELETE_COUNT = "ybp_metric_delete_count";
-
-  // Counters
-  private static final Counter METRIC_UPDATE_COUNTER =
-      Counter.build(METRIC_UPDATE_COUNT, "Number of created/updated metric values")
-          .labelNames("persistent")
-          .register(CollectorRegistry.defaultRegistry);
-  private static final Counter METRIC_DELETE_COUNTER =
-      Counter.build(METRIC_DELETE_COUNT, "Number of deleted metric values")
-          .labelNames("persistent")
-          .register(CollectorRegistry.defaultRegistry);
-
   private final MetricStorage metricStorage;
 
   @Inject
@@ -93,62 +48,22 @@ public class MetricService {
     this.metricStorage = metricStorage;
   }
 
-  public List<Metric> save(List<Metric> metrics) {
-    try {
-      acquireLocks(metrics);
-      return saveInternal(metrics, false);
-    } finally {
-      releaseLocks(metrics);
-    }
+  public void save(List<Metric> metrics) {
+    metricStorage.save(metrics);
   }
 
-  private List<Metric> saveInternal(List<Metric> metrics, boolean persist) {
-    List<Metric> beforeMetrics = Collections.emptyList();
-    Set<MetricKey> metricKeys = metrics.stream().map(MetricKey::from).collect(Collectors.toSet());
-    if (!metricKeys.isEmpty()) {
-      MetricFilter filter = MetricFilter.builder().keys(metricKeys).build();
-      beforeMetrics = list(filter, persist);
-    }
-    Map<MetricKey, Metric> beforeMetricMap =
-        beforeMetrics.stream().collect(Collectors.toMap(MetricKey::from, Function.identity()));
-
-    Map<EntityOperation, List<Metric>> toCreateAndUpdate =
-        metrics
-            .stream()
-            .peek(this::validate)
-            .filter(metric -> filterForSave(metric, beforeMetricMap.get(MetricKey.from(metric))))
-            .map(metric -> prepareForSave(metric, beforeMetricMap.get(MetricKey.from(metric))))
-            .collect(Collectors.groupingBy(metric -> metric.isNew() ? CREATE : UPDATE));
-
-    List<Metric> toCreate = toCreateAndUpdate.getOrDefault(CREATE, Collections.emptyList());
-    if (!toCreate.isEmpty()) {
-      log.trace("Creating metrics {} in {} storage", toCreate, persist ? "DB" : "Memory");
-      if (persist) {
-        toCreate.forEach(Metric::generateUUID);
-        Metric.db().saveAll(toCreate);
-      } else {
-        metricStorage.save(toCreate);
-      }
-    }
-
-    List<Metric> toUpdate = toCreateAndUpdate.getOrDefault(UPDATE, Collections.emptyList());
-    if (!toUpdate.isEmpty()) {
-      log.trace("Updating metrics {} in {} storage", toUpdate, persist ? "DB" : "Memory");
-      if (persist) {
-        Metric.db().updateAll(toUpdate);
-      } else {
-        metricStorage.save(toUpdate);
-      }
-    }
+  public void save(Metric metric) {
+    save(Collections.singletonList(metric));
+  }
 
-    int savedMetrics = toCreate.size() + toUpdate.size();
-    METRIC_UPDATE_COUNTER.labels(String.valueOf(persist)).inc(savedMetrics);
-    log.trace("{} metrics saved", savedMetrics);
-    return metrics;
+  public void cleanAndSave(List<Metric> toSave, MetricFilter toClean) {
+    Set<MetricKey> toSaveKeys = toSave.stream().map(MetricKey::from).collect(Collectors.toSet());
+    metricStorage.delete(toClean.toBuilder().keysExcluded(toSaveKeys).build());
+    metricStorage.save(toSave);
   }
 
-  public Metric save(Metric metric) {
-    return save(Collections.singletonList(metric)).get(0);
+  public void delete(MetricFilter metricFilter) {
+    metricStorage.delete(metricFilter);
   }
 
   public Metric get(MetricKey key) {
@@ -156,119 +71,47 @@ public class MetricService {
   }
 
   public List<Metric> list(MetricFilter filter) {
-    return list(filter, false);
-  }
-
-  public List<Metric> list(MetricFilter filter, boolean persisted) {
-    if (persisted) {
-      return createQueryByFilter(filter).findList();
-    } else {
-      return metricStorage.list(filter);
-    }
+    List<Metric> result = new ArrayList<>();
+    metricStorage.process(filter, result::add);
+    return result;
   }
 
-  public void delete(MetricFilter filter) {
-    List<Metric> toDelete = list(filter);
-    if (toDelete.isEmpty()) {
-      return;
-    }
-    try {
-      acquireLocks(toDelete);
-      deleteInternal(toDelete, false);
-    } finally {
-      releaseLocks(toDelete);
-    }
+  public void setOkStatusMetric(Metric metric) {
+    setMetric(metric, STATUS_OK);
   }
 
-  private void deleteInternal(Collection<Metric> toDelete, boolean persist) {
-    if (toDelete.isEmpty()) {
-      return;
-    }
-    int deleted;
-    log.trace("Deleting metrics {} from {} storage", toDelete, persist ? "DB" : "Memory");
-    if (persist) {
-      MetricFilter deleteFilter =
-          MetricFilter.builder()
-              .keys(toDelete.stream().map(MetricKey::from).collect(Collectors.toSet()))
-              .build();
-      deleted = createQueryByFilter(deleteFilter).delete();
-    } else {
-      deleted = metricStorage.delete(toDelete);
-    }
-
-    METRIC_DELETE_COUNTER.labels(String.valueOf(persist)).inc(deleted);
-    log.trace("{} metrics deleted", deleted);
+  public void setFailureStatusMetric(Metric metric) {
+    setMetric(metric, STATUS_NOT_OK);
   }
 
-  public void cleanAndSave(List<Metric> toSave, List<MetricFilter> toClean) {
-    Map<MetricKey, Metric> toDelete =
-        toClean
-            .stream()
-            .map(this::list)
-            .flatMap(Collection::stream)
-            .collect(Collectors.toMap(MetricKey::from, Function.identity(), (a, b) -> a));
-    Set<MetricKey> toSaveKeys = toSave.stream().map(MetricKey::from).collect(Collectors.toSet());
-    toSaveKeys.forEach(toDelete::remove);
-    Set<MetricKey> allKeys =
-        Stream.concat(toSaveKeys.stream(), toDelete.keySet().stream()).collect(Collectors.toSet());
-
-    try {
-      metricKeyLock.acquireLocks(allKeys);
-      if (!toSave.isEmpty()) {
-        saveInternal(toSave, false);
-      }
-      if (!toDelete.isEmpty()) {
-        deleteInternal(toDelete.values(), false);
-      }
-    } finally {
-      metricKeyLock.releaseLocks(allKeys);
-    }
+  public void setMetric(Metric metric, double value) {
+    metric.setValue(value);
+    save(Collections.singletonList(metric));
   }
 
-  public void cleanAndSave(List<Metric> toSave, MetricFilter toClean) {
-    cleanAndSave(toSave, ImmutableList.of(toClean));
+  public void markSourceActive(UUID customerUuid, UUID sourceUuid) {
+    metricStorage.markSource(customerUuid, sourceUuid, MetricSourceState.ACTIVE);
   }
 
-  public void cleanAndSave(List<Metric> toSave) {
-    if (CollectionUtils.isEmpty(toSave)) {
-      return;
-    }
-    MetricFilter toClean =
+  public void markSourceInactive(UUID customerUuid, UUID sourceUuid) {
+    MetricFilterBuilder filter =
         MetricFilter.builder()
-            .keys(toSave.stream().map(MetricKey::from).collect(Collectors.toSet()))
-            .build();
-    cleanAndSave(toSave, toClean);
-  }
-
-  public void setOkStatusMetric(Metric metric) {
-    setStatusMetric(metric, StringUtils.EMPTY);
-  }
-
-  public void setFailureStatusMetric(Metric metric) {
-    metric.setValue(STATUS_NOT_OK);
-    cleanAndSave(Collections.singletonList(metric));
-  }
-
-  public void setStatusMetric(Metric metric, String message) {
-    boolean isSuccess = StringUtils.isEmpty(message);
-    metric.setValue(isSuccess ? STATUS_OK : STATUS_NOT_OK);
-    if (!isSuccess) {
-      metric.setLabel(KnownAlertLabels.ERROR_MESSAGE, message);
+            .customerUuid(customerUuid)
+            .metricNames(PlatformMetrics.invalidForState(MetricSourceState.INACTIVE));
+    if (sourceUuid != null) {
+      filter.sourceUuid(sourceUuid);
     }
-    cleanAndSave(Collections.singletonList(metric));
-  }
-
-  public void setMetric(Metric metric, double value) {
-    metric.setValue(value);
-    cleanAndSave(Collections.singletonList(metric));
+    metricStorage.markSource(customerUuid, sourceUuid, MetricSourceState.INACTIVE);
+    metricStorage.delete(filter.build());
   }
 
-  public void handleSourceRemoval(UUID customerUuid, UUID sourceUuid) {
+  public void markSourceRemoved(UUID customerUuid, UUID sourceUuid) {
     MetricFilterBuilder filter = MetricFilter.builder().customerUuid(customerUuid);
     if (sourceUuid != null) {
       filter.sourceUuid(sourceUuid);
     }
-    delete(filter.build());
+    metricStorage.markSource(customerUuid, sourceUuid, MetricSourceState.REMOVED);
+    metricStorage.delete(filter.build());
   }
 
   private void validate(Metric metric) {
@@ -283,42 +126,6 @@ public class MetricService {
     }
   }
 
-  private boolean filterForSave(Metric metric, Metric before) {
-    if (before == null) {
-      return true;
-    }
-    if (!Objects.equals(before.getValue(), metric.getValue())) {
-      return true;
-    }
-    Date now = new Date();
-    double oldExpiryDuration = getDurationSeconds(now, before.getExpireTime());
-    long newExpiryDuration = getDurationSeconds(now, metric.getExpireTime());
-    // Update if expiry time becomes less OR if expiry time grows at least for 10%
-    // - once a day for default expiry period.
-    if (newExpiryDuration < oldExpiryDuration
-        || oldExpiryDuration * EXPIRY_TIME_UPDATE_COEFFICIENT < newExpiryDuration) {
-      return true;
-    }
-    return false;
-  }
-
-  private Metric prepareForSave(Metric metric, Metric before) {
-    List<MetricLabel> newSourceLabels =
-        metric.getLabels().stream().filter(MetricLabel::isSourceLabel).collect(Collectors.toList());
-    String newSourceLabelStr = Metric.getSourceLabelsStr(newSourceLabels);
-    Metric result = before == null ? metric : before;
-    result.setSourceLabels(newSourceLabelStr);
-    result.setUpdateTime(CommonUtils.nowWithoutMillis());
-    if (before != null) {
-      result.setValue(metric.getValue());
-      result.setLabels(metric.getLabels());
-      result.setExpireTime(metric.getExpireTime());
-    } else if (result.getUuid() != null) {
-      log.warn("Trying to save metric with uuid, which is not found in storage: {}", result);
-    }
-    return result;
-  }
-
   public static Metric buildMetricTemplate(PlatformMetrics metric) {
     return buildMetricTemplate(metric, DEFAULT_METRIC_EXPIRY_SEC);
   }
@@ -364,58 +171,4 @@ public class MetricService {
         .setSourceUuid(universe.getUniverseUUID())
         .setLabels(MetricLabelsBuilder.create().appendSource(universe).getMetricLabels());
   }
-
-  private void acquireLocks(Collection<Metric> metrics) {
-    Set<MetricKey> keys = metrics.stream().map(MetricKey::from).collect(Collectors.toSet());
-    metricKeyLock.acquireLocks(keys);
-  }
-
-  private void releaseLocks(Collection<Metric> metrics) {
-    Set<MetricKey> keys = metrics.stream().map(MetricKey::from).collect(Collectors.toSet());
-    metricKeyLock.releaseLocks(keys);
-  }
-
-  @Transactional
-  public void flushMetricsToDb() {
-    Set<MetricKey> dirtyKeys = metricStorage.getDirtyMetrics();
-    Set<MetricKey> deletedKeys = metricStorage.getDeletedMetrics();
-    Set<MetricKey> keysToLock =
-        Stream.concat(dirtyKeys.stream(), deletedKeys.stream()).collect(Collectors.toSet());
-    if (CollectionUtils.isEmpty(keysToLock)) {
-      return;
-    }
-    try {
-      Set<UUID> customerUuids =
-          keysToLock
-              .stream()
-              .map(MetricKey::getSourceKey)
-              .map(MetricSourceKey::getCustomerUuid)
-              .filter(Objects::nonNull)
-              .collect(Collectors.toSet());
-      // This is required to avoid DB deadlock on customer delete +
-      // parallel metric save/delete operation
-      if (CollectionUtils.isNotEmpty(customerUuids)) {
-        Customer.getForUpdate(customerUuids);
-      }
-      metricKeyLock.acquireLocks(keysToLock);
-      if (CollectionUtils.isNotEmpty(dirtyKeys)) {
-        // querying in-memory metrics here to update persistent storage.
-        List<Metric> toSave = list(MetricFilter.builder().keys(dirtyKeys).build(), false);
-        saveInternal(toSave, true);
-        metricStorage.getDirtyMetrics().removeAll(dirtyKeys);
-      }
-      if (CollectionUtils.isNotEmpty(deletedKeys)) {
-        // querying persistent metrics here to delete them.
-        List<Metric> toDelete = list(MetricFilter.builder().keys(deletedKeys).build(), true);
-        deleteInternal(toDelete, true);
-        metricStorage.getDeletedMetrics().removeAll(deletedKeys);
-      }
-    } finally {
-      metricKeyLock.releaseLocks(keysToLock);
-    }
-  }
-
-  void initialize() {
-    metricStorage.initialize(list(MetricFilter.builder().build(), true));
-  }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricStorage.java b/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricStorage.java
index 6fecc7ce13..ac48d94cbd 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricStorage.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/metrics/MetricStorage.java
@@ -9,96 +9,300 @@
  */
 package com.yugabyte.yw.common.metrics;
 
+import com.yugabyte.yw.common.utils.Pair;
 import com.yugabyte.yw.models.Metric;
 import com.yugabyte.yw.models.MetricKey;
+import com.yugabyte.yw.models.MetricSourceKey;
 import com.yugabyte.yw.models.filters.MetricFilter;
+import com.yugabyte.yw.models.helpers.MetricSourceState;
+import com.yugabyte.yw.models.helpers.PlatformMetrics;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Map.Entry;
 import java.util.Objects;
+import java.util.Optional;
 import java.util.Set;
+import java.util.UUID;
 import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import java.util.function.Consumer;
 import java.util.stream.Collectors;
+import java.util.stream.Stream;
 import javax.inject.Singleton;
+import lombok.Value;
 import lombok.extern.slf4j.Slf4j;
 import org.apache.commons.collections.CollectionUtils;
 
+/**
+ * Metric store. Used to store last metric value in-memory and return the list of metrics to
+ * prometheus endpoint. Used instead of regular Prometheus client classes as we need to remove
+ * metrics, which are not applicable anymore - for example object is deleted. Also allows to delete
+ * old expired metrics, which are not deleted explicitly.
+ */
 @Singleton
 @Slf4j
 public class MetricStorage {
 
-  private final Map<MetricKey, Metric> metricsByKey = new ConcurrentHashMap<>();
-  private final Set<MetricKey> dirtyMetrics = Collections.newSetFromMap(new ConcurrentHashMap<>());
-  private final Set<MetricKey> deletedMetrics =
-      Collections.newSetFromMap(new ConcurrentHashMap<>());
+  private static final UUID NULL_UUID = UUID.fromString("00000000-0000-0000-0000-000000000000");
+  private final Map<String, NamedMetricStore> metricsByKey = new ConcurrentHashMap<>();
+  private final Map<String, Lock> metricNameLock = new ConcurrentHashMap<>();
+  private final Map<Pair<UUID, UUID>, MetricSourceState> sourceStateMap = new ConcurrentHashMap<>();
 
   public Metric get(MetricKey key) {
-    return metricsByKey.get(key);
+    return get(key.getSourceKey().getName())
+        .flatMap(namedStore -> namedStore.get(key.getSourceKey().getCustomerUuid()))
+        .flatMap(customerStore -> customerStore.get(key.getSourceKey().getSourceUuid()))
+        .flatMap(sourceStore -> sourceStore.get(key.getSourceLabels()))
+        .filter(metric -> !metric.isDeleted())
+        .orElse(null);
+  }
+
+  public void process(MetricFilter metricFilter, Consumer<Metric> metricConsumer) {
+    get(metricFilter)
+        .flatMap(namedStore -> namedStore.get(metricFilter))
+        .flatMap(customerStore -> customerStore.get(metricFilter))
+        .flatMap(sourceStore -> sourceStore.get(metricFilter))
+        .forEach(metricConsumer);
   }
 
-  public List<Metric> list(MetricFilter metricFilter) {
-    Collection<Metric> candidates;
-    if (CollectionUtils.isNotEmpty(metricFilter.getKeys())) {
-      candidates =
-          metricFilter
+  public void save(List<Metric> metrics) {
+    if (CollectionUtils.isEmpty(metrics)) {
+      return;
+    }
+    acquireLocks(metrics);
+    try {
+      metrics.forEach(this::save);
+    } finally {
+      releaseLocks(metrics);
+    }
+  }
+
+  public void delete(MetricFilter filter) {
+    process(filter, metric -> metric.setDeleted(true));
+  }
+
+  public void markSource(UUID customerUuid, UUID metricSource, MetricSourceState state) {
+    sourceStateMap.put(new Pair<>(customerUuid, metricSource), state);
+  }
+
+  private void acquireLocks(Collection<Metric> metrics) {
+    metrics
+        .stream()
+        .map(Metric::getName)
+        .distinct()
+        .sorted()
+        .forEach(
+            metricName ->
+                metricNameLock.computeIfAbsent(metricName, n -> new ReentrantLock()).lock());
+  }
+
+  private void releaseLocks(Collection<Metric> metrics) {
+    metrics
+        .stream()
+        .map(Metric::getName)
+        .distinct()
+        .sorted()
+        .forEach(metricName -> metricNameLock.get(metricName).unlock());
+  }
+
+  private Optional<NamedMetricStore> get(String name) {
+    return Optional.ofNullable(metricsByKey.get(name));
+  }
+
+  private Stream<NamedMetricStore> get(MetricFilter filter) {
+    Set<String> names = getNames(filter);
+    return metricsByKey
+        .entrySet()
+        .stream()
+        .filter(e -> names.contains(e.getKey()))
+        .map(Entry::getValue);
+  }
+
+  private Set<String> getNames(MetricFilter filter) {
+    Set<String> names = new HashSet<>();
+    if (CollectionUtils.isNotEmpty(filter.getMetricNames())) {
+      names.addAll(filter.getMetricNames());
+    }
+    if (CollectionUtils.isNotEmpty(filter.getSourceKeys())) {
+      names.addAll(
+          filter
+              .getSourceKeys()
+              .stream()
+              .map(MetricSourceKey::getName)
+              .collect(Collectors.toList()));
+    }
+    if (CollectionUtils.isNotEmpty(filter.getKeys())) {
+      names.addAll(
+          filter
               .getKeys()
               .stream()
-              .map(metricsByKey::get)
-              .filter(Objects::nonNull)
-              .collect(Collectors.toList());
-    } else {
-      candidates = new ArrayList<>(metricsByKey.values());
+              .map(MetricKey::getSourceKey)
+              .map(MetricSourceKey::getName)
+              .collect(Collectors.toList()));
+    }
+    if (CollectionUtils.isEmpty(names)) {
+      names = new HashSet<>(metricsByKey.keySet());
     }
-    return candidates.stream().filter(metricFilter::match).collect(Collectors.toList());
+    return names;
   }
 
-  void initialize(List<Metric> metrics) {
-    metrics.forEach(
-        metric -> {
-          MetricKey key = MetricKey.from(metric);
-          metricsByKey.put(key, metric);
-        });
+  private void save(Metric metric) {
+    if (metric.getSourceUuid() != null) {
+      MetricSourceState metricSourceState =
+          sourceStateMap.getOrDefault(
+              new Pair<>(metric.getCustomerUUID(), metric.getSourceUuid()),
+              MetricSourceState.ACTIVE);
+      PlatformMetrics platformMetric = PlatformMetrics.fromMetricName(metric.getName());
+      if (!platformMetric.getValidForSourceStates().contains(metricSourceState)) {
+        log.debug(
+            "Skipping metric {} from source {} as it's marked {}",
+            metric.getName(),
+            metric.getSourceUuid(),
+            metricSourceState.name());
+        return;
+      }
+    }
+
+    NamedMetricStore store =
+        metricsByKey.computeIfAbsent(metric.getName(), n -> new NamedMetricStore());
+
+    store.save(metric);
   }
 
-  public List<Metric> save(List<Metric> metrics) {
-    if (CollectionUtils.isEmpty(metrics)) {
-      return metrics;
-    }
-    metrics.forEach(
-        metric -> {
-          MetricKey key = MetricKey.from(metric);
-          metricsByKey.put(key, metric);
-          dirtyMetrics.add(key);
-          deletedMetrics.remove(key);
-        });
-    return metrics;
+  private static UUID getUuidKey(UUID uuid) {
+    return uuid != null ? uuid : NULL_UUID;
   }
 
-  public int delete(Collection<Metric> metrics) {
-    if (CollectionUtils.isEmpty(metrics)) {
-      return 0;
-    }
-    return (int)
-        metrics
-            .stream()
-            .filter(
-                metric -> {
-                  MetricKey key = MetricKey.from(metric);
-                  boolean deleted = (metricsByKey.remove(key) != null);
-                  dirtyMetrics.remove(key);
-                  deletedMetrics.add(key);
-                  return deleted;
-                })
-            .count();
+  @Value
+  private static class NamedMetricStore {
+    Map<UUID, CustomerMetricStore> customerMetrics = new HashMap<>();
+
+    private Optional<CustomerMetricStore> get(UUID uuid) {
+      return Optional.ofNullable(customerMetrics.get(getUuidKey(uuid)));
+    }
+
+    private Stream<CustomerMetricStore> get(MetricFilter filter) {
+      Set<UUID> uuids = new HashSet<>();
+      if (filter.getCustomerUuid() != null) {
+        uuids.add(filter.getCustomerUuid());
+      }
+      if (CollectionUtils.isNotEmpty(filter.getSourceKeys())) {
+        uuids.addAll(
+            filter
+                .getSourceKeys()
+                .stream()
+                .map(MetricSourceKey::getCustomerUuid)
+                .filter(Objects::nonNull)
+                .collect(Collectors.toList()));
+      }
+      if (CollectionUtils.isNotEmpty(filter.getKeys())) {
+        uuids.addAll(
+            filter
+                .getKeys()
+                .stream()
+                .map(MetricKey::getSourceKey)
+                .map(MetricSourceKey::getCustomerUuid)
+                .filter(Objects::nonNull)
+                .collect(Collectors.toList()));
+      }
+
+      return customerMetrics
+          .entrySet()
+          .stream()
+          .filter(e -> CollectionUtils.isEmpty(uuids) || uuids.contains(e.getKey()))
+          .map(Entry::getValue);
+    }
+
+    private void save(Metric metric) {
+      customerMetrics
+          .computeIfAbsent(getUuidKey(metric.getCustomerUUID()), k -> new CustomerMetricStore())
+          .save(metric);
+    }
   }
 
-  public Set<MetricKey> getDirtyMetrics() {
-    return dirtyMetrics;
+  @Value
+  private static class CustomerMetricStore {
+    Map<UUID, SourceMetricStore> sourceMetrics = new HashMap<>();
+
+    private Optional<SourceMetricStore> get(UUID uuid) {
+      return Optional.ofNullable(sourceMetrics.get(getUuidKey(uuid)));
+    }
+
+    private Stream<SourceMetricStore> get(MetricFilter filter) {
+      Set<UUID> uuids = new HashSet<>();
+      if (filter.getSourceUuid() != null) {
+        uuids.add(filter.getSourceUuid());
+      }
+      if (CollectionUtils.isNotEmpty(filter.getSourceKeys())) {
+        uuids.addAll(
+            filter
+                .getSourceKeys()
+                .stream()
+                .map(MetricSourceKey::getSourceUuid)
+                .filter(Objects::nonNull)
+                .collect(Collectors.toList()));
+      }
+      if (CollectionUtils.isNotEmpty(filter.getKeys())) {
+        uuids.addAll(
+            filter
+                .getKeys()
+                .stream()
+                .map(MetricKey::getSourceKey)
+                .map(MetricSourceKey::getSourceUuid)
+                .filter(Objects::nonNull)
+                .collect(Collectors.toList()));
+      }
+
+      return sourceMetrics
+          .entrySet()
+          .stream()
+          .filter(e -> CollectionUtils.isEmpty(uuids) || uuids.contains(e.getKey()))
+          .map(Entry::getValue);
+    }
+
+    private void save(Metric metric) {
+      sourceMetrics
+          .computeIfAbsent(getUuidKey(metric.getSourceUuid()), k -> new SourceMetricStore())
+          .save(metric);
+    }
   }
 
-  public Set<MetricKey> getDeletedMetrics() {
-    return deletedMetrics;
+  @Value
+  private static class SourceMetricStore {
+    List<Metric> sourceMetrics = new ArrayList<>();
+
+    private Optional<Metric> get(Map<String, String> labels) {
+      return sourceMetrics
+          .stream()
+          .filter(
+              metric ->
+                  labels
+                      .entrySet()
+                      .stream()
+                      .allMatch(
+                          l -> Objects.equals(metric.getLabelValue(l.getKey()), l.getValue())))
+          .findFirst();
+    }
+
+    private Stream<Metric> get(MetricFilter filter) {
+      return sourceMetrics.stream().filter(filter::match);
+    }
+
+    private void save(Metric metric) {
+      Metric existing = get(metric.getKeyLabelValues()).orElse(null);
+      if (existing != null) {
+        existing.setValue(metric.getValue());
+        existing.setUpdateTime(metric.getUpdateTime());
+        existing.setExpireTime(metric.getExpireTime());
+        existing.setDeleted(false);
+      } else {
+        sourceMetrics.add(metric);
+      }
+    }
   }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/common/metrics/PlatformMetricsProcessor.java b/managed/src/main/java/com/yugabyte/yw/common/metrics/PlatformMetricsProcessor.java
index c952b3c2eb..eacc4c4567 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/metrics/PlatformMetricsProcessor.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/metrics/PlatformMetricsProcessor.java
@@ -16,9 +16,7 @@ import akka.actor.ActorSystem;
 import com.google.common.annotations.VisibleForTesting;
 import com.google.inject.Inject;
 import com.google.inject.Singleton;
-import com.yugabyte.yw.models.Metric;
 import com.yugabyte.yw.models.filters.MetricFilter;
-import com.yugabyte.yw.models.helpers.CommonUtils;
 import com.yugabyte.yw.models.helpers.PlatformMetrics;
 import java.util.ArrayList;
 import java.util.List;
@@ -57,7 +55,6 @@ public class PlatformMetricsProcessor {
   }
 
   public void start() {
-    metricService.initialize();
     this.actorSystem
         .scheduler()
         .schedule(
@@ -73,56 +70,30 @@ public class PlatformMetricsProcessor {
       log.info("Previous run of metrics processor is still underway");
       return;
     }
-    String errorMessage = null;
     try {
-      errorMessage = updateMetrics();
+      updateMetrics();
       cleanExpiredMetrics();
-      metricService.flushMetricsToDb();
     } catch (Exception e) {
-      errorMessage = "Error processing metrics: " + e.getMessage();
       log.error("Error processing metrics", e);
     } finally {
-      metricService.setStatusMetric(
-          buildMetricTemplate(PlatformMetrics.METRIC_PROCESSOR_STATUS), errorMessage);
+      metricService.setFailureStatusMetric(
+          buildMetricTemplate(PlatformMetrics.METRIC_PROCESSOR_STATUS));
       running.set(false);
     }
   }
 
-  private String updateMetrics() {
-    List<MetricSaveGroup> metricSaveGroups = new ArrayList<>();
-    String errorMessage = null;
+  private void updateMetrics() {
     for (MetricsProvider provider : metricsProviderList) {
       try {
-        metricSaveGroups.addAll(provider.getMetricGroups());
+        provider
+            .getMetricGroups()
+            .forEach(
+                group ->
+                    metricService.cleanAndSave(group.getMetrics(), group.getCleanMetricFilter()));
       } catch (Exception e) {
         log.error("Failed to get platform metrics from provider " + provider.getName(), e);
-        if (errorMessage == null) {
-          errorMessage =
-              "Failed to get platform metrics from provider "
-                  + provider.getName()
-                  + ": "
-                  + e.getMessage();
-        }
       }
     }
-    int metricsToSave = 0;
-    List<Metric> metrics = new ArrayList<>();
-    List<MetricFilter> toRemove = new ArrayList<>();
-    for (MetricSaveGroup metricSaveGroup : metricSaveGroups) {
-      metricsToSave += metricSaveGroup.getMetrics().size();
-      if (metricsToSave > CommonUtils.DB_OR_CHAIN_TO_WARN) {
-        metricService.cleanAndSave(metrics, toRemove);
-        metricsToSave = metricSaveGroup.getMetrics().size();
-        metrics.clear();
-        toRemove.clear();
-      }
-      metrics.addAll(metricSaveGroup.getMetrics());
-      toRemove.addAll(metricSaveGroup.getCleanMetricFilters());
-    }
-    if (!metrics.isEmpty()) {
-      metricService.cleanAndSave(metrics, toRemove);
-    }
-    return errorMessage;
   }
 
   private void cleanExpiredMetrics() {
diff --git a/managed/src/main/java/com/yugabyte/yw/common/metrics/UniverseMetricProvider.java b/managed/src/main/java/com/yugabyte/yw/common/metrics/UniverseMetricProvider.java
index 2d6c49edcd..28ea6bc0d4 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/metrics/UniverseMetricProvider.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/metrics/UniverseMetricProvider.java
@@ -63,6 +63,12 @@ public class UniverseMetricProvider implements MetricsProvider {
                 universe,
                 PlatformMetrics.UNIVERSE_BACKUP_IN_PROGRESS,
                 statusValue(universe.getUniverseDetails().backupInProgress)));
+        universeGroup.metric(
+            createUniverseMetric(
+                customer,
+                universe,
+                PlatformMetrics.UNIVERSE_REPLICATION_FACTOR,
+                universe.getUniverseDetails().getPrimaryCluster().userIntent.replicationFactor));
 
         if (universe.getUniverseDetails().nodeDetailsSet != null) {
           for (NodeDetails nodeDetails : universe.getUniverseDetails().nodeDetailsSet) {
diff --git a/managed/src/main/java/com/yugabyte/yw/common/utils/Pair.java b/managed/src/main/java/com/yugabyte/yw/common/utils/Pair.java
index d3465a49ce..e8757e5958 100644
--- a/managed/src/main/java/com/yugabyte/yw/common/utils/Pair.java
+++ b/managed/src/main/java/com/yugabyte/yw/common/utils/Pair.java
@@ -10,6 +10,9 @@
 
 package com.yugabyte.yw.common.utils;
 
+import lombok.Data;
+
+@Data
 public class Pair<U, V> {
   private final U first;
   private final V second;
@@ -18,34 +21,4 @@ public class Pair<U, V> {
     this.first = first;
     this.second = second;
   }
-
-  public U getFirst() {
-    return first;
-  }
-
-  public V getSecond() {
-    return second;
-  }
-
-  @Override
-  public boolean equals(Object o) {
-    if (this == o) return true;
-
-    if (o == null || getClass() != o.getClass()) return false;
-
-    Pair<?, ?> pair = (Pair<?, ?>) o;
-
-    if (!first.equals(pair.first)) return false;
-    return second.equals(pair.second);
-  }
-
-  @Override
-  public int hashCode() {
-    return 31 * first.hashCode() + second.hashCode();
-  }
-
-  @Override
-  public String toString() {
-    return "(" + first + ", " + second + ")";
-  }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/controllers/AlertController.java b/managed/src/main/java/com/yugabyte/yw/controllers/AlertController.java
index d935f8e9fa..f2e0962744 100644
--- a/managed/src/main/java/com/yugabyte/yw/controllers/AlertController.java
+++ b/managed/src/main/java/com/yugabyte/yw/controllers/AlertController.java
@@ -384,7 +384,7 @@ public class AlertController extends AuthenticatedController {
     Customer.getOrBadRequest(customerUUID);
     AlertChannel channel = alertChannelService.getOrBadRequest(customerUUID, alertChannelUUID);
     alertChannelService.delete(customerUUID, alertChannelUUID);
-    metricService.handleSourceRemoval(channel.getCustomerUUID(), alertChannelUUID);
+    metricService.markSourceRemoved(channel.getCustomerUUID(), alertChannelUUID);
     auditService().createAuditEntry(ctx(), request());
     return YBPSuccess.empty();
   }
diff --git a/managed/src/main/java/com/yugabyte/yw/controllers/CustomerController.java b/managed/src/main/java/com/yugabyte/yw/controllers/CustomerController.java
index 3acd2a852f..82bd22a175 100644
--- a/managed/src/main/java/com/yugabyte/yw/controllers/CustomerController.java
+++ b/managed/src/main/java/com/yugabyte/yw/controllers/CustomerController.java
@@ -279,7 +279,7 @@ public class CustomerController extends AuthenticatedController {
           INTERNAL_SERVER_ERROR, "Unable to delete Customer UUID: " + customerUUID);
     }
 
-    metricService.handleSourceRemoval(customerUUID, null);
+    metricService.markSourceRemoved(customerUUID, null);
 
     auditService().createAuditEntry(ctx(), request());
     return YBPSuccess.empty();
diff --git a/managed/src/main/java/com/yugabyte/yw/controllers/MetricsController.java b/managed/src/main/java/com/yugabyte/yw/controllers/MetricsController.java
index f1e4a54715..23f2f27aaa 100644
--- a/managed/src/main/java/com/yugabyte/yw/controllers/MetricsController.java
+++ b/managed/src/main/java/com/yugabyte/yw/controllers/MetricsController.java
@@ -7,7 +7,6 @@ import com.yugabyte.yw.common.ApiHelper;
 import com.yugabyte.yw.common.PlatformServiceException;
 import com.yugabyte.yw.common.metrics.MetricService;
 import com.yugabyte.yw.models.Metric;
-import com.yugabyte.yw.models.MetricLabel;
 import com.yugabyte.yw.models.filters.MetricFilter;
 import com.yugabyte.yw.models.helpers.CommonUtils;
 import com.yugabyte.yw.models.helpers.KnownAlertLabels;
@@ -24,7 +23,6 @@ import java.io.OutputStreamWriter;
 import java.time.temporal.ChronoUnit;
 import java.util.ArrayList;
 import java.util.Collections;
-import java.util.Comparator;
 import java.util.Date;
 import java.util.List;
 import java.util.Map;
@@ -66,7 +64,7 @@ public class MetricsController extends Controller {
       // Write runtime metrics
       TextFormat.write004(osw, CollectorRegistry.defaultRegistry.metricFamilySamples());
       // Write persisted metrics
-      TextFormat.write004(osw, Collections.enumeration(getPersistedMetrics()));
+      TextFormat.write004(osw, Collections.enumeration(getPrecalculatedMetrics()));
       // Write Kamon metrics
       osw.write(getKamonMetrics());
 
@@ -97,7 +95,7 @@ public class MetricsController extends Controller {
     return StringUtils.EMPTY;
   }
 
-  private List<Collector.MetricFamilySamples> getPersistedMetrics() {
+  private List<Collector.MetricFamilySamples> getPrecalculatedMetrics() {
     List<MetricFamilySamples> result = new ArrayList<>();
     List<Metric> allMetrics = metricService.list(MetricFilter.builder().expired(false).build());
 
@@ -125,16 +123,8 @@ public class MetricsController extends Controller {
   }
 
   private Collector.MetricFamilySamples.Sample convert(Metric metric) {
-    List<MetricLabel> metricLabels =
-        metric
-            .getLabels()
-            .stream()
-            .sorted(Comparator.comparing(MetricLabel::getName))
-            .collect(Collectors.toList());
-    List<String> labelNames =
-        metricLabels.stream().map(MetricLabel::getName).collect(Collectors.toList());
-    List<String> labelValues =
-        metricLabels.stream().map(MetricLabel::getValue).collect(Collectors.toList());
+    List<String> labelNames = new ArrayList<>(metric.getLabels().keySet());
+    List<String> labelValues = new ArrayList<>(metric.getLabels().values());
     if (metric.getCustomerUUID() != null) {
       labelNames.add(KnownAlertLabels.CUSTOMER_UUID.labelName());
       labelValues.add(metric.getCustomerUUID().toString());
diff --git a/managed/src/main/java/com/yugabyte/yw/controllers/PlatformHttpActionAdapter.java b/managed/src/main/java/com/yugabyte/yw/controllers/PlatformHttpActionAdapter.java
index 0bc0a8bc1e..58fda08a85 100644
--- a/managed/src/main/java/com/yugabyte/yw/controllers/PlatformHttpActionAdapter.java
+++ b/managed/src/main/java/com/yugabyte/yw/controllers/PlatformHttpActionAdapter.java
@@ -8,6 +8,8 @@ import org.pac4j.play.PlayWebContext;
 import org.pac4j.play.http.DefaultHttpActionAdapter;
 import play.mvc.Result;
 
+// TODO(sbapat): investigate why this class is needed .. looks redundant as
+//  base class provides all the functionality
 public class PlatformHttpActionAdapter extends DefaultHttpActionAdapter {
 
   @Override
diff --git a/managed/src/main/java/com/yugabyte/yw/controllers/RuntimeConfController.java b/managed/src/main/java/com/yugabyte/yw/controllers/RuntimeConfController.java
index a6e237a280..30c894d4d7 100644
--- a/managed/src/main/java/com/yugabyte/yw/controllers/RuntimeConfController.java
+++ b/managed/src/main/java/com/yugabyte/yw/controllers/RuntimeConfController.java
@@ -10,12 +10,18 @@
 
 package com.yugabyte.yw.controllers;
 
+import static com.yugabyte.yw.models.ScopedRuntimeConfig.GLOBAL_SCOPE_UUID;
+
 import com.google.common.collect.ImmutableSet;
+import com.google.common.collect.Sets;
+import com.google.common.collect.Streams;
 import com.google.inject.Inject;
 import com.typesafe.config.Config;
+import com.typesafe.config.ConfigRenderOptions;
 import com.yugabyte.yw.common.PlatformServiceException;
 import com.yugabyte.yw.common.config.impl.RuntimeConfig;
 import com.yugabyte.yw.common.config.impl.SettableRuntimeConfigFactory;
+import com.yugabyte.yw.common.ha.PlatformInstanceClientFactory;
 import com.yugabyte.yw.forms.PlatformResults;
 import com.yugabyte.yw.forms.PlatformResults.YBPSuccess;
 import com.yugabyte.yw.forms.RuntimeConfigFormData;
@@ -35,6 +41,7 @@ import io.swagger.annotations.ApiOperation;
 import io.swagger.annotations.Authorization;
 import java.util.List;
 import java.util.Map;
+import java.util.Map.Entry;
 import java.util.Optional;
 import java.util.Set;
 import java.util.UUID;
@@ -49,14 +56,24 @@ import play.mvc.Result;
 public class RuntimeConfController extends AuthenticatedController {
   private static final Logger LOG = LoggerFactory.getLogger(RuntimeConfController.class);
   private final SettableRuntimeConfigFactory settableRuntimeConfigFactory;
+  private final PlatformInstanceClientFactory platformInstanceClientFactory;
   private final Result mutableKeysResult;
+  private final Set<String> mutableObjects;
   private final Set<String> mutableKeys;
   private static final Set<String> sensitiveKeys =
       ImmutableSet.of("yb.security.ldap.ldap_service_account_password", "yb.security.secret");
 
   @Inject
-  public RuntimeConfController(SettableRuntimeConfigFactory settableRuntimeConfigFactory) {
+  public RuntimeConfController(
+      SettableRuntimeConfigFactory settableRuntimeConfigFactory,
+      PlatformInstanceClientFactory platformInstanceClientFactory) {
     this.settableRuntimeConfigFactory = settableRuntimeConfigFactory;
+    this.platformInstanceClientFactory = platformInstanceClientFactory;
+    this.mutableObjects =
+        Sets.newLinkedHashSet(
+            settableRuntimeConfigFactory
+                .staticApplicationConf()
+                .getStringList("runtime_config.included_objects"));
     this.mutableKeys = buildMutableKeysSet();
     this.mutableKeysResult = buildCachedResult();
   }
@@ -91,14 +108,16 @@ public class RuntimeConfController extends AuthenticatedController {
     Config config = settableRuntimeConfigFactory.staticApplicationConf();
     List<String> included = config.getStringList("runtime_config.included_paths");
     List<String> excluded = config.getStringList("runtime_config.excluded_paths");
-    return config
-        .entrySet()
-        .stream()
-        .map(Map.Entry::getKey)
-        .filter(
-            key ->
-                included.stream().anyMatch(key::startsWith)
-                    && excluded.stream().noneMatch(key::startsWith))
+    return Streams.concat(
+            mutableObjects.stream(),
+            config
+                .entrySet()
+                .stream()
+                .map(Entry::getKey)
+                .filter(
+                    key ->
+                        included.stream().anyMatch(key::startsWith)
+                            && excluded.stream().noneMatch(key::startsWith)))
         .collect(Collectors.toSet());
   }
 
@@ -142,7 +161,8 @@ public class RuntimeConfController extends AuthenticatedController {
       LOG.trace(
           "key: {} overriddenInScope: {} includeInherited: {}", k, isOverridden, includeInherited);
 
-      String value = fullConfig.getString(k);
+      String value = fullConfig.getValue(k).render(ConfigRenderOptions.concise());
+      value = unwrap(value);
       if (sensitiveKeys.contains(k)) {
         value = CommonUtils.getMaskedValue(k, value);
       }
@@ -159,6 +179,13 @@ public class RuntimeConfController extends AuthenticatedController {
     return PlatformResults.withData(scopedConfig);
   }
 
+  private String unwrap(String maybeQuoted) {
+    if (maybeQuoted.startsWith("\"") && maybeQuoted.endsWith("\"")) {
+      return maybeQuoted.substring(1, maybeQuoted.length() - 1);
+    }
+    return maybeQuoted;
+  }
+
   @ApiOperation(
       value = "Get a configuration key",
       nickname = "getConfigurationKey",
@@ -220,11 +247,38 @@ public class RuntimeConfController extends AuthenticatedController {
         scopeUUID,
         (logValue.length() < 50 ? logValue : "[long value hidden]"),
         logValue.length());
-    getMutableRuntimeConfigForScopeOrFail(customerUUID, scopeUUID).setValue(path, newValue);
-
+    if (mutableObjects.contains(path)) {
+      getMutableRuntimeConfigForScopeOrFail(customerUUID, scopeUUID).setObject(path, newValue);
+    } else {
+      getMutableRuntimeConfigForScopeOrFail(customerUUID, scopeUUID).setValue(path, newValue);
+    }
+    postConfigChange(customerUUID, scopeUUID, path);
     return YBPSuccess.empty();
   }
 
+  // TODO: In future we can "register" change listeners for specific customer/scope/path
+  // And implement proper subscribe notify mechanism
+  // For now this is just hardcoded here. We can also have a preHook where config change can be
+  // validated and rejected
+  private void postConfigChange(UUID customerUUID, UUID scopeUUID, String path) {
+    try {
+      if (GLOBAL_SCOPE_UUID.equals(scopeUUID)) {
+        if (path.equals("yb.ha.ws")) {
+          platformInstanceClientFactory.refreshWsClient(path);
+          // } else if (path.equals("")) {
+          // invoke handler;
+        }
+      }
+    } catch (RuntimeException e) {
+      // TODO: Should we instead propagate error to caller? Should we rollback and error?
+      LOG.warn(
+          "Ignoring unexpected exception while processing config change for {}:{}:{}",
+          customerUUID,
+          scopeUUID,
+          path);
+    }
+  }
+
   @ApiOperation(value = "Delete a configuration key", response = YBPSuccess.class)
   public Result deleteKey(UUID customerUUID, UUID scopeUUID, String path) {
     if (!mutableKeys.contains(path))
diff --git a/managed/src/main/java/com/yugabyte/yw/controllers/SupportBundleController.java b/managed/src/main/java/com/yugabyte/yw/controllers/SupportBundleController.java
index 5f29eec0f8..bc5f012885 100644
--- a/managed/src/main/java/com/yugabyte/yw/controllers/SupportBundleController.java
+++ b/managed/src/main/java/com/yugabyte/yw/controllers/SupportBundleController.java
@@ -69,7 +69,7 @@ public class SupportBundleController extends AuthenticatedController {
   @ApiOperation(
       value = "Download support bundle",
       nickname = "downloadSupportBundle",
-      response = SupportBundle.class,
+      response = String.class,
       produces = "application/x-compressed")
   public Result get(UUID customerUUID, UUID universeUUID, UUID bundleUUID) {
     Customer customer = Customer.getOrBadRequest(customerUUID);
diff --git a/managed/src/main/java/com/yugabyte/yw/controllers/handlers/UpgradeUniverseHandler.java b/managed/src/main/java/com/yugabyte/yw/controllers/handlers/UpgradeUniverseHandler.java
index 50df8f9c32..e0f64ad0a8 100644
--- a/managed/src/main/java/com/yugabyte/yw/controllers/handlers/UpgradeUniverseHandler.java
+++ b/managed/src/main/java/com/yugabyte/yw/controllers/handlers/UpgradeUniverseHandler.java
@@ -20,7 +20,6 @@ import com.yugabyte.yw.forms.UniverseDefinitionTaskParams;
 import com.yugabyte.yw.forms.UniverseDefinitionTaskParams.UserIntent;
 import com.yugabyte.yw.forms.UpgradeTaskParams;
 import com.yugabyte.yw.forms.VMImageUpgradeParams;
-import com.yugabyte.yw.models.AccessKey;
 import com.yugabyte.yw.models.CertificateInfo;
 import com.yugabyte.yw.models.Customer;
 import com.yugabyte.yw.models.CustomerTask;
diff --git a/managed/src/main/java/com/yugabyte/yw/forms/PlatformInstanceFormData.java b/managed/src/main/java/com/yugabyte/yw/forms/PlatformInstanceFormData.java
index 5e801247ae..81867cd19d 100644
--- a/managed/src/main/java/com/yugabyte/yw/forms/PlatformInstanceFormData.java
+++ b/managed/src/main/java/com/yugabyte/yw/forms/PlatformInstanceFormData.java
@@ -10,14 +10,17 @@
 
 package com.yugabyte.yw.forms;
 
+import org.hibernate.validator.constraints.URL;
 import play.data.validation.Constraints;
 
 public class PlatformInstanceFormData {
+
   @Constraints.Required()
+  @Constraints.MaxLength(263) // "https://" + 255 for dns
   @Constraints.Pattern(
       message = "Must be prefixed with http:// or https://",
       value = "\\b(?:http://|https://).+(/|\\b)")
-  @YugabyteConstraints.ValidURL()
+  @URL
   public String address;
 
   @Constraints.Required() public boolean is_leader;
diff --git a/managed/src/main/java/com/yugabyte/yw/forms/YugabyteConstraints.java b/managed/src/main/java/com/yugabyte/yw/forms/YugabyteConstraints.java
deleted file mode 100644
index 1fbf4af832..0000000000
--- a/managed/src/main/java/com/yugabyte/yw/forms/YugabyteConstraints.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Copyright 2021 YugaByte, Inc. and Contributors
- *
- * Licensed under the Polyform Free Trial License 1.0.0 (the "License"); you
- * may not use this file except in compliance with the License. You
- * may obtain a copy of the License at
- *
- * https://github.com/YugaByte/yugabyte-db/blob/master/licenses/POLYFORM-FREE-TRIAL-LICENSE-1.0.0.txt
- */
-
-package com.yugabyte.yw.forms;
-
-import static java.lang.annotation.ElementType.ANNOTATION_TYPE;
-import static java.lang.annotation.ElementType.CONSTRUCTOR;
-import static java.lang.annotation.ElementType.FIELD;
-import static java.lang.annotation.ElementType.METHOD;
-import static java.lang.annotation.ElementType.PARAMETER;
-import static java.lang.annotation.ElementType.TYPE_USE;
-import static java.lang.annotation.RetentionPolicy.RUNTIME;
-import static play.libs.F.Tuple;
-
-import java.lang.annotation.Retention;
-import java.lang.annotation.Target;
-import java.net.MalformedURLException;
-import java.net.URL;
-import javax.validation.Constraint;
-import javax.validation.ConstraintValidator;
-import javax.validation.Payload;
-import play.data.validation.Constraints;
-import play.libs.F;
-
-public class YugabyteConstraints extends Constraints {
-  @Target({METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER, TYPE_USE})
-  @Retention(RUNTIME)
-  @Constraint(validatedBy = ValidURLValidator.class)
-  @play.data.Form.Display(name = "constraint.invalidURL")
-  public @interface ValidURL {
-    String message() default ValidURLValidator.message;
-
-    Class<?>[] groups() default {};
-
-    Class<? extends Payload>[] payload() default {};
-  }
-
-  /** Validator for {@code @ValidUrl} fields. */
-  public static class ValidURLValidator extends Validator<Object>
-      implements ConstraintValidator<ValidURL, Object> {
-
-    public static final String message = "Invalid URL provided";
-
-    public void initialize(ValidURL constraintAnnotation) {}
-
-    public boolean isValid(Object object) {
-      if (!(object instanceof String)) {
-        return false;
-      }
-
-      try {
-        new URL((String) object);
-      } catch (MalformedURLException e) {
-        return false;
-      }
-
-      return true;
-    }
-
-    public F.Tuple<String, Object[]> getErrorMessageKey() {
-      return Tuple(message, new Object[] {});
-    }
-  }
-}
diff --git a/managed/src/main/java/com/yugabyte/yw/models/AlertConfiguration.java b/managed/src/main/java/com/yugabyte/yw/models/AlertConfiguration.java
index c64224321d..9a3a661657 100644
--- a/managed/src/main/java/com/yugabyte/yw/models/AlertConfiguration.java
+++ b/managed/src/main/java/com/yugabyte/yw/models/AlertConfiguration.java
@@ -183,7 +183,7 @@ public class AlertConfiguration extends Model {
   @ApiModelProperty(
       value = "Duration in seconds, while condition is met to raise an alert",
       accessMode = READ_WRITE)
-  private Integer durationSec = 15;
+  private Integer durationSec = 0;
 
   @NotNull
   @ApiModelProperty(value = "Is configured alerts raised or not", accessMode = READ_WRITE)
diff --git a/managed/src/main/java/com/yugabyte/yw/models/Metric.java b/managed/src/main/java/com/yugabyte/yw/models/Metric.java
index 8023a843d4..119bf199e6 100644
--- a/managed/src/main/java/com/yugabyte/yw/models/Metric.java
+++ b/managed/src/main/java/com/yugabyte/yw/models/Metric.java
@@ -2,50 +2,29 @@
 
 package com.yugabyte.yw.models;
 
-import static com.yugabyte.yw.models.helpers.CommonUtils.DB_OR_CHAIN_TO_WARN;
-import static com.yugabyte.yw.models.helpers.CommonUtils.appendInClause;
 import static com.yugabyte.yw.models.helpers.CommonUtils.nowWithoutMillis;
-import static com.yugabyte.yw.models.helpers.CommonUtils.setUniqueListValue;
-import static com.yugabyte.yw.models.helpers.CommonUtils.setUniqueListValues;
 
-import com.fasterxml.jackson.annotation.JsonFormat;
-import com.fasterxml.jackson.annotation.JsonIgnore;
-import com.yugabyte.yw.models.filters.MetricFilter;
 import com.yugabyte.yw.models.helpers.KnownAlertLabels;
-import io.ebean.ExpressionList;
-import io.ebean.Finder;
-import io.ebean.Junction;
-import io.ebean.Model;
-import io.ebean.PersistenceContextScope;
 import io.prometheus.client.Collector;
-import java.util.Comparator;
 import java.util.Date;
-import java.util.List;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+import java.util.TreeMap;
 import java.util.UUID;
 import java.util.stream.Collectors;
-import javax.persistence.Access;
-import javax.persistence.AccessType;
-import javax.persistence.CascadeType;
-import javax.persistence.Column;
-import javax.persistence.Entity;
-import javax.persistence.EnumType;
-import javax.persistence.Enumerated;
-import javax.persistence.Id;
-import javax.persistence.OneToMany;
 import lombok.Data;
 import lombok.EqualsAndHashCode;
 import lombok.experimental.Accessors;
 import lombok.extern.slf4j.Slf4j;
-import org.apache.commons.collections.CollectionUtils;
-import org.apache.commons.lang3.StringUtils;
 
-@Entity
 @Data
 @Accessors(chain = true)
 @EqualsAndHashCode(callSuper = false)
 @Slf4j
-@Access(AccessType.PROPERTY)
-public class Metric extends Model {
+public class Metric {
 
   // For now only support gauge.
   public enum Type {
@@ -62,173 +41,63 @@ public class Metric extends Model {
     }
   }
 
-  @Id private UUID uuid;
+  private UUID customerUUID;
 
-  @Column private UUID customerUUID;
-
-  @Column(nullable = false)
   private String name;
 
-  @Enumerated(EnumType.STRING)
   private Type type;
 
-  @Column(nullable = false)
-  @JsonFormat(shape = JsonFormat.Shape.STRING, pattern = "yyyy-MM-dd HH:mm:ss")
   private Date createTime = nowWithoutMillis();
 
-  @Column(nullable = false)
-  @JsonFormat(shape = JsonFormat.Shape.STRING, pattern = "yyyy-MM-dd HH:mm:ss")
   private Date updateTime = nowWithoutMillis();
 
-  @Column(nullable = false)
-  @JsonFormat(shape = JsonFormat.Shape.STRING, pattern = "yyyy-MM-dd HH:mm:ss")
   private Date expireTime;
 
-  @Column private UUID sourceUuid;
+  private UUID sourceUuid;
 
-  @Column private String sourceLabels;
+  private Set<String> keyLabels = new HashSet<>();
 
-  @OneToMany(mappedBy = "metric", cascade = CascadeType.ALL, orphanRemoval = true)
-  private List<MetricLabel> labels;
+  private Map<String, String> labels = new TreeMap<>();
 
-  @Column(nullable = false)
   private Double value;
 
-  private static final Finder<UUID, Metric> find = new Finder<UUID, Metric>(Metric.class) {};
-
-  @JsonIgnore
-  public boolean isNew() {
-    return uuid == null;
-  }
-
-  public Metric setUuid(UUID uuid) {
-    this.uuid = uuid;
-    this.labels.forEach(label -> label.setMetric(this));
-    return this;
-  }
-
-  public Metric generateUUID() {
-    return setUuid(UUID.randomUUID());
-  }
+  private boolean deleted;
 
   public String getLabelValue(KnownAlertLabels knownLabel) {
     return getLabelValue(knownLabel.labelName());
   }
 
   public String getLabelValue(String name) {
-    return labels
-        .stream()
-        .filter(label -> name.equals(label.getName()))
-        .map(MetricLabel::getValue)
-        .findFirst()
-        .orElse(null);
-  }
-
-  public Metric setLabel(KnownAlertLabels label, String value) {
-    return setLabel(label.labelName(), value);
-  }
-
-  public Metric setLabel(String name, String value) {
-    return setLabel(name, value, false);
+    return labels.get(name);
   }
 
   public Metric setKeyLabel(KnownAlertLabels label, String value) {
-    return setKeyLabel(label.labelName(), value);
+    setLabel(label.labelName(), value);
+    keyLabels.add(label.labelName());
+    return this;
   }
 
-  public Metric setKeyLabel(String name, String value) {
-    return setLabel(name, value, true);
+  public Metric setLabel(KnownAlertLabels label, String value) {
+    return setLabel(label.labelName(), value);
   }
 
-  public Metric setLabel(String name, String value, boolean keyLabel) {
-    MetricLabel toAdd = new MetricLabel(this, name, value);
-    toAdd.setSourceLabel(keyLabel);
-    this.labels = setUniqueListValue(labels, toAdd);
+  public Metric setLabel(String name, String value) {
+    labels.put(name, value);
+    keyLabels.remove(name);
     return this;
   }
 
-  public Metric setLabels(List<MetricLabel> labels) {
-    this.labels = setUniqueListValues(this.labels, labels);
-    this.labels.forEach(label -> label.setMetric(this));
+  public Metric setLabels(Map<String, String> labels) {
+    this.labels = new HashMap<>(labels);
+    this.keyLabels.clear();
     return this;
   }
 
-  public List<MetricLabel> getLabels() {
-    return labels
-        .stream()
-        .sorted(Comparator.comparing(MetricLabel::getName))
-        .collect(Collectors.toList());
-  }
-
-  public static ExpressionList<Metric> createQueryByFilter(MetricFilter filter) {
-    ExpressionList<Metric> query =
-        find.query()
-            .setPersistenceContextScope(PersistenceContextScope.QUERY)
-            .fetch("labels")
-            .where();
-    if (filter.getCustomerUuid() != null) {
-      query.eq("customerUUID", filter.getCustomerUuid());
-    }
-    if (filter.getSourceUuid() != null) {
-      query.eq("sourceUuid", filter.getSourceUuid());
-    }
-    appendInClause(query, "name", filter.getMetricNames());
-    if (!CollectionUtils.isEmpty(filter.getKeys())
-        || !CollectionUtils.isEmpty(filter.getSourceKeys())) {
-      if (filter.getKeys().size() + filter.getSourceKeys().size() > DB_OR_CHAIN_TO_WARN) {
-        log.warn("Querying for {} metric keys - may affect performance", filter.getKeys().size());
-      }
-      Junction<Metric> orExpr = query.or();
-      for (MetricKey key : filter.getKeys()) {
-        Junction<Metric> andExpr = orExpr.and();
-        MetricSourceKey sourceKey = key.getSourceKey();
-        appendMetricSourceKey(andExpr, sourceKey);
-        if (!StringUtils.isEmpty(key.getSourceLabels())) {
-          andExpr.eq("sourceLabels", key.getSourceLabels());
-        } else {
-          andExpr.isNull("sourceLabels");
-        }
-        orExpr.endAnd();
-      }
-      for (MetricSourceKey sourceKey : filter.getSourceKeys()) {
-        Junction<Metric> andExpr = orExpr.and();
-        appendMetricSourceKey(andExpr, sourceKey);
-        orExpr.endAnd();
-      }
-      query.endOr();
-    }
-    if (filter.getExpired() != null) {
-      if (filter.getExpired()) {
-        query.lt("expireTime", nowWithoutMillis());
-      } else {
-        query.gt("expireTime", nowWithoutMillis());
-      }
-    }
-    return query;
-  }
-
-  private static void appendMetricSourceKey(Junction<Metric> andExpr, MetricSourceKey key) {
-    andExpr.eq("name", key.getName());
-    if (key.getCustomerUuid() != null) {
-      andExpr.eq("customerUUID", key.getCustomerUuid());
-    } else {
-      andExpr.isNull("customerUUID");
-    }
-    if (key.getSourceUuid() != null) {
-      andExpr.eq("sourceUuid", key.getSourceUuid());
-    } else {
-      andExpr.isNull("sourceUuid");
-    }
-  }
-
-  public static String getSourceLabelsStr(List<MetricLabel> labels) {
-    if (CollectionUtils.isEmpty(labels)) {
-      return null;
-    }
+  public Map<String, String> getKeyLabelValues() {
     return labels
+        .entrySet()
         .stream()
-        .sorted(Comparator.comparing(MetricLabel::getName))
-        .map(label -> label.getName() + ":" + label.getValue())
-        .collect(Collectors.joining(","));
+        .filter(e -> keyLabels.contains(e.getKey()))
+        .collect(Collectors.toMap(Entry::getKey, Entry::getValue));
   }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/models/MetricKey.java b/managed/src/main/java/com/yugabyte/yw/models/MetricKey.java
index 461a59d731..ad892a72c8 100644
--- a/managed/src/main/java/com/yugabyte/yw/models/MetricKey.java
+++ b/managed/src/main/java/com/yugabyte/yw/models/MetricKey.java
@@ -2,8 +2,9 @@
 
 package com.yugabyte.yw.models;
 
+import java.util.HashMap;
+import java.util.Map;
 import java.util.UUID;
-import java.util.stream.Collectors;
 import lombok.Builder;
 import lombok.EqualsAndHashCode;
 import lombok.Value;
@@ -13,12 +14,13 @@ import lombok.Value;
 @EqualsAndHashCode
 public class MetricKey {
   MetricSourceKey sourceKey;
-  String sourceLabels;
+  Map<String, String> sourceLabels;
 
   public static class MetricKeyBuilder {
     private UUID customerUuid;
     private String name;
     private UUID sourceUuid;
+    Map<String, String> sourceLabels = new HashMap<>();
 
     public MetricKeyBuilder customerUuid(UUID customerUuid) {
       this.customerUuid = customerUuid;
@@ -35,6 +37,16 @@ public class MetricKey {
       return this;
     }
 
+    public MetricKeyBuilder sourceLabels(Map<String, String> sourceLabels) {
+      this.sourceLabels = sourceLabels;
+      return this;
+    }
+
+    public MetricKeyBuilder sourceLabel(String name, String value) {
+      this.sourceLabels.put(name, value);
+      return this;
+    }
+
     public MetricKey build() {
       MetricSourceKey sourceKey = this.sourceKey;
       if (sourceKey == null) {
@@ -52,13 +64,7 @@ public class MetricKey {
   public static MetricKey from(Metric metric) {
     return MetricKey.builder()
         .sourceKey(MetricSourceKey.from(metric))
-        .sourceLabels(
-            Metric.getSourceLabelsStr(
-                metric
-                    .getLabels()
-                    .stream()
-                    .filter(MetricLabel::isSourceLabel)
-                    .collect(Collectors.toList())))
+        .sourceLabels(metric.getKeyLabelValues())
         .build();
   }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/models/MetricLabel.java b/managed/src/main/java/com/yugabyte/yw/models/MetricLabel.java
deleted file mode 100644
index 983b91cd8b..0000000000
--- a/managed/src/main/java/com/yugabyte/yw/models/MetricLabel.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Copyright 2021 YugaByte, Inc. and Contributors
- *
- * Licensed under the Polyform Free Trial License 1.0.0 (the "License"); you
- * may not use this file except in compliance with the License. You
- * may obtain a copy of the License at
- *
- * http://github.com/YugaByte/yugabyte-db/blob/master/licenses/POLYFORM-FREE-TRIAL-LICENSE-1.0.0.txt
- */
-
-package com.yugabyte.yw.models;
-
-import com.fasterxml.jackson.annotation.JsonIgnore;
-import com.yugabyte.yw.models.helpers.KnownAlertLabels;
-import com.yugabyte.yw.models.helpers.UniqueKeyListValue;
-import io.ebean.Model;
-import java.util.Objects;
-import javax.persistence.Column;
-import javax.persistence.EmbeddedId;
-import javax.persistence.Entity;
-import javax.persistence.ManyToOne;
-import lombok.Data;
-import lombok.EqualsAndHashCode;
-import lombok.ToString;
-
-@Data
-@Entity
-@EqualsAndHashCode(callSuper = false)
-@ToString
-public class MetricLabel extends Model implements UniqueKeyListValue<MetricLabel> {
-
-  @EmbeddedId private MetricLabelKey key;
-
-  @Column(nullable = false)
-  private String value;
-
-  @ManyToOne @JsonIgnore @EqualsAndHashCode.Exclude @ToString.Exclude private Metric metric;
-
-  @Column(nullable = false)
-  private boolean sourceLabel;
-
-  public MetricLabel() {
-    this.key = new MetricLabelKey();
-  }
-
-  public MetricLabel(String name, String value) {
-    this();
-    key.setName(name);
-    this.value = value;
-  }
-
-  public MetricLabel(KnownAlertLabels label, String value) {
-    this(label.labelName(), value);
-  }
-
-  public MetricLabel(Metric metric, String name, String value) {
-    this(name, value);
-    setMetric(metric);
-  }
-
-  public String getName() {
-    return key.getName();
-  }
-
-  public void setMetric(Metric metric) {
-    this.metric = metric;
-    key.setMetricUuid(metric.getUuid());
-  }
-
-  @Override
-  @JsonIgnore
-  public boolean keyEquals(MetricLabel other) {
-    return Objects.equals(getName(), other.getName());
-  }
-
-  @Override
-  @JsonIgnore
-  public boolean valueEquals(MetricLabel other) {
-    return keyEquals(other) && Objects.equals(getValue(), other.getValue());
-  }
-}
diff --git a/managed/src/main/java/com/yugabyte/yw/models/MetricLabelKey.java b/managed/src/main/java/com/yugabyte/yw/models/MetricLabelKey.java
deleted file mode 100644
index 5a09a5d18a..0000000000
--- a/managed/src/main/java/com/yugabyte/yw/models/MetricLabelKey.java
+++ /dev/null
@@ -1,27 +0,0 @@
-/*
- * Copyright 2021 YugaByte, Inc. and Contributors
- *
- * Licensed under the Polyform Free Trial License 1.0.0 (the "License"); you
- * may not use this file except in compliance with the License. You
- * may obtain a copy of the License at
- *
- * http://github.com/YugaByte/yugabyte-db/blob/master/licenses/POLYFORM-FREE-TRIAL-LICENSE-1.0.0.txt
- */
-
-package com.yugabyte.yw.models;
-
-import com.fasterxml.jackson.annotation.JsonIgnore;
-import lombok.Data;
-import lombok.EqualsAndHashCode;
-
-import javax.persistence.Embeddable;
-import java.io.Serializable;
-import java.util.UUID;
-
-@Embeddable
-@Data
-@EqualsAndHashCode(callSuper = false)
-public class MetricLabelKey implements Serializable {
-  @JsonIgnore private UUID metricUuid;
-  private String name;
-}
diff --git a/managed/src/main/java/com/yugabyte/yw/models/NodeInstance.java b/managed/src/main/java/com/yugabyte/yw/models/NodeInstance.java
index cb74dfbc65..b6da07111f 100644
--- a/managed/src/main/java/com/yugabyte/yw/models/NodeInstance.java
+++ b/managed/src/main/java/com/yugabyte/yw/models/NodeInstance.java
@@ -20,6 +20,7 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
+import java.util.Optional;
 import java.util.UUID;
 import javax.persistence.Column;
 import javax.persistence.Entity;
@@ -29,6 +30,7 @@ import lombok.Data;
 import lombok.EqualsAndHashCode;
 import lombok.Getter;
 import lombok.Setter;
+import org.apache.commons.collections.CollectionUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import play.libs.Json;
@@ -216,11 +218,19 @@ public class NodeInstance extends Model {
   // TODO: this is a temporary hack until we manage to plumb through the node UUID through the task
   // framework.
   public static NodeInstance getByName(String name) {
+    return maybeGetByName(name)
+        .orElseThrow(() -> new RuntimeException("Expecting to find a node with name: " + name));
+  }
+
+  public static Optional<NodeInstance> maybeGetByName(String name) {
     List<NodeInstance> nodes = NodeInstance.find.query().where().eq("node_name", name).findList();
-    if (nodes == null || nodes.size() != 1) {
+    if (CollectionUtils.isEmpty(nodes)) {
+      return Optional.empty();
+    }
+    if (nodes.size() > 1) {
       throw new RuntimeException("Expecting to find a single node with name: " + name);
     }
-    return nodes.get(0);
+    return Optional.of(nodes.get(0));
   }
 
   public static NodeInstance create(UUID zoneUuid, NodeInstanceData formData) {
diff --git a/managed/src/main/java/com/yugabyte/yw/models/ScopedRuntimeConfig.java b/managed/src/main/java/com/yugabyte/yw/models/ScopedRuntimeConfig.java
index a7cdf3fdbe..50552e1e84 100644
--- a/managed/src/main/java/com/yugabyte/yw/models/ScopedRuntimeConfig.java
+++ b/managed/src/main/java/com/yugabyte/yw/models/ScopedRuntimeConfig.java
@@ -107,7 +107,7 @@ public class ScopedRuntimeConfig extends Model {
 
   @Override
   public String toString() {
-    if (uuid == GLOBAL_SCOPE_UUID) {
+    if (GLOBAL_SCOPE_UUID.equals(uuid)) {
       return "ScopedRuntimeConfig(GLOBAL_SCOPE)";
     } else {
       return "ScopedRuntimeConfig{"
diff --git a/managed/src/main/java/com/yugabyte/yw/models/TaskInfo.java b/managed/src/main/java/com/yugabyte/yw/models/TaskInfo.java
index ff0085a7b5..081e8f1611 100644
--- a/managed/src/main/java/com/yugabyte/yw/models/TaskInfo.java
+++ b/managed/src/main/java/com/yugabyte/yw/models/TaskInfo.java
@@ -183,6 +183,18 @@ public class TaskInfo extends Model {
     return details;
   }
 
+  @JsonIgnore
+  public String getErrorMessage() {
+    if (details == null || taskState == State.Success) {
+      return null;
+    }
+    JsonNode node = details.get("errorString");
+    if (node == null || node.isNull()) {
+      return null;
+    }
+    return node.asText();
+  }
+
   public State getTaskState() {
     return taskState;
   }
diff --git a/managed/src/main/java/com/yugabyte/yw/models/filters/MetricFilter.java b/managed/src/main/java/com/yugabyte/yw/models/filters/MetricFilter.java
index 5455cae715..125782b9b1 100644
--- a/managed/src/main/java/com/yugabyte/yw/models/filters/MetricFilter.java
+++ b/managed/src/main/java/com/yugabyte/yw/models/filters/MetricFilter.java
@@ -32,12 +32,42 @@ public class MetricFilter {
   Set<String> metricNames;
   Set<MetricSourceKey> sourceKeys;
   Set<MetricKey> keys;
+  Set<MetricKey> keysExcluded;
   Boolean expired;
 
+  // Can't use @Builder(toBuilder = true) as it sets null fields as well, which breaks non null
+  // checks.
+  public MetricFilterBuilder toBuilder() {
+    MetricFilterBuilder result = MetricFilter.builder();
+    if (customerUuid != null) {
+      result.customerUuid(customerUuid);
+    }
+    if (sourceUuid != null) {
+      result.sourceUuid(sourceUuid);
+    }
+    if (metricNames != null) {
+      result.metricNamesStr(metricNames);
+    }
+    if (sourceKeys != null) {
+      result.sourceKeys(sourceKeys);
+    }
+    if (keys != null) {
+      result.keys(keys);
+    }
+    if (keysExcluded != null) {
+      result.keysExcluded(keysExcluded);
+    }
+    if (expired != null) {
+      result.expired(expired);
+    }
+    return result;
+  }
+
   public static class MetricFilterBuilder {
     Set<String> metricNames = new HashSet<>();
     Set<MetricSourceKey> sourceKeys = new HashSet<>();
     Set<MetricKey> keys = new HashSet<>();
+    Set<MetricKey> keysExcluded = new HashSet<>();
 
     public MetricFilterBuilder customerUuid(@NonNull UUID customerUuid) {
       this.customerUuid = customerUuid;
@@ -58,6 +88,11 @@ public class MetricFilter {
       return this;
     }
 
+    public MetricFilterBuilder metricNamesStr(@NonNull Collection<String> names) {
+      this.metricNames.addAll(names);
+      return this;
+    }
+
     public MetricFilterBuilder metricName(@NonNull PlatformMetrics platformMetric) {
       this.metricNames.add(platformMetric.getMetricName());
       return this;
@@ -83,6 +118,16 @@ public class MetricFilter {
       return this;
     }
 
+    public MetricFilterBuilder keysExcluded(@NonNull Collection<MetricKey> keysExcluded) {
+      this.keysExcluded.addAll(keysExcluded);
+      return this;
+    }
+
+    public MetricFilterBuilder keyExcluded(@NonNull MetricKey keyExcluded) {
+      this.keysExcluded.add(keyExcluded);
+      return this;
+    }
+
     public MetricFilterBuilder expireTime(@NonNull Boolean expired) {
       this.expired = expired;
       return this;
@@ -106,9 +151,17 @@ public class MetricFilter {
     if (CollectionUtils.isNotEmpty(keys) && !keys.contains(metricKey)) {
       return false;
     }
+    if (CollectionUtils.isNotEmpty(keysExcluded) && keysExcluded.contains(metricKey)) {
+      return false;
+    }
     if (expired != null) {
-      return metric.getExpireTime().before(new Date()) == expired;
+      if (expired && metric.getExpireTime().after(new Date())) {
+        return false;
+      }
+      if (!expired && metric.getExpireTime().before(new Date())) {
+        return false;
+      }
     }
-    return true;
+    return !metric.isDeleted();
   }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/models/helpers/KnownAlertLabels.java b/managed/src/main/java/com/yugabyte/yw/models/helpers/KnownAlertLabels.java
index 0bbaf94022..2caae64641 100644
--- a/managed/src/main/java/com/yugabyte/yw/models/helpers/KnownAlertLabels.java
+++ b/managed/src/main/java/com/yugabyte/yw/models/helpers/KnownAlertLabels.java
@@ -24,7 +24,6 @@ public enum KnownAlertLabels {
   ALERT_STATE,
   SEVERITY,
   THRESHOLD,
-  ERROR_MESSAGE,
   NODE_NAME,
   NODE_PREFIX,
   INSTANCE,
diff --git a/managed/src/main/java/com/yugabyte/yw/models/helpers/MetricSourceState.java b/managed/src/main/java/com/yugabyte/yw/models/helpers/MetricSourceState.java
new file mode 100644
index 0000000000..fd3d4a6af7
--- /dev/null
+++ b/managed/src/main/java/com/yugabyte/yw/models/helpers/MetricSourceState.java
@@ -0,0 +1,16 @@
+/*
+ * Copyright 2022 YugaByte, Inc. and Contributors
+ *
+ * Licensed under the Polyform Free Trial License 1.0.0 (the "License"); you
+ * may not use this file except in compliance with the License. You
+ * may obtain a copy of the License at
+ *
+ * http://github.com/YugaByte/yugabyte-db/blob/master/licenses/POLYFORM-FREE-TRIAL-LICENSE-1.0.0.txt
+ */
+package com.yugabyte.yw.models.helpers;
+
+public enum MetricSourceState {
+  ACTIVE,
+  INACTIVE,
+  REMOVED
+}
diff --git a/managed/src/main/java/com/yugabyte/yw/models/helpers/NodeDetails.java b/managed/src/main/java/com/yugabyte/yw/models/helpers/NodeDetails.java
index c213756ff0..bf7a759aea 100644
--- a/managed/src/main/java/com/yugabyte/yw/models/helpers/NodeDetails.java
+++ b/managed/src/main/java/com/yugabyte/yw/models/helpers/NodeDetails.java
@@ -100,7 +100,7 @@ public class NodeDetails {
     // Set after the node has been removed (unjoined) from the cluster.
     Removed(ADD, RELEASE),
     // Set when node is about to enter the Live state from Removed/Decommissioned state.
-    Adding(DELETE),
+    Adding(DELETE, RELEASE),
     // Set when a stopped/removed node is about to enter the Decommissioned state.
     // The actions in Removed state should apply because of the transition from Removed to
     // BeingDecommissioned.
diff --git a/managed/src/main/java/com/yugabyte/yw/models/helpers/PlatformMetrics.java b/managed/src/main/java/com/yugabyte/yw/models/helpers/PlatformMetrics.java
index a76875c399..84c7109c17 100644
--- a/managed/src/main/java/com/yugabyte/yw/models/helpers/PlatformMetrics.java
+++ b/managed/src/main/java/com/yugabyte/yw/models/helpers/PlatformMetrics.java
@@ -10,9 +10,15 @@
 
 package com.yugabyte.yw.models.helpers;
 
+import com.google.common.collect.ImmutableSet;
 import com.yugabyte.yw.models.common.Unit;
+import java.util.Arrays;
+import java.util.Set;
+import java.util.stream.Collectors;
+import lombok.Getter;
 import org.apache.commons.lang3.StringUtils;
 
+@Getter
 public enum PlatformMetrics {
   // Health check common
   HEALTH_CHECK_STATUS("Health check status for universe", Unit.STATUS),
@@ -91,26 +97,31 @@ public enum PlatformMetrics {
   ALERT_MANAGER_CHANNEL_STATUS("Alert manager channel status", Unit.STATUS),
   METRIC_PROCESSOR_STATUS("Platform metrics processor status", Unit.STATUS),
 
-  UNIVERSE_EXISTS("Flag, indicating that universe exists", Unit.STATUS),
-  UNIVERSE_PAUSED("Flag, indicating that universe is paused", Unit.STATUS),
-  UNIVERSE_UPDATE_IN_PROGRESS("Flag, indicating that universe update is in progress", Unit.STATUS),
   UNIVERSE_BACKUP_IN_PROGRESS("Flag, indicating that universe backup is in progress", Unit.STATUS),
-  UNIVERSE_NODE_FUNCTION("Flag, indicating expected node functions", Unit.STATUS);
+  UNIVERSE_EXISTS("Flag, indicating that universe exists", Unit.STATUS, false),
+  UNIVERSE_PAUSED("Flag, indicating that universe is paused", Unit.STATUS, false),
+  UNIVERSE_UPDATE_IN_PROGRESS(
+      "Flag, indicating that universe update is in progress", Unit.STATUS, false),
+  UNIVERSE_NODE_FUNCTION("Flag, indicating expected node functions", Unit.STATUS, false),
+  UNIVERSE_REPLICATION_FACTOR("Universe replication factor", Unit.COUNT, false);
 
   private final String help;
   private final Unit unit;
+  private final Set<MetricSourceState> validForSourceStates;
 
+  // By default metrics are valid only for active source
   PlatformMetrics(String help, Unit unit) {
-    this.help = help;
-    this.unit = unit;
+    this(help, unit, true);
   }
 
-  public String getHelp() {
-    return help;
-  }
-
-  public Unit getUnit() {
-    return unit;
+  PlatformMetrics(String help, Unit unit, boolean onlyActive) {
+    Set<MetricSourceState> validForSourceStates =
+        onlyActive
+            ? ImmutableSet.of(MetricSourceState.ACTIVE)
+            : ImmutableSet.of(MetricSourceState.ACTIVE, MetricSourceState.INACTIVE);
+    this.help = help;
+    this.unit = unit;
+    this.validForSourceStates = validForSourceStates;
   }
 
   public String getUnitName() {
@@ -121,4 +132,14 @@ public enum PlatformMetrics {
     // ybp is required to list all platform alerts in Prometheus UI by prefix
     return "ybp_" + name().toLowerCase();
   }
+
+  public static Set<PlatformMetrics> invalidForState(MetricSourceState state) {
+    return Arrays.stream(values())
+        .filter(m -> !m.getValidForSourceStates().contains(state))
+        .collect(Collectors.toSet());
+  }
+
+  public static PlatformMetrics fromMetricName(String metricName) {
+    return valueOf(metricName.substring(4).toUpperCase());
+  }
 }
diff --git a/managed/src/main/java/com/yugabyte/yw/scheduler/Scheduler.java b/managed/src/main/java/com/yugabyte/yw/scheduler/Scheduler.java
index 637c1942f0..bdb1190ae0 100644
--- a/managed/src/main/java/com/yugabyte/yw/scheduler/Scheduler.java
+++ b/managed/src/main/java/com/yugabyte/yw/scheduler/Scheduler.java
@@ -59,8 +59,8 @@ import scala.concurrent.duration.Duration;
 @Slf4j
 public class Scheduler {
 
-  private final int YB_SCHEDULER_INTERVAL = 2;
-  private final int MIN_TO_SEC = 60;
+  private static final int YB_SCHEDULER_INTERVAL = 2;
+  private static final int MIN_TO_SEC = 60;
 
   private final ActorSystem actorSystem;
   private final ExecutionContext executionContext;
@@ -113,8 +113,7 @@ public class Scheduler {
         }
         TaskType taskType = schedule.getTaskType();
         // TODO: Come back and maybe address if using relations between schedule and
-        //  schedule_task is
-        // a better approach.
+        //  schedule_task is a better approach.
         ScheduleTask lastTask = ScheduleTask.getLastTask(schedule.getScheduleUUID());
         Date lastScheduledTime = null;
         Date lastCompletedTime = null;
@@ -124,7 +123,7 @@ public class Scheduler {
         }
         boolean shouldRunTask = false;
         boolean alreadyRunning = false;
-        long diff = 0;
+        long diff;
 
         // Check if task needs to be scheduled again.
         if (lastScheduledTime != null) {
diff --git a/managed/src/main/resources/db/migration/default/common/V150__Drop_Metric_Tables.sql b/managed/src/main/resources/db/migration/default/common/V150__Drop_Metric_Tables.sql
new file mode 100644
index 0000000000..3641729c34
--- /dev/null
+++ b/managed/src/main/resources/db/migration/default/common/V150__Drop_Metric_Tables.sql
@@ -0,0 +1,4 @@
+-- Copyright (c) YugaByte, Inc.
+
+DROP TABLE IF EXISTS metric_label;
+DROP TABLE IF EXISTS metric;
diff --git a/managed/src/main/resources/db/migration/default/common/V166__Fix_DNS_Size.sql b/managed/src/main/resources/db/migration/default/common/V166__Fix_DNS_Size.sql
new file mode 100644
index 0000000000..68df837616
--- /dev/null
+++ b/managed/src/main/resources/db/migration/default/common/V166__Fix_DNS_Size.sql
@@ -0,0 +1,2 @@
+-- Copyright (c) YugaByte, Inc.
+ALTER TABLE platform_instance ALTER COLUMN address TYPE varchar(300);
diff --git a/managed/src/main/resources/db/migration/default/h2/V151__Remove_Message_Labels.sql b/managed/src/main/resources/db/migration/default/h2/V151__Remove_Message_Labels.sql
new file mode 100644
index 0000000000..639607f5b4
--- /dev/null
+++ b/managed/src/main/resources/db/migration/default/h2/V151__Remove_Message_Labels.sql
@@ -0,0 +1,3 @@
+-- Copyright (c) YugaByte, Inc.
+
+-- No need to create anything for h2.
diff --git a/managed/src/main/resources/db/migration/default/h2/V167__Add_Underreplicated_Masters_Alert.sql b/managed/src/main/resources/db/migration/default/h2/V167__Add_Underreplicated_Masters_Alert.sql
new file mode 100644
index 0000000000..ea967d4940
--- /dev/null
+++ b/managed/src/main/resources/db/migration/default/h2/V167__Add_Underreplicated_Masters_Alert.sql
@@ -0,0 +1,3 @@
+-- Copyright (c) YugaByte, Inc.
+
+alter table alert_configuration alter column duration_sec set default 0;
diff --git a/managed/src/main/resources/db/migration/default/postgres/V151__Remove_Message_Labels.sql b/managed/src/main/resources/db/migration/default/postgres/V151__Remove_Message_Labels.sql
new file mode 100644
index 0000000000..b68e2aa1ad
--- /dev/null
+++ b/managed/src/main/resources/db/migration/default/postgres/V151__Remove_Message_Labels.sql
@@ -0,0 +1,47 @@
+-- Copyright (c) YugaByte, Inc.
+
+-- HEALTH_CHECK_ERROR
+select replace_configuration_query(
+ 'HEALTH_CHECK_ERROR',
+ 'last_over_time(ybp_health_check_status{universe_uuid = "__universeUuid__"}[1d])'
+    || ' {{ query_condition }} 1');
+
+-- HEALTH_CHECK_NOTIFICATION_ERROR
+select replace_configuration_query(
+ 'HEALTH_CHECK_NOTIFICATION_ERROR',
+ 'last_over_time(ybp_health_check_notification_status{universe_uuid = "__universeUuid__"}[1d])'
+    || ' {{ query_condition }} 1');
+
+-- BACKUP_FAILURE
+select replace_configuration_query(
+ 'BACKUP_FAILURE',
+ 'last_over_time(ybp_create_backup_status{universe_uuid = "__universeUuid__"}[1d])'
+    || ' {{ query_condition }} 1');
+
+-- BACKUP_SCHEDULE_FAILURE
+select replace_configuration_query(
+ 'BACKUP_SCHEDULE_FAILURE',
+ 'last_over_time(ybp_schedule_backup_status{universe_uuid = "__universeUuid__"}[1d])'
+    || ' {{ query_condition }} 1');
+
+-- ALERT_QUERY_FAILED
+select replace_configuration_query(
+ 'ALERT_QUERY_FAILED',
+ 'last_over_time(ybp_alert_query_status[1d]) {{ query_condition }} 1');
+
+-- ALERT_CONFIG_WRITING_FAILED
+select replace_configuration_query(
+ 'ALERT_CONFIG_WRITING_FAILED',
+ 'last_over_time(ybp_alert_config_writer_status[1d]) {{ query_condition }} 1');
+
+-- ALERT_NOTIFICATION_ERROR
+select replace_configuration_query(
+ 'ALERT_NOTIFICATION_ERROR',
+ 'last_over_time(ybp_alert_manager_status{customer_uuid = "__customerUuid__"}[1d])'
+    || ' {{ query_condition }} 1');
+
+-- ALERT_NOTIFICATION_CHANNEL_ERROR
+select replace_configuration_query(
+ 'ALERT_NOTIFICATION_CHANNEL_ERROR',
+ 'last_over_time(ybp_alert_manager_channel_status{customer_uuid = "__customerUuid__"}[1d])'
+    || ' {{ query_condition }} 1');
diff --git a/managed/src/main/resources/db/migration/default/postgres/V167__Add_Underreplicated_Masters_Alert.sql b/managed/src/main/resources/db/migration/default/postgres/V167__Add_Underreplicated_Masters_Alert.sql
new file mode 100644
index 0000000000..3899dbd078
--- /dev/null
+++ b/managed/src/main/resources/db/migration/default/postgres/V167__Add_Underreplicated_Masters_Alert.sql
@@ -0,0 +1,35 @@
+-- Copyright (c) YugaByte, Inc.
+
+-- MASTER_UNDER_REPLICATED
+insert into alert_configuration
+  (uuid, customer_uuid, name, description, create_time, target_type, target, thresholds, threshold_unit, template, active, default_destination)
+select
+  gen_random_uuid(),
+  uuid,
+  'Under-replicated master',
+  'Master is missing from raft group or has follower lag higher than threshold',
+  current_timestamp,
+  'UNIVERSE',
+  '{"all":true}',
+  '{"SEVERE":{"condition":"GREATER_THAN", "threshold":900}}',
+  'SECOND',
+  'MASTER_UNDER_REPLICATED',
+  true,
+  true
+from customer;
+
+select create_universe_alert_definitions(
+ 'Under-replicated master',
+ '(min_over_time((ybp_universe_replication_factor{node_prefix="__nodePrefix__"} - on(node_prefix)'
+    || ' count by(node_prefix) (count by (node_prefix, exported_instance)(follower_lag_ms'
+    || '{export_type="master_export", node_prefix="__nodePrefix__"})))[{{ query_threshold }}s:]) > 0'
+    || ' or (max by(node_prefix) (follower_lag_ms{export_type="master_export",'
+    || ' node_prefix="__nodePrefix__"}) {{ query_condition }} ({{ query_threshold }} * 1000)))');
+
+-- As we now evaluate alerting rules each 2 minutes - we want alert to be raised on first fire.
+-- Otherwise it will wait for second evaluation - which is 2 more minutes.
+-- Exception is alert configs with duration set to > 15 seconds.
+
+alter table alert_configuration alter column duration_sec set default 0;
+update alert_configuration set duration_sec = 0 where duration_sec <= 15;
+update alert_definition set config_written = false;
diff --git a/managed/src/main/resources/reference.conf b/managed/src/main/resources/reference.conf
index 245abe0e43..bef0cb958c 100644
--- a/managed/src/main/resources/reference.conf
+++ b/managed/src/main/resources/reference.conf
@@ -55,7 +55,8 @@ db {
 
 yb {
   mode="PLATFORM"
-  universe_version_check_mode=HA_ONLY  # possible values: NEVER, HA_ONLY, ALWAYS
+  universe_version_check_mode=NEVER  # possible values: NEVER, HA_ONLY, ALWAYS
+  universe_boot_script = null
 
   # Alerts thresholds
   alert {
@@ -105,6 +106,8 @@ yb {
     max_ysql_throughput = 100000
     # Maximum YCQL throughput
     max_ycql_throughput = 100000
+    # Underreplicated masters threshold which triggers severe alert
+    underreplicated_masters_secs_severe = 900
   }
   # Used to skip certificates validation for the configure phase.
   # Possible values - ALL, HOSTNAME
@@ -212,7 +215,6 @@ yb {
     #  Allow for leader blacklisting during universe upgrades
     blacklist_leaders = false
     blacklist_leader_wait_time_ms = 60000
-
     max_follower_lag_threshold_ms = 60000
   }
   ha {
@@ -221,6 +223,27 @@ yb {
     prometheus_config_dir = "/prometheus_configs"
     num_backup_retention = 10
     logScriptOutput = false
+    ws = ${play.ws}
+    # Override this ws config in runtime_config at global level
+    # Reference: https://github.com/playframework/play-ws/blob/main/play-ws-standalone/src/main/resources/reference.conf
+    # Example:
+#      {
+#      ssl {
+#         acceptAnyCert = false
+#         trustManager = {
+#           stores += { # append to certs defined in play.ws
+#               type = "PEM"
+#               data = """-----BEGIN CERTIFICATE-----
+# MIIDzTCCArWgAwIBAgIQCjeHZF5ftIwiTv0b7RQMPDANBgkqhkiG9w0BAQsFADBa
+# ... You can use triple quoted string for multiline data ...
+# -----END CERTIFICATE-----"""
+#           }
+#           stores += {
+#             ... you can trust multiple certs ...
+#           }
+#        }
+#      }
+#    }
   }
   wait_for_server_timeout = 300000 ms
   # Timeout for proxy endpoint request of db node
@@ -275,6 +298,8 @@ yb {
     oidcScope = ${?YB_OIDC_SCOPE}
     oidcEmailAttribute = ""
     oidcEmailAttribute = ${?YB_OIDC_EMAIL_ATTR}
+    oidcDefaultRedirectUrl = "/",
+    oidcCallbackUrl = "/api/v1/callback",
     ldap {
       use_ldap = "false"
       ldap_url = ""
@@ -338,9 +363,21 @@ yb {
     cmdOutputDelete = true
     max_msg_size = 2M
   }
+
+  # TODO(vipulbansal) start using this now that setting objects is allowed
+  external_script {
+    content = ""
+    params = ""
+    schedule = ""
+  }
+
 }
 
 runtime_config {
+  included_objects = [
+    "yb.external_script"
+    "yb.ha.ws"
+  ]
   included_paths = [
       #  We can set this to "yb." if/when there are more includedPaths than excludedPaths
       "yb.taskGC."
@@ -353,9 +390,9 @@ runtime_config {
       "yb.ha.logScriptOutput"
       "yb.internal.",
       "yb.ansible.",
-      "yb.upgrade",
+      "yb.upgrade.",
       "yb.tls.skip_cert_validation",
-      "yb.dbmem",
+      "yb.dbmem.",
       "yb.security.ldap.",
       "yb.security.use_oauth",
       "yb.security.type",
@@ -364,6 +401,7 @@ runtime_config {
       "yb.security.discoveryURI",
       "yb.security.oidcScope",
       "yb.security.oidcEmailAttribute",
+      "yb.security.oidcCallbackUrl",
       "yb.backup.pg_based",
       "yb.logs.",
       "yb.metrics.db_read_write_test",
diff --git a/managed/src/main/resources/swagger.json b/managed/src/main/resources/swagger.json
index 74afd6072f..c30a04512d 100644
--- a/managed/src/main/resources/swagger.json
+++ b/managed/src/main/resources/swagger.json
@@ -410,7 +410,7 @@
         },
         "template" : {
           "description" : "Template name",
-          "enum" : [ "REPLICATION_LAG", "CLOCK_SKEW", "MEMORY_CONSUMPTION", "HEALTH_CHECK_ERROR", "HEALTH_CHECK_NOTIFICATION_ERROR", "BACKUP_FAILURE", "BACKUP_SCHEDULE_FAILURE", "INACTIVE_CRON_NODES", "ALERT_QUERY_FAILED", "ALERT_CONFIG_WRITING_FAILED", "ALERT_NOTIFICATION_ERROR", "ALERT_NOTIFICATION_CHANNEL_ERROR", "NODE_DOWN", "NODE_RESTART", "NODE_CPU_USAGE", "NODE_DISK_USAGE", "NODE_FILE_DESCRIPTORS_USAGE", "NODE_OOM_KILLS", "DB_VERSION_MISMATCH", "DB_INSTANCE_DOWN", "DB_INSTANCE_RESTART", "DB_FATAL_LOGS", "DB_ERROR_LOGS", "DB_CORE_FILES", "DB_YSQL_CONNECTION", "DB_YCQL_CONNECTION", "DB_REDIS_CONNECTION", "DB_MEMORY_OVERLOAD", "DB_COMPACTION_OVERLOAD", "DB_QUEUES_OVERFLOW", "DB_WRITE_READ_TEST_ERROR", "NODE_TO_NODE_CA_CERT_EXPIRY", "NODE_TO_NODE_CERT_EXPIRY", "CLIENT_TO_NODE_CA_CERT_EXPIRY", "CLIENT_TO_NODE_CERT_EXPIRY", "YSQL_OP_AVG_LATENCY", "YCQL_OP_AVG_LATENCY", "YSQL_OP_P99_LATENCY", "YCQL_OP_P99_LATENCY", "HIGH_NUM_YSQL_CONNECTIONS", "HIGH_NUM_YCQL_CONNECTIONS", "HIGH_NUM_YEDIS_CONNECTIONS", "YSQL_THROUGHPUT", "YCQL_THROUGHPUT", "MASTER_LEADER_MISSING", "LEADERLESS_TABLETS", "UNDER_REPLICATED_TABLETS" ],
+          "enum" : [ "REPLICATION_LAG", "CLOCK_SKEW", "MEMORY_CONSUMPTION", "HEALTH_CHECK_ERROR", "HEALTH_CHECK_NOTIFICATION_ERROR", "BACKUP_FAILURE", "BACKUP_SCHEDULE_FAILURE", "INACTIVE_CRON_NODES", "ALERT_QUERY_FAILED", "ALERT_CONFIG_WRITING_FAILED", "ALERT_NOTIFICATION_ERROR", "ALERT_NOTIFICATION_CHANNEL_ERROR", "NODE_DOWN", "NODE_RESTART", "NODE_CPU_USAGE", "NODE_DISK_USAGE", "NODE_FILE_DESCRIPTORS_USAGE", "NODE_OOM_KILLS", "DB_VERSION_MISMATCH", "DB_INSTANCE_DOWN", "DB_INSTANCE_RESTART", "DB_FATAL_LOGS", "DB_ERROR_LOGS", "DB_CORE_FILES", "DB_YSQL_CONNECTION", "DB_YCQL_CONNECTION", "DB_REDIS_CONNECTION", "DB_MEMORY_OVERLOAD", "DB_COMPACTION_OVERLOAD", "DB_QUEUES_OVERFLOW", "DB_WRITE_READ_TEST_ERROR", "NODE_TO_NODE_CA_CERT_EXPIRY", "NODE_TO_NODE_CERT_EXPIRY", "CLIENT_TO_NODE_CA_CERT_EXPIRY", "CLIENT_TO_NODE_CERT_EXPIRY", "YSQL_OP_AVG_LATENCY", "YCQL_OP_AVG_LATENCY", "YSQL_OP_P99_LATENCY", "YCQL_OP_P99_LATENCY", "HIGH_NUM_YSQL_CONNECTIONS", "HIGH_NUM_YCQL_CONNECTIONS", "HIGH_NUM_YEDIS_CONNECTIONS", "YSQL_THROUGHPUT", "YCQL_THROUGHPUT", "MASTER_LEADER_MISSING", "MASTER_UNDER_REPLICATED", "LEADERLESS_TABLETS", "UNDER_REPLICATED_TABLETS" ],
           "readOnly" : true,
           "type" : "string"
         },
@@ -465,7 +465,7 @@
           "type" : "string"
         },
         "template" : {
-          "enum" : [ "REPLICATION_LAG", "CLOCK_SKEW", "MEMORY_CONSUMPTION", "HEALTH_CHECK_ERROR", "HEALTH_CHECK_NOTIFICATION_ERROR", "BACKUP_FAILURE", "BACKUP_SCHEDULE_FAILURE", "INACTIVE_CRON_NODES", "ALERT_QUERY_FAILED", "ALERT_CONFIG_WRITING_FAILED", "ALERT_NOTIFICATION_ERROR", "ALERT_NOTIFICATION_CHANNEL_ERROR", "NODE_DOWN", "NODE_RESTART", "NODE_CPU_USAGE", "NODE_DISK_USAGE", "NODE_FILE_DESCRIPTORS_USAGE", "NODE_OOM_KILLS", "DB_VERSION_MISMATCH", "DB_INSTANCE_DOWN", "DB_INSTANCE_RESTART", "DB_FATAL_LOGS", "DB_ERROR_LOGS", "DB_CORE_FILES", "DB_YSQL_CONNECTION", "DB_YCQL_CONNECTION", "DB_REDIS_CONNECTION", "DB_MEMORY_OVERLOAD", "DB_COMPACTION_OVERLOAD", "DB_QUEUES_OVERFLOW", "DB_WRITE_READ_TEST_ERROR", "NODE_TO_NODE_CA_CERT_EXPIRY", "NODE_TO_NODE_CERT_EXPIRY", "CLIENT_TO_NODE_CA_CERT_EXPIRY", "CLIENT_TO_NODE_CERT_EXPIRY", "YSQL_OP_AVG_LATENCY", "YCQL_OP_AVG_LATENCY", "YSQL_OP_P99_LATENCY", "YCQL_OP_P99_LATENCY", "HIGH_NUM_YSQL_CONNECTIONS", "HIGH_NUM_YCQL_CONNECTIONS", "HIGH_NUM_YEDIS_CONNECTIONS", "YSQL_THROUGHPUT", "YCQL_THROUGHPUT", "MASTER_LEADER_MISSING", "LEADERLESS_TABLETS", "UNDER_REPLICATED_TABLETS" ],
+          "enum" : [ "REPLICATION_LAG", "CLOCK_SKEW", "MEMORY_CONSUMPTION", "HEALTH_CHECK_ERROR", "HEALTH_CHECK_NOTIFICATION_ERROR", "BACKUP_FAILURE", "BACKUP_SCHEDULE_FAILURE", "INACTIVE_CRON_NODES", "ALERT_QUERY_FAILED", "ALERT_CONFIG_WRITING_FAILED", "ALERT_NOTIFICATION_ERROR", "ALERT_NOTIFICATION_CHANNEL_ERROR", "NODE_DOWN", "NODE_RESTART", "NODE_CPU_USAGE", "NODE_DISK_USAGE", "NODE_FILE_DESCRIPTORS_USAGE", "NODE_OOM_KILLS", "DB_VERSION_MISMATCH", "DB_INSTANCE_DOWN", "DB_INSTANCE_RESTART", "DB_FATAL_LOGS", "DB_ERROR_LOGS", "DB_CORE_FILES", "DB_YSQL_CONNECTION", "DB_YCQL_CONNECTION", "DB_REDIS_CONNECTION", "DB_MEMORY_OVERLOAD", "DB_COMPACTION_OVERLOAD", "DB_QUEUES_OVERFLOW", "DB_WRITE_READ_TEST_ERROR", "NODE_TO_NODE_CA_CERT_EXPIRY", "NODE_TO_NODE_CERT_EXPIRY", "CLIENT_TO_NODE_CA_CERT_EXPIRY", "CLIENT_TO_NODE_CERT_EXPIRY", "YSQL_OP_AVG_LATENCY", "YCQL_OP_AVG_LATENCY", "YSQL_OP_P99_LATENCY", "YCQL_OP_P99_LATENCY", "HIGH_NUM_YSQL_CONNECTIONS", "HIGH_NUM_YCQL_CONNECTIONS", "HIGH_NUM_YEDIS_CONNECTIONS", "YSQL_THROUGHPUT", "YCQL_THROUGHPUT", "MASTER_LEADER_MISSING", "MASTER_UNDER_REPLICATED", "LEADERLESS_TABLETS", "UNDER_REPLICATED_TABLETS" ],
           "type" : "string"
         },
         "uuids" : {
@@ -619,7 +619,7 @@
         },
         "template" : {
           "description" : "Template name",
-          "enum" : [ "REPLICATION_LAG", "CLOCK_SKEW", "MEMORY_CONSUMPTION", "HEALTH_CHECK_ERROR", "HEALTH_CHECK_NOTIFICATION_ERROR", "BACKUP_FAILURE", "BACKUP_SCHEDULE_FAILURE", "INACTIVE_CRON_NODES", "ALERT_QUERY_FAILED", "ALERT_CONFIG_WRITING_FAILED", "ALERT_NOTIFICATION_ERROR", "ALERT_NOTIFICATION_CHANNEL_ERROR", "NODE_DOWN", "NODE_RESTART", "NODE_CPU_USAGE", "NODE_DISK_USAGE", "NODE_FILE_DESCRIPTORS_USAGE", "NODE_OOM_KILLS", "DB_VERSION_MISMATCH", "DB_INSTANCE_DOWN", "DB_INSTANCE_RESTART", "DB_FATAL_LOGS", "DB_ERROR_LOGS", "DB_CORE_FILES", "DB_YSQL_CONNECTION", "DB_YCQL_CONNECTION", "DB_REDIS_CONNECTION", "DB_MEMORY_OVERLOAD", "DB_COMPACTION_OVERLOAD", "DB_QUEUES_OVERFLOW", "DB_WRITE_READ_TEST_ERROR", "NODE_TO_NODE_CA_CERT_EXPIRY", "NODE_TO_NODE_CERT_EXPIRY", "CLIENT_TO_NODE_CA_CERT_EXPIRY", "CLIENT_TO_NODE_CERT_EXPIRY", "YSQL_OP_AVG_LATENCY", "YCQL_OP_AVG_LATENCY", "YSQL_OP_P99_LATENCY", "YCQL_OP_P99_LATENCY", "HIGH_NUM_YSQL_CONNECTIONS", "HIGH_NUM_YCQL_CONNECTIONS", "HIGH_NUM_YEDIS_CONNECTIONS", "YSQL_THROUGHPUT", "YCQL_THROUGHPUT", "MASTER_LEADER_MISSING", "LEADERLESS_TABLETS", "UNDER_REPLICATED_TABLETS" ],
+          "enum" : [ "REPLICATION_LAG", "CLOCK_SKEW", "MEMORY_CONSUMPTION", "HEALTH_CHECK_ERROR", "HEALTH_CHECK_NOTIFICATION_ERROR", "BACKUP_FAILURE", "BACKUP_SCHEDULE_FAILURE", "INACTIVE_CRON_NODES", "ALERT_QUERY_FAILED", "ALERT_CONFIG_WRITING_FAILED", "ALERT_NOTIFICATION_ERROR", "ALERT_NOTIFICATION_CHANNEL_ERROR", "NODE_DOWN", "NODE_RESTART", "NODE_CPU_USAGE", "NODE_DISK_USAGE", "NODE_FILE_DESCRIPTORS_USAGE", "NODE_OOM_KILLS", "DB_VERSION_MISMATCH", "DB_INSTANCE_DOWN", "DB_INSTANCE_RESTART", "DB_FATAL_LOGS", "DB_ERROR_LOGS", "DB_CORE_FILES", "DB_YSQL_CONNECTION", "DB_YCQL_CONNECTION", "DB_REDIS_CONNECTION", "DB_MEMORY_OVERLOAD", "DB_COMPACTION_OVERLOAD", "DB_QUEUES_OVERFLOW", "DB_WRITE_READ_TEST_ERROR", "NODE_TO_NODE_CA_CERT_EXPIRY", "NODE_TO_NODE_CERT_EXPIRY", "CLIENT_TO_NODE_CA_CERT_EXPIRY", "CLIENT_TO_NODE_CERT_EXPIRY", "YSQL_OP_AVG_LATENCY", "YCQL_OP_AVG_LATENCY", "YSQL_OP_P99_LATENCY", "YCQL_OP_P99_LATENCY", "HIGH_NUM_YSQL_CONNECTIONS", "HIGH_NUM_YCQL_CONNECTIONS", "HIGH_NUM_YEDIS_CONNECTIONS", "YSQL_THROUGHPUT", "YCQL_THROUGHPUT", "MASTER_LEADER_MISSING", "MASTER_UNDER_REPLICATED", "LEADERLESS_TABLETS", "UNDER_REPLICATED_TABLETS" ],
           "readOnly" : true,
           "type" : "string"
         },
@@ -2424,30 +2424,6 @@
       },
       "type" : "object"
     },
-    "FileSystem" : {
-      "properties" : {
-        "fileStores" : {
-          "$ref" : "#/definitions/IterableFileStore"
-        },
-        "open" : {
-          "type" : "boolean"
-        },
-        "readOnly" : {
-          "type" : "boolean"
-        },
-        "rootDirectories" : {
-          "$ref" : "#/definitions/IterablePath"
-        },
-        "separator" : {
-          "type" : "string"
-        },
-        "userPrincipalLookupService" : {
-          "$ref" : "#/definitions/UserPrincipalLookupService"
-        }
-      },
-      "required" : [ "fileStores", "open", "readOnly", "rootDirectories", "separator", "userPrincipalLookupService" ],
-      "type" : "object"
-    },
     "GCSLocation" : {
       "properties" : {
         "paths" : {
@@ -2775,15 +2751,6 @@
       "required" : [ "instanceTypeCode", "providerUuid" ],
       "type" : "object"
     },
-    "Iterable" : {
-      "type" : "object"
-    },
-    "IterableFileStore" : {
-      "type" : "object"
-    },
-    "IterablePath" : {
-      "type" : "object"
-    },
     "KeyInfo" : {
       "properties" : {
         "airGapInstall" : {
@@ -3862,31 +3829,6 @@
       },
       "type" : "object"
     },
-    "Path" : {
-      "properties" : {
-        "absolute" : {
-          "type" : "boolean"
-        },
-        "fileName" : {
-          "$ref" : "#/definitions/Path"
-        },
-        "fileSystem" : {
-          "$ref" : "#/definitions/FileSystem"
-        },
-        "nameCount" : {
-          "format" : "int32",
-          "type" : "integer"
-        },
-        "parent" : {
-          "$ref" : "#/definitions/Path"
-        },
-        "root" : {
-          "$ref" : "#/definitions/Path"
-        }
-      },
-      "required" : [ "absolute", "fileName", "fileSystem", "nameCount", "parent", "root" ],
-      "type" : "object"
-    },
     "PlacementAZ" : {
       "properties" : {
         "isAffinitized" : {
@@ -4764,38 +4706,6 @@
       "required" : [ "clusters", "kubernetesUpgradeSupported", "sleepAfterMasterRestartMillis", "sleepAfterTServerRestartMillis", "upgradeOption", "ybSoftwareVersion" ],
       "type" : "object"
     },
-    "SupportBundle" : {
-      "properties" : {
-        "bundleUUID" : {
-          "format" : "uuid",
-          "type" : "string"
-        },
-        "endDate" : {
-          "format" : "date-time",
-          "type" : "string"
-        },
-        "fileName" : {
-          "type" : "string"
-        },
-        "path" : {
-          "$ref" : "#/definitions/Path"
-        },
-        "scopeUUID" : {
-          "format" : "uuid",
-          "type" : "string"
-        },
-        "startDate" : {
-          "format" : "date-time",
-          "type" : "string"
-        },
-        "status" : {
-          "enum" : [ "Running", "Success", "Failed" ],
-          "type" : "string"
-        }
-      },
-      "required" : [ "bundleUUID", "endDate", "fileName", "path", "scopeUUID", "startDate", "status" ],
-      "type" : "object"
-    },
     "SystemdUpgradeParams" : {
       "properties" : {
         "allowInsecure" : {
@@ -6242,9 +6152,6 @@
       },
       "type" : "object"
     },
-    "UserPrincipalLookupService" : {
-      "type" : "object"
-    },
     "UserProfileData" : {
       "description" : "User profile data. The API and UI use this to validate form data.",
       "properties" : {
@@ -11498,7 +11405,7 @@
           "200" : {
             "description" : "successful operation",
             "schema" : {
-              "$ref" : "#/definitions/SupportBundle"
+              "type" : "string"
             }
           }
         },
diff --git a/managed/src/main/resources/version.txt b/managed/src/main/resources/version.txt
index a1fcc08493..f3f42b9148 100644
--- a/managed/src/main/resources/version.txt
+++ b/managed/src/main/resources/version.txt
@@ -1 +1 @@
-2.12.4.2-b0
+2.12.7.0-b0
diff --git a/managed/src/test/java/com/yugabyte/yw/commissioner/HealthCheckerTest.java b/managed/src/test/java/com/yugabyte/yw/commissioner/HealthCheckerTest.java
index c4fbd59cba..68eb8bbeed 100644
--- a/managed/src/test/java/com/yugabyte/yw/commissioner/HealthCheckerTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/commissioner/HealthCheckerTest.java
@@ -50,7 +50,6 @@ import com.yugabyte.yw.models.Provider;
 import com.yugabyte.yw.models.Region;
 import com.yugabyte.yw.models.Universe;
 import com.yugabyte.yw.models.helpers.CloudSpecificInfo;
-import com.yugabyte.yw.models.helpers.KnownAlertLabels;
 import com.yugabyte.yw.models.helpers.NodeDetails;
 import com.yugabyte.yw.models.helpers.PlacementInfo;
 import com.yugabyte.yw.models.helpers.PlatformMetrics;
@@ -180,6 +179,7 @@ public class HealthCheckerTest extends FakeDBApplication {
             return new RuntimeConfig<>(mockRuntimeConfig);
           }
         };
+    healthChecker.initialize();
   }
 
   private Universe setupUniverse(String name) {
@@ -636,9 +636,6 @@ public class HealthCheckerTest extends FakeDBApplication {
                 .targetUuid(u.getUniverseUUID())
                 .build(),
             0.0);
-    assertEquals(
-        "Error sending Health check email: TestException",
-        hcNotificationMetric.getLabelValue(KnownAlertLabels.ERROR_MESSAGE));
   }
 
   @Test
@@ -679,12 +676,12 @@ public class HealthCheckerTest extends FakeDBApplication {
     setupAlertingData(YB_ALERT_TEST_EMAIL, false, false);
     mockGoodHealthResponse();
 
-    metricService.setStatusMetric(
-        buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_STATUS, u), "Some error");
-    metricService.setStatusMetric(
-        buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_NOTIFICATION_STATUS, u), "Some error");
-    metricService.setStatusMetric(
-        buildMetricTemplate(PlatformMetrics.ALERT_MANAGER_STATUS, u), "Some error");
+    metricService.setFailureStatusMetric(
+        buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_STATUS, u));
+    metricService.setFailureStatusMetric(
+        buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_NOTIFICATION_STATUS, u));
+    metricService.setFailureStatusMetric(
+        buildMetricTemplate(PlatformMetrics.ALERT_MANAGER_STATUS, u));
 
     healthChecker.checkSingleUniverse(
         new HealthChecker.CheckSingleUniverseParams(
@@ -746,10 +743,6 @@ public class HealthCheckerTest extends FakeDBApplication {
                 .targetUuid(u.getUniverseUUID())
                 .build(),
             0.0);
-
-    assertEquals(
-        metric.getLabelValue(KnownAlertLabels.ERROR_MESSAGE),
-        "Can't run health check for the universe due to unprovisioned node test.");
   }
 
   @Test
@@ -779,10 +772,6 @@ public class HealthCheckerTest extends FakeDBApplication {
                 .targetUuid(u.getUniverseUUID())
                 .build(),
             0.0);
-
-    assertEquals(
-        metric.getLabelValue(KnownAlertLabels.ERROR_MESSAGE),
-        String.format("Can't run health check for the universe due to unprovisioned node."));
   }
 
   @Test
diff --git a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/AddNodeToUniverseTest.java b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/AddNodeToUniverseTest.java
index abfefbf5f7..13c312eea4 100644
--- a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/AddNodeToUniverseTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/AddNodeToUniverseTest.java
@@ -2,13 +2,17 @@
 
 package com.yugabyte.yw.commissioner.tasks;
 
+import static com.yugabyte.yw.commissioner.tasks.UniverseTaskBase.VersionCheckMode.HA_ONLY;
 import static com.yugabyte.yw.common.AssertHelper.assertJsonEqual;
 import static com.yugabyte.yw.common.ModelFactory.createUniverse;
 import static com.yugabyte.yw.models.TaskInfo.State.Failure;
 import static com.yugabyte.yw.models.TaskInfo.State.Success;
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.hamcrest.MatcherAssert.assertThat;
 import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNull;
+import static org.junit.Assert.fail;
 import static org.mockito.ArgumentMatchers.any;
 import static org.mockito.ArgumentMatchers.anyBoolean;
 import static org.mockito.ArgumentMatchers.anyInt;
@@ -27,6 +31,7 @@ import com.google.common.collect.ImmutableSet;
 import com.yugabyte.yw.commissioner.tasks.params.NodeTaskParams;
 import com.yugabyte.yw.common.ApiUtils;
 import com.yugabyte.yw.common.NodeActionType;
+import com.yugabyte.yw.common.config.impl.SettableRuntimeConfigFactory;
 import com.yugabyte.yw.forms.UniverseDefinitionTaskParams;
 import com.yugabyte.yw.forms.UniverseDefinitionTaskParams.Cluster;
 import com.yugabyte.yw.models.AvailabilityZone;
@@ -81,12 +86,14 @@ public class AddNodeToUniverseTest extends UniverseModifyBaseTest {
     setDefaultNodeState(onPremUniverse, NodeState.Removed, DEFAULT_NODE_NAME);
 
     try {
+      when(mockClient.waitForMaster(any(), anyLong())).thenReturn(true);
       when(mockClient.changeMasterClusterConfig(any())).thenReturn(ccr);
       when(mockClient.setFlag(any(), anyString(), anyString(), anyBoolean())).thenReturn(true);
       ListMastersResponse listMastersResponse = mock(ListMastersResponse.class);
       when(listMastersResponse.getMasters()).thenReturn(Collections.emptyList());
       when(mockClient.listMasters()).thenReturn(listMastersResponse);
     } catch (Exception e) {
+      fail();
     }
 
     mockWaits(mockClient, 4);
@@ -115,6 +122,22 @@ public class AddNodeToUniverseTest extends UniverseModifyBaseTest {
         universe.universeUUID, getNodeUpdater(nodeName, node -> node.state = desiredState));
   }
 
+  private void decomissionOnPremNode(String nodeName) {
+    Universe.saveDetails(
+        onPremUniverse.universeUUID,
+        u -> {
+          NodeDetails node = u.getNode(nodeName);
+          node.state = NodeState.Decommissioned;
+          NodeInstance.maybeGetByName(nodeName)
+              .ifPresent(
+                  nodeInstance -> {
+                    nodeInstance.setInUse(false);
+                    nodeInstance.setNodeName("");
+                    nodeInstance.save();
+                  });
+        });
+  }
+
   private TaskInfo submitTask(UUID universeUUID, String nodeName, int version) {
     return submitTask(universeUUID, defaultProvider, nodeName, version);
   }
@@ -174,6 +197,40 @@ public class AddNodeToUniverseTest extends UniverseModifyBaseTest {
           Json.toJson(ImmutableMap.of("state", "Live")),
           Json.toJson(ImmutableMap.of()));
 
+  private static final List<TaskType> ADD_NODE_TASK_DECOMISSIONED_NODE_SEQUENCE =
+      ImmutableList.of(
+          TaskType.SetNodeState,
+          TaskType.AnsibleCreateServer,
+          TaskType.AnsibleUpdateNodeInfo,
+          TaskType.AnsibleSetupServer,
+          TaskType.AnsibleConfigureServers,
+          TaskType.SetNodeState,
+          TaskType.AnsibleConfigureServers,
+          TaskType.AnsibleClusterServerCtl,
+          TaskType.UpdateNodeProcess,
+          TaskType.WaitForServer,
+          TaskType.SwamperTargetsFileUpdate,
+          TaskType.WaitForLoadBalance,
+          TaskType.SetNodeState,
+          TaskType.UniverseUpdateSucceeded);
+
+  private static final List<JsonNode> ADD_NODE_TASK_DECOMISSIONED_NODE_EXPECTED_RESULTS =
+      ImmutableList.of(
+          Json.toJson(ImmutableMap.of("state", "Adding")),
+          Json.toJson(ImmutableMap.of()),
+          Json.toJson(ImmutableMap.of()),
+          Json.toJson(ImmutableMap.of()),
+          Json.toJson(ImmutableMap.of()),
+          Json.toJson(ImmutableMap.of("state", "ToJoinCluster")),
+          Json.toJson(ImmutableMap.of()),
+          Json.toJson(ImmutableMap.of("process", "tserver", "command", "start")),
+          Json.toJson(ImmutableMap.of("processType", "TSERVER", "isAdd", true)),
+          Json.toJson(ImmutableMap.of()),
+          Json.toJson(ImmutableMap.of()),
+          Json.toJson(ImmutableMap.of()),
+          Json.toJson(ImmutableMap.of("state", "Live")),
+          Json.toJson(ImmutableMap.of()));
+
   private static final List<TaskType> WITH_MASTER_UNDER_REPLICATED =
       ImmutableList.of(
           TaskType.SetNodeState,
@@ -223,30 +280,27 @@ public class AddNodeToUniverseTest extends UniverseModifyBaseTest {
           Json.toJson(ImmutableMap.of()));
 
   private void assertAddNodeSequence(
-      Map<Integer, List<TaskInfo>> subTasksByPosition, boolean masterUnderReplicated) {
+      Map<Integer, List<TaskInfo>> subTasksByPosition,
+      boolean isNodeDecomissioned,
+      boolean masterUnderReplicated) {
     int position = 0;
-    if (masterUnderReplicated) {
-      for (TaskType taskType : WITH_MASTER_UNDER_REPLICATED) {
-        List<TaskInfo> tasks = subTasksByPosition.get(position);
-        assertEquals("At position: " + position, taskType, tasks.get(0).getTaskType());
-        JsonNode expectedResults = WITH_MASTER_UNDER_REPLICATED_RESULTS.get(position);
-        List<JsonNode> taskDetails =
-            tasks.stream().map(TaskInfo::getTaskDetails).collect(Collectors.toList());
-        assertJsonEqual(expectedResults, taskDetails.get(0));
-        position++;
-      }
-    } else {
-      for (TaskType taskType : ADD_NODE_TASK_SEQUENCE) {
-        List<TaskInfo> tasks = subTasksByPosition.get(position);
-        assertEquals(1, tasks.size());
-        assertEquals("At position: " + position, taskType, tasks.get(0).getTaskType());
-        JsonNode expectedResults = ADD_NODE_TASK_EXPECTED_RESULTS.get(position);
-        List<JsonNode> taskDetails =
-            tasks.stream().map(TaskInfo::getTaskDetails).collect(Collectors.toList());
-        LOG.info(taskDetails.get(0).toString());
-        assertJsonEqual(expectedResults, taskDetails.get(0));
-        position++;
-      }
+    List<TaskType> taskSequence = ADD_NODE_TASK_SEQUENCE;
+    List<JsonNode> taskExpectedResults = ADD_NODE_TASK_EXPECTED_RESULTS;
+    if (isNodeDecomissioned) {
+      taskSequence = ADD_NODE_TASK_DECOMISSIONED_NODE_SEQUENCE;
+      taskExpectedResults = ADD_NODE_TASK_DECOMISSIONED_NODE_EXPECTED_RESULTS;
+    } else if (masterUnderReplicated) {
+      taskSequence = WITH_MASTER_UNDER_REPLICATED;
+      taskExpectedResults = WITH_MASTER_UNDER_REPLICATED_RESULTS;
+    }
+    for (TaskType taskType : taskSequence) {
+      List<TaskInfo> tasks = subTasksByPosition.get(position);
+      assertEquals("At position: " + position, taskType, tasks.get(0).getTaskType());
+      JsonNode expectedResults = taskExpectedResults.get(position);
+      List<JsonNode> taskDetails =
+          tasks.stream().map(TaskInfo::getTaskDetails).collect(Collectors.toList());
+      assertJsonEqual(expectedResults, taskDetails.get(0));
+      position++;
     }
   }
 
@@ -255,6 +309,9 @@ public class AddNodeToUniverseTest extends UniverseModifyBaseTest {
   public void testAddNodeSuccess(boolean isHAConfig) throws Exception {
 
     if (isHAConfig) {
+      SettableRuntimeConfigFactory factory =
+          app.injector().instanceOf(SettableRuntimeConfigFactory.class);
+      factory.globalRuntimeConf().setValue("yb.universe_version_check_mode", HA_ONLY.name());
       HighAvailabilityConfig.create("clusterKey");
     }
     mockWaits(mockClient, 3);
@@ -267,7 +324,7 @@ public class AddNodeToUniverseTest extends UniverseModifyBaseTest {
     List<TaskInfo> subTasks = taskInfo.getSubTasks();
     Map<Integer, List<TaskInfo>> subTasksByPosition =
         subTasks.stream().collect(Collectors.groupingBy(TaskInfo::getPosition));
-    assertAddNodeSequence(subTasksByPosition, false);
+    assertAddNodeSequence(subTasksByPosition, false, false);
 
     if (isHAConfig) {
       // In HA config mode, we expect any save of universe details to result in
@@ -288,30 +345,44 @@ public class AddNodeToUniverseTest extends UniverseModifyBaseTest {
         submitTask(onPremUniverse.universeUUID, onPremProvider, DEFAULT_NODE_NAME, 3);
     assertEquals(Success, taskInfo.getTaskState());
 
-    verify(mockNodeManager, times(5)).nodeCommand(any(), any());
+    verify(mockNodeManager, times(4)).nodeCommand(any(), any());
+    List<TaskInfo> subTasks = taskInfo.getSubTasks();
+    Map<Integer, List<TaskInfo>> subTasksByPosition =
+        subTasks.stream().collect(Collectors.groupingBy(TaskInfo::getPosition));
+    assertAddNodeSequence(subTasksByPosition, false, false);
+  }
+
+  @Test
+  public void testAddNodeOnPremSuccessForDecommissionedNode() throws Exception {
+    mockWaits(mockClient, 4);
+    decomissionOnPremNode(DEFAULT_NODE_NAME);
+    TaskInfo taskInfo =
+        submitTask(onPremUniverse.universeUUID, onPremProvider, DEFAULT_NODE_NAME, 4);
+    assertEquals(Success, taskInfo.getTaskState());
+
+    verify(mockNodeManager, times(8)).nodeCommand(any(), any());
     List<TaskInfo> subTasks = taskInfo.getSubTasks();
     Map<Integer, List<TaskInfo>> subTasksByPosition =
         subTasks.stream().collect(Collectors.groupingBy(TaskInfo::getPosition));
-    assertAddNodeSequence(subTasksByPosition, false);
+    assertAddNodeSequence(subTasksByPosition, true, false);
   }
 
   @Test
   public void testAddNodeOnPrem_FailedPreflightCheck() throws Exception {
-    mockWaits(mockClient, 3);
+    mockWaits(mockClient, 4);
     preflightResponse.message = "{\"test\": false}";
-
+    decomissionOnPremNode(DEFAULT_NODE_NAME);
     TaskInfo taskInfo =
-        submitTask(onPremUniverse.universeUUID, onPremProvider, DEFAULT_NODE_NAME, 3);
+        submitTask(onPremUniverse.universeUUID, onPremProvider, DEFAULT_NODE_NAME, 4);
     assertEquals(Failure, taskInfo.getTaskState());
 
     verify(mockNodeManager, times(1)).nodeCommand(any(), any());
-    List<TaskInfo> subTasks = taskInfo.getSubTasks();
-    assertEquals(1, subTasks.size());
-    assertEquals(TaskType.PrecheckNode, subTasks.get(0).getTaskType());
+    assertThat(
+        taskInfo.getErrorMessage(),
+        containsString("failed preflight check. Error: {\"test\": false}"));
 
-    NodeInstance instance = NodeInstance.getByName(DEFAULT_NODE_NAME);
-    assertNotNull(instance.getNodeName());
-    assertNotNull(instance.getDetails().nodeName);
+    // Node must not be reserved on failure.
+    assertFalse(NodeInstance.maybeGetByName(DEFAULT_NODE_NAME).isPresent());
   }
 
   @Test
@@ -329,7 +400,7 @@ public class AddNodeToUniverseTest extends UniverseModifyBaseTest {
     List<TaskInfo> subTasks = taskInfo.getSubTasks();
     Map<Integer, List<TaskInfo>> subTasksByPosition =
         subTasks.stream().collect(Collectors.groupingBy(TaskInfo::getPosition));
-    assertAddNodeSequence(subTasksByPosition, true);
+    assertAddNodeSequence(subTasksByPosition, false, true);
   }
 
   @Test
@@ -357,7 +428,7 @@ public class AddNodeToUniverseTest extends UniverseModifyBaseTest {
     List<TaskInfo> subTasks = taskInfo.getSubTasks();
     Map<Integer, List<TaskInfo>> subTasksByPosition =
         subTasks.stream().collect(Collectors.groupingBy(TaskInfo::getPosition));
-    assertAddNodeSequence(subTasksByPosition, true /* Master start is expected */);
+    assertAddNodeSequence(subTasksByPosition, false, true /* Master start is expected */);
   }
 
   @Test
@@ -384,7 +455,7 @@ public class AddNodeToUniverseTest extends UniverseModifyBaseTest {
     List<TaskInfo> subTasks = taskInfo.getSubTasks();
     Map<Integer, List<TaskInfo>> subTasksByPosition =
         subTasks.stream().collect(Collectors.groupingBy(TaskInfo::getPosition));
-    assertAddNodeSequence(subTasksByPosition, false /* Master start is unexpected */);
+    assertAddNodeSequence(subTasksByPosition, false, false /* Master start is unexpected */);
   }
 
   private void setDefaultGFlags(Universe universe) {
diff --git a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/CreateUniverseTest.java b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/CreateUniverseTest.java
index 09c393d6a2..4b5b8d50ad 100644
--- a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/CreateUniverseTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/CreateUniverseTest.java
@@ -7,6 +7,7 @@ import static com.yugabyte.yw.models.TaskInfo.State.Success;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 import static org.mockito.ArgumentMatchers.any;
 import static org.mockito.ArgumentMatchers.anyLong;
 import static org.mockito.Mockito.doAnswer;
@@ -106,10 +107,12 @@ public class CreateUniverseTest extends UniverseModifyBaseTest {
     when(mockListTabletServersResponse.getTabletServersCount()).thenReturn(10);
 
     try {
+      when(mockClient.waitForMaster(any(), anyLong())).thenReturn(true);
       when(mockClient.getMasterClusterConfig()).thenReturn(mockConfigResponse);
       when(mockClient.changeMasterClusterConfig(any())).thenReturn(mockMasterChangeConfigResponse);
       when(mockClient.listTabletServers()).thenReturn(mockListTabletServersResponse);
     } catch (Exception e) {
+      fail();
     }
     mockWaits(mockClient);
     when(mockClient.waitForServer(any(), anyLong())).thenReturn(true);
diff --git a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/EditUniverseTest.java b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/EditUniverseTest.java
index d967ce4efb..d3f4302c26 100644
--- a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/EditUniverseTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/EditUniverseTest.java
@@ -8,6 +8,7 @@ import static com.yugabyte.yw.models.TaskInfo.State.Success;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 import static org.mockito.ArgumentMatchers.any;
 import static org.mockito.ArgumentMatchers.anyBoolean;
 import static org.mockito.ArgumentMatchers.anyInt;
@@ -141,6 +142,7 @@ public class EditUniverseTest extends UniverseModifyBaseTest {
     when(mockListTabletServersResponse.getTabletServersCount()).thenReturn(10);
 
     try {
+      when(mockClient.waitForMaster(any(), anyLong())).thenReturn(true);
       when(mockClient.getMasterClusterConfig()).thenReturn(mockConfigResponse);
       when(mockClient.changeMasterClusterConfig(any())).thenReturn(mockMasterChangeConfigResponse);
       when(mockClient.changeMasterConfig(anyString(), anyInt(), anyBoolean(), anyBoolean()))
@@ -152,6 +154,7 @@ public class EditUniverseTest extends UniverseModifyBaseTest {
       when(listMastersResponse.getMasters()).thenReturn(Collections.emptyList());
       when(mockClient.listMasters()).thenReturn(listMastersResponse);
     } catch (Exception e) {
+      fail();
     }
     mockWaits(mockClient);
     when(mockClient.waitForServer(any(), anyLong())).thenReturn(true);
diff --git a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/ReleaseInstanceFromUniverseTest.java b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/ReleaseInstanceFromUniverseTest.java
index 5f44622d7c..0a2f7a9310 100644
--- a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/ReleaseInstanceFromUniverseTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/ReleaseInstanceFromUniverseTest.java
@@ -211,7 +211,11 @@ public class ReleaseInstanceFromUniverseTest extends CommissionerBaseTest {
   public void testReleaseNodeAllowedState() {
     Set<NodeState> allowedStates = NodeState.allowedStatesForAction(NodeActionType.RELEASE);
     Set<NodeState> expectedStates =
-        ImmutableSet.of(NodeState.BeingDecommissioned, NodeState.Removed, NodeState.Terminating);
+        ImmutableSet.of(
+            NodeState.Adding,
+            NodeState.BeingDecommissioned,
+            NodeState.Removed,
+            NodeState.Terminating);
     assertEquals(expectedStates, allowedStates);
   }
 }
diff --git a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/ResumeUniverseTest.java b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/ResumeUniverseTest.java
index fa0752cda5..3bf72e2e49 100644
--- a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/ResumeUniverseTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/ResumeUniverseTest.java
@@ -10,6 +10,7 @@ import static com.yugabyte.yw.models.TaskInfo.State.Success;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNull;
+import static org.junit.Assert.fail;
 import static org.mockito.ArgumentMatchers.any;
 import static org.mockito.ArgumentMatchers.anyLong;
 import static org.mockito.Mockito.mock;
@@ -56,6 +57,11 @@ public class ResumeUniverseTest extends CommissionerBaseTest {
     YBClient mockClient = mock(YBClient.class);
     when(mockYBClient.getClient(any(), any())).thenReturn(mockClient);
     when(mockClient.waitForServer(any(), anyLong())).thenReturn(true);
+    try {
+      when(mockClient.waitForMaster(any(), anyLong())).thenReturn(true);
+    } catch (Exception e) {
+      fail();
+    }
     ShellResponse dummyShellResponse = new ShellResponse();
     dummyShellResponse.message = "true";
     when(mockNodeManager.nodeCommand(any(), any())).thenReturn(dummyShellResponse);
diff --git a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/StartMasterOnNodeTest.java b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/StartMasterOnNodeTest.java
index f79435a6b0..dc258f6eb9 100644
--- a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/StartMasterOnNodeTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/StartMasterOnNodeTest.java
@@ -8,11 +8,13 @@ import static com.yugabyte.yw.models.TaskInfo.State.Failure;
 import static com.yugabyte.yw.models.TaskInfo.State.Success;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNull;
+import static org.junit.Assert.fail;
 import static org.mockito.ArgumentMatchers.any;
 import static org.mockito.ArgumentMatchers.anyBoolean;
 import static org.mockito.ArgumentMatchers.anyInt;
 import static org.mockito.ArgumentMatchers.anyLong;
 import static org.mockito.ArgumentMatchers.anyString;
+import static org.mockito.Mockito.lenient;
 import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.times;
 import static org.mockito.Mockito.verify;
@@ -79,18 +81,20 @@ public class StartMasterOnNodeTest extends CommissionerBaseTest {
     when(mockNodeManager.nodeCommand(any(), any())).thenReturn(dummyShellResponse);
 
     YBClient mockClient = mock(YBClient.class);
-    when(mockClient.waitForServer(any(), anyLong())).thenReturn(true);
 
     try {
       ChangeConfigResponse mockChangeConfigResponse = mock(ChangeConfigResponse.class);
       when(mockClient.changeMasterConfig(anyString(), anyInt(), anyBoolean(), anyBoolean()))
           .thenReturn(mockChangeConfigResponse);
+      lenient().when(mockClient.waitForServer(any(), anyLong())).thenReturn(true);
+      lenient().when(mockClient.waitForMaster(any(), anyLong())).thenReturn(true);
       when(mockClient.setFlag(any(HostAndPort.class), anyString(), anyString(), anyBoolean()))
           .thenReturn(true);
       ListMastersResponse listMastersResponse = mock(ListMastersResponse.class);
       when(listMastersResponse.getMasters()).thenReturn(Collections.emptyList());
       when(mockClient.listMasters()).thenReturn(listMastersResponse);
     } catch (Exception e) {
+      fail();
     }
 
     when(mockYBClient.getClient(any(), any())).thenReturn(mockClient);
diff --git a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/StartNodeInUniverseTest.java b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/StartNodeInUniverseTest.java
index 10294ffd15..5a3d62e65c 100644
--- a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/StartNodeInUniverseTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/StartNodeInUniverseTest.java
@@ -6,6 +6,7 @@ import static com.yugabyte.yw.common.AssertHelper.assertJsonEqual;
 import static com.yugabyte.yw.common.ModelFactory.createUniverse;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNull;
+import static org.junit.Assert.fail;
 import static org.mockito.ArgumentMatchers.any;
 import static org.mockito.ArgumentMatchers.anyBoolean;
 import static org.mockito.ArgumentMatchers.anyLong;
@@ -68,11 +69,13 @@ public class StartNodeInUniverseTest extends CommissionerBaseTest {
     mockClient = mock(YBClient.class);
     when(mockClient.waitForServer(any(), anyLong())).thenReturn(true);
     try {
+      when(mockClient.waitForMaster(any(), anyLong())).thenReturn(true);
       when(mockClient.setFlag(any(), anyString(), anyString(), anyBoolean())).thenReturn(true);
       ListMastersResponse listMastersResponse = mock(ListMastersResponse.class);
       when(listMastersResponse.getMasters()).thenReturn(Collections.emptyList());
       when(mockClient.listMasters()).thenReturn(listMastersResponse);
     } catch (Exception e) {
+      fail();
     }
     when(mockYBClient.getClient(any(), any())).thenReturn(mockClient);
     when(mockYBClient.getClientWithConfig(any())).thenReturn(mockClient);
diff --git a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/UniverseModifyBaseTest.java b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/UniverseModifyBaseTest.java
index 26f118b802..a3c387f40d 100644
--- a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/UniverseModifyBaseTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/UniverseModifyBaseTest.java
@@ -1,6 +1,7 @@
 package com.yugabyte.yw.commissioner.tasks;
 
 import static com.yugabyte.yw.common.ModelFactory.createUniverse;
+import static org.junit.Assert.fail;
 import static org.mockito.ArgumentMatchers.any;
 import static org.mockito.ArgumentMatchers.anyLong;
 import static org.mockito.Mockito.mock;
@@ -111,22 +112,26 @@ public abstract class UniverseModifyBaseTest extends CommissionerBaseTest {
         Universe.saveDetails(
             result.universeUUID, ApiUtils.mockUniverseUpdater(userIntent, true /* setMasters */));
     if (providerType == Common.CloudType.onprem) {
-      String instanceType =
-          result.getUniverseDetails().nodeDetailsSet.iterator().next().cloudInfo.instance_type;
-      Map<UUID, List<String>> onpremAzToNodes = new HashMap<>();
-      for (NodeDetails node : result.getUniverseDetails().nodeDetailsSet) {
-        List<String> nodeNames = onpremAzToNodes.getOrDefault(node.azUuid, new ArrayList<>());
-        nodeNames.add(node.nodeName);
-        onpremAzToNodes.put(node.azUuid, nodeNames);
-      }
-      Map<String, NodeInstance> nodeMap = NodeInstance.pickNodes(onpremAzToNodes, instanceType);
-      for (NodeDetails node : result.getUniverseDetails().nodeDetailsSet) {
-        NodeInstance nodeInstance = nodeMap.get(node.nodeName);
-        if (nodeInstance != null) {
-          node.nodeUuid = nodeInstance.getNodeUuid();
-        }
-      }
-      result.save();
+      Universe.saveDetails(
+          result.universeUUID,
+          u -> {
+            String instanceType = u.getNodes().iterator().next().cloudInfo.instance_type;
+            Map<UUID, List<String>> onpremAzToNodes = new HashMap<>();
+            for (NodeDetails node : u.getNodes()) {
+              List<String> nodeNames = onpremAzToNodes.getOrDefault(node.azUuid, new ArrayList<>());
+              nodeNames.add(node.nodeName);
+              onpremAzToNodes.put(node.azUuid, nodeNames);
+            }
+            Map<String, NodeInstance> nodeMap =
+                NodeInstance.pickNodes(onpremAzToNodes, instanceType);
+            for (NodeDetails node : u.getNodes()) {
+              NodeInstance nodeInstance = nodeMap.get(node.nodeName);
+              if (nodeInstance != null) {
+                node.nodeUuid = nodeInstance.getNodeUuid();
+              }
+            }
+          },
+          false);
     }
 
     return result;
diff --git a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/UpgradeKubernetesUniverseTest.java b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/UpgradeKubernetesUniverseTest.java
index 63d1fced69..c70ee166e2 100644
--- a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/UpgradeKubernetesUniverseTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/UpgradeKubernetesUniverseTest.java
@@ -10,6 +10,7 @@ import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertThat;
 import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 import static org.mockito.ArgumentMatchers.any;
 import static org.mockito.ArgumentMatchers.anyBoolean;
 import static org.mockito.ArgumentMatchers.anyLong;
@@ -117,8 +118,10 @@ public class UpgradeKubernetesUniverseTest extends CommissionerBaseTest {
         .thenReturn(HostAndPort.fromString(masterLeaderName).withDefaultPort(11));
     IsServerReadyResponse okReadyResp = new IsServerReadyResponse(0, "", null, 0, 0);
     try {
+      when(mockClient.waitForMaster(any(), anyLong())).thenReturn(true);
       when(mockClient.isServerReady(any(), anyBoolean())).thenReturn(okReadyResp);
     } catch (Exception ex) {
+      fail();
     }
     when(mockYBClient.getClient(any(), any())).thenReturn(mockClient);
   }
diff --git a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/UpgradeUniverseTest.java b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/UpgradeUniverseTest.java
index 7d58b2d4c4..dfe4cc25a9 100644
--- a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/UpgradeUniverseTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/UpgradeUniverseTest.java
@@ -223,6 +223,7 @@ public class UpgradeUniverseTest extends CommissionerBaseTest {
                 return new GetMasterClusterConfigResponse(1111, "", configBuilder.build(), null);
               });
     } catch (Exception ignored) {
+      fail();
     }
     when(mockYBClient.getClient(any(), any())).thenReturn(mockClient);
     when(mockYBClient.getClientWithConfig(any())).thenReturn(mockClient);
@@ -231,6 +232,7 @@ public class UpgradeUniverseTest extends CommissionerBaseTest {
         .thenReturn(HostAndPort.fromString("host-n2").withDefaultPort(11));
     IsServerReadyResponse okReadyResp = new IsServerReadyResponse(0, "", null, 0, 0);
     try {
+      when(mockClient.waitForMaster(any(HostAndPort.class), anyLong())).thenReturn(true);
       when(mockClient.isServerReady(any(HostAndPort.class), anyBoolean())).thenReturn(okReadyResp);
       when(mockClient.setFlag(any(HostAndPort.class), anyString(), anyString(), anyBoolean()))
           .thenReturn(true);
@@ -238,6 +240,7 @@ public class UpgradeUniverseTest extends CommissionerBaseTest {
       when(listMastersResponse.getMasters()).thenReturn(Collections.emptyList());
       when(mockClient.listMasters()).thenReturn(listMastersResponse);
     } catch (Exception ignored) {
+      fail();
     }
     ShellResponse dummyShellResponse = new ShellResponse();
     when(mockNodeManager.nodeCommand(any(), any())).thenReturn(dummyShellResponse);
diff --git a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/upgrade/KubernetesUpgradeTaskTest.java b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/upgrade/KubernetesUpgradeTaskTest.java
index 4c6365a50f..af965dcdc4 100644
--- a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/upgrade/KubernetesUpgradeTaskTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/upgrade/KubernetesUpgradeTaskTest.java
@@ -78,6 +78,7 @@ public abstract class KubernetesUpgradeTaskTest extends CommissionerBaseTest {
           "{\"status\": { \"phase\": \"Running\", \"conditions\": [{\"status\": \"True\"}]}}";
       when(mockKubernetesManager.getPodStatus(any(), any(), any())).thenReturn(responsePod);
       YBClient mockClient = mock(YBClient.class);
+      when(mockClient.waitForMaster(any(), anyLong())).thenReturn(true);
       when(mockClient.waitForServer(any(), anyLong())).thenReturn(true);
 
       String masterLeaderName = "yb-master-0.yb-masters.demo-universe.svc.cluster.local";
diff --git a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/upgrade/UpgradeTaskTest.java b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/upgrade/UpgradeTaskTest.java
index 50e621f012..5d12fbc955 100644
--- a/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/upgrade/UpgradeTaskTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/commissioner/tasks/upgrade/UpgradeTaskTest.java
@@ -7,6 +7,7 @@ import static com.yugabyte.yw.common.TestHelper.createTempFile;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 import static org.mockito.ArgumentMatchers.any;
 import static org.mockito.ArgumentMatchers.anyBoolean;
 import static org.mockito.ArgumentMatchers.anyLong;
@@ -162,12 +163,14 @@ public abstract class UpgradeTaskTest extends CommissionerBaseTest {
     mockClient = mock(YBClient.class);
     try {
       when(mockYBClient.getClient(any(), any())).thenReturn(mockClient);
+      when(mockClient.waitForMaster(any(HostAndPort.class), anyLong())).thenReturn(true);
       when(mockClient.waitForServer(any(HostAndPort.class), anyLong())).thenReturn(true);
       when(mockClient.getLeaderMasterHostAndPort())
           .thenReturn(HostAndPort.fromString("host-n2").withDefaultPort(11));
       IsServerReadyResponse okReadyResp = new IsServerReadyResponse(0, "", null, 0, 0);
       when(mockClient.isServerReady(any(HostAndPort.class), anyBoolean())).thenReturn(okReadyResp);
     } catch (Exception ignored) {
+      fail();
     }
 
     // Create dummy shell response
diff --git a/managed/src/test/java/com/yugabyte/yw/common/AlertManagerTest.java b/managed/src/test/java/com/yugabyte/yw/common/AlertManagerTest.java
index cedcb5afbe..4e8c720325 100644
--- a/managed/src/test/java/com/yugabyte/yw/common/AlertManagerTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/common/AlertManagerTest.java
@@ -126,10 +126,9 @@ public class AlertManagerTest extends FakeDBApplication {
 
   @Test
   public void testSendNotification_MetricsSetOk() {
-    metricService.setStatusMetric(
-        buildMetricTemplate(PlatformMetrics.ALERT_MANAGER_STATUS, defaultCustomer), "Some error");
-    am.setChannelStatusMetric(
-        PlatformMetrics.ALERT_MANAGER_CHANNEL_STATUS, defaultChannel, "Some channel error");
+    metricService.setFailureStatusMetric(
+        buildMetricTemplate(PlatformMetrics.ALERT_MANAGER_STATUS, defaultCustomer));
+    am.setChannelStatusMetric(PlatformMetrics.ALERT_MANAGER_CHANNEL_STATUS, defaultChannel, false);
 
     Alert alert = ModelFactory.createAlert(defaultCustomer);
 
@@ -144,7 +143,6 @@ public class AlertManagerTest extends FakeDBApplication {
                 .targetUuid(defaultCustomer.getUuid())
                 .build(),
             1.0);
-    assertThat(amStatus.getLabelValue(KnownAlertLabels.ERROR_MESSAGE), nullValue());
     Metric channelStatus =
         AssertHelper.assertMetricValue(
             metricService,
@@ -154,7 +152,6 @@ public class AlertManagerTest extends FakeDBApplication {
                 .targetUuid(defaultChannel.getUuid())
                 .build(),
             1.0);
-    assertThat(channelStatus.getLabelValue(KnownAlertLabels.ERROR_MESSAGE), nullValue());
   }
 
   @Test
@@ -177,9 +174,6 @@ public class AlertManagerTest extends FakeDBApplication {
                 .targetUuid(defaultChannel.getUuid())
                 .build(),
             0.0);
-    assertThat(
-        channelStatus.getLabelValue(KnownAlertLabels.ERROR_MESSAGE),
-        equalTo("Error sending notification: test"));
   }
 
   @Test
@@ -273,11 +267,6 @@ public class AlertManagerTest extends FakeDBApplication {
                 .targetUuid(defaultCustomer.getUuid())
                 .build(),
             0.0);
-    assertThat(
-        amStatus.getLabelValue(KnownAlertLabels.ERROR_MESSAGE),
-        equalTo(
-            "Unable to notify about alert(s) using default destination, "
-                + "there are no recipients configured in the customer's profile."));
 
     // Restoring recipients.
     when(emailHelper.getDestinations(defaultCustomer.getUuid()))
@@ -295,7 +284,6 @@ public class AlertManagerTest extends FakeDBApplication {
                 .targetUuid(defaultCustomer.getUuid())
                 .build(),
             1.0);
-    assertThat(amStatus.getLabelValue(KnownAlertLabels.ERROR_MESSAGE), nullValue());
   }
 
   // Aren't checking ACKNOWLEDGED in any state fields as such alert should not be
diff --git a/managed/src/test/java/com/yugabyte/yw/common/NodeManagerTest.java b/managed/src/test/java/com/yugabyte/yw/common/NodeManagerTest.java
index d76714ad08..f41b733478 100644
--- a/managed/src/test/java/com/yugabyte/yw/common/NodeManagerTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/common/NodeManagerTest.java
@@ -3367,7 +3367,6 @@ public class NodeManagerTest extends FakeDBApplication {
               universe.getUniverseDetails().getClusterByUuid(nodeDetails.placementUuid).userIntent;
           userIntent.enableNodeToNodeEncrypt = true;
         });
-
     nodeManager.nodeCommand(NodeManager.NodeCommandType.Precheck, nodeTaskParams);
     ArgumentCaptor<List> arg = ArgumentCaptor.forClass(List.class);
     verify(shellProcessHandler).run(arg.capture(), any(), anyString());
diff --git a/managed/src/test/java/com/yugabyte/yw/common/ShellProcessHandlerTest.java b/managed/src/test/java/com/yugabyte/yw/common/ShellProcessHandlerTest.java
index b1539fe81f..2ee4d67742 100644
--- a/managed/src/test/java/com/yugabyte/yw/common/ShellProcessHandlerTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/common/ShellProcessHandlerTest.java
@@ -111,4 +111,19 @@ public class ShellProcessHandlerTest extends TestCase {
     fileName.toFile().setExecutable(true);
     return fileName.toString();
   }
+
+  @Test
+  public void testGetPythonErrMsg() {
+    String errMsg =
+        "<yb-python-error>{\"type\": \"YBOpsRuntimeError\","
+            + "\"message\": \"Runtime error: Instance: i does not exist\","
+            + "\"file\": \"/Users/test/code/yugabyte-db/managed/devops/venv/bin/ybcloud.py\","
+            + "\"method\": \"<module>\", \"line\": 4}</yb-python-error>";
+    String out = ShellProcessHandler.getPythonErrMsg(0, errMsg);
+    assertNull(out);
+    out = ShellProcessHandler.getPythonErrMsg(2, errMsg);
+    assertEquals("YBOpsRuntimeError: Runtime error: Instance: i does not exist", out);
+    out = ShellProcessHandler.getPythonErrMsg(2, "{}");
+    assertNull(out);
+  }
 }
diff --git a/managed/src/test/java/com/yugabyte/yw/common/config/impl/SettableRuntimeConfigFactoryTest.java b/managed/src/test/java/com/yugabyte/yw/common/config/impl/SettableRuntimeConfigFactoryTest.java
index b45924e698..5d3b809254 100644
--- a/managed/src/test/java/com/yugabyte/yw/common/config/impl/SettableRuntimeConfigFactoryTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/common/config/impl/SettableRuntimeConfigFactoryTest.java
@@ -13,6 +13,7 @@ package com.yugabyte.yw.common.config.impl;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import com.typesafe.config.Config;
@@ -23,8 +24,10 @@ import com.yugabyte.yw.models.Customer;
 import com.yugabyte.yw.models.Provider;
 import com.yugabyte.yw.models.Universe;
 import io.ebean.Model;
+import java.util.Arrays;
 import java.util.Map;
 import java.util.Set;
+import java.util.stream.Collectors;
 import org.junit.Before;
 import org.junit.Test;
 
@@ -39,6 +42,7 @@ public class SettableRuntimeConfigFactoryTest extends FakeDBApplication {
   public static final String YB_CUSTOMER_RUNTIME_ONLY_KEY = "yb.runtime.customer";
   public static final String YB_PROVIDER_RUNTIME_ONLY_KEY = "yb.runtime.provider";
   public static final String YB_UNIVERSE_RUNTIME_ONLY_KEY = "yb.runtime.universe";
+  private static final String YB_CLOUD_ENABLED_KEY = "yb.cloud.enabled";
 
   // Key not defined in any scope
   public static final String YB_NOT_PRESENT_KEY = "yb.not.present";
@@ -53,10 +57,16 @@ public class SettableRuntimeConfigFactoryTest extends FakeDBApplication {
   }
 
   // app config
-  private static final Map<String, String> staticConfigMap =
+  private static final Map<String, Object> staticConfigMap =
       ImmutableMap.of(
-          YB_STATIC_ONLY_KEY, Scope.STATIC.toString(),
-          YB_OVERRIDDEN_KEY, Scope.STATIC.toString());
+          SettableRuntimeConfigFactory.RUNTIME_CONFIG_INCLUDED_OBJECTS,
+          ImmutableList.of("yb.external_script"),
+          YB_STATIC_ONLY_KEY,
+          Scope.STATIC.toString(),
+          YB_OVERRIDDEN_KEY,
+          Scope.STATIC.toString(),
+          YB_CLOUD_ENABLED_KEY,
+          Boolean.TRUE);
 
   // overrides in global scope:
   private static final Set<String> globalConfigSet =
@@ -79,7 +89,7 @@ public class SettableRuntimeConfigFactoryTest extends FakeDBApplication {
   private Provider defaultProvider;
 
   SettableRuntimeConfigFactory configFactory =
-      new SettableRuntimeConfigFactory(ConfigFactory.parseMap(staticConfigMap), null);
+      new SettableRuntimeConfigFactory(ConfigFactory.parseMap(staticConfigMap), null, null);
 
   @Before
   public void setUp() {
@@ -208,6 +218,73 @@ public class SettableRuntimeConfigFactoryTest extends FakeDBApplication {
     assertEquals(2L, configFactory.forUniverse(universe2).getDuration(TASK_GC_FREQUENCY).toDays());
   }
 
+  @Test
+  public void testToRedactedString() {
+    Map<String, Object> inputMap =
+        ImmutableMap.<String, Object>builder()
+            .put("testemail", "email")
+            .put("testpassword", "password")
+            .put("testserver", "server")
+            .put("email", "test")
+            .put("password", "password")
+            .put("server", "server")
+            .put("test_email", "email")
+            .put("test_password", "password")
+            .put("test_server", "server")
+            .put("test_email_user", "user")
+            .put("test-email", "email")
+            .put("test-password", "password")
+            .put("test-server", "server")
+            .put("test-email-user", "user")
+            .put(
+                "test1",
+                ImmutableMap.of(
+                    "email", "test1@mail.com",
+                    "password", "password1",
+                    "server", "server1",
+                    "email_user", "user1"))
+            .put(
+                "test2",
+                ImmutableMap.of(
+                    "email", "test2@mail.com",
+                    "password", "password2",
+                    "server", "server2",
+                    "email_user", "user2"))
+            .build();
+    Map<String, String> expectedMap =
+        ImmutableMap.<String, String>builder()
+            .put("testemail", "Quoted(\"email\")")
+            .put("testpassword", "Quoted(\"password\")")
+            .put("testserver", "Quoted(\"server\")")
+            .put("email", "REDACTED")
+            .put("password", "REDACTED")
+            .put("server", "REDACTED")
+            .put("test_email", "REDACTED")
+            .put("test_password", "REDACTED")
+            .put("test_server", "REDACTED")
+            .put("test_email_user", "Quoted(\"user\")")
+            .put("test-email", "REDACTED")
+            .put("test-password", "REDACTED")
+            .put("test-server", "REDACTED")
+            .put("test-email-user", "Quoted(\"user\")")
+            .put("test1.email", "REDACTED")
+            .put("test1.password", "REDACTED")
+            .put("test1.server", "REDACTED")
+            .put("test1.email_user", "Quoted(\"user1\")")
+            .put("test2.email", "REDACTED")
+            .put("test2.password", "REDACTED")
+            .put("test2.server", "REDACTED")
+            .put("test2.email_user", "Quoted(\"user2\")")
+            .build();
+
+    String output = SettableRuntimeConfigFactory.toRedactedString(ConfigFactory.parseMap(inputMap));
+    Map<String, String> outputMap =
+        Arrays.stream(output.split(","))
+            .map(s -> s.split("="))
+            .collect(Collectors.toMap(tokens -> tokens[0].trim(), tokens -> tokens[1].trim()));
+    assertEquals(expectedMap, outputMap);
+  }
+
   private RuntimeConfig<Model> setupGlobalConfig() {
     RuntimeConfig<Model> runtimeConfig = configFactory.globalRuntimeConf();
     globalConfigSet.forEach(s -> runtimeConfig.setValue(s, Scope.GLOBAL.name()));
diff --git a/managed/src/test/java/com/yugabyte/yw/common/ha/PlatformInstanceClientFactoryTest.java b/managed/src/test/java/com/yugabyte/yw/common/ha/PlatformInstanceClientFactoryTest.java
new file mode 100644
index 0000000000..a9e7fa868e
--- /dev/null
+++ b/managed/src/test/java/com/yugabyte/yw/common/ha/PlatformInstanceClientFactoryTest.java
@@ -0,0 +1,163 @@
+/*
+ * Copyright 2022 YugaByte, Inc. and Contributors
+ *
+ * Licensed under the Polyform Free Trial License 1.0.0 (the "License"); you
+ * may not use this file except in compliance with the License. You
+ * may obtain a copy of the License at
+ *
+ * http://github.com/YugaByte/yugabyte-db/blob/master/licenses/POLYFORM-FREE-TRIAL-LICENSE-1.0.0.txt
+ */
+
+package com.yugabyte.yw.common.ha;
+
+import static com.yugabyte.yw.models.ScopedRuntimeConfig.GLOBAL_SCOPE_UUID;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotEquals;
+import static org.junit.Assert.assertThrows;
+import static play.test.Helpers.fakeRequest;
+
+import com.yugabyte.yw.common.FakeApi;
+import com.yugabyte.yw.common.FakeDBApplication;
+import com.yugabyte.yw.common.ModelFactory;
+import com.yugabyte.yw.models.Customer;
+import com.yugabyte.yw.models.Users;
+import com.yugabyte.yw.models.Users.Role;
+import io.ebean.Ebean;
+import io.ebean.EbeanServer;
+import org.junit.Before;
+import org.junit.Test;
+import play.inject.guice.GuiceApplicationBuilder;
+import play.libs.ws.WSClient;
+import play.mvc.Http;
+
+public class PlatformInstanceClientFactoryTest extends FakeDBApplication {
+
+  private static final String KEY = "/api/customers/%s/runtime_config/%s/key/%s";
+  private static final String NOTIFY = "/api/customers/%s/runtime_config/%s/notify/%s";
+  private String authToken;
+  private Customer customer;
+  private EbeanServer localEBeanServer;
+  private FakeApi fakeApi;
+  private PlatformInstanceClientFactory platformInstanceClientFactory;
+
+  private static final String REMOTE_ACME_ORG = "http://remote.acme.org";
+
+  private static final String BAD_CA_CERT_KEY = "-----BAD CERT-----\n";
+
+  private static final String GOOD_CA_CERT_KEY =
+      "-----BEGIN CERTIFICATE-----\n"
+          + "MIIDzTCCArWgAwIBAgIQCjeHZF5ftIwiTv0b7RQMPDANBgkqhkiG9w0BAQsFADBa\n"
+          + "MQswCQYDVQQGEwJJRTESMBAGA1UEChMJQmFsdGltb3JlMRMwEQYDVQQLEwpDeWJl\n"
+          + "clRydXN0MSIwIAYDVQQDExlCYWx0aW1vcmUgQ3liZXJUcnVzdCBSb290MB4XDTIw\n"
+          + "MDEyNzEyNDgwOFoXDTI0MTIzMTIzNTk1OVowSjELMAkGA1UEBhMCVVMxGTAXBgNV\n"
+          + "BAoTEENsb3VkZmxhcmUsIEluYy4xIDAeBgNVBAMTF0Nsb3VkZmxhcmUgSW5jIEVD\n"
+          + "QyBDQS0zMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEua1NZpkUC0bsH4HRKlAe\n"
+          + "nQMVLzQSfS2WuIg4m4Vfj7+7Te9hRsTJc9QkT+DuHM5ss1FxL2ruTAUJd9NyYqSb\n"
+          + "16OCAWgwggFkMB0GA1UdDgQWBBSlzjfq67B1DpRniLRF+tkkEIeWHzAfBgNVHSME\n"
+          + "GDAWgBTlnVkwgkdYzKz6CFQ2hns6tQRN8DAOBgNVHQ8BAf8EBAMCAYYwHQYDVR0l\n"
+          + "BBYwFAYIKwYBBQUHAwEGCCsGAQUFBwMCMBIGA1UdEwEB/wQIMAYBAf8CAQAwNAYI\n"
+          + "KwYBBQUHAQEEKDAmMCQGCCsGAQUFBzABhhhodHRwOi8vb2NzcC5kaWdpY2VydC5j\n"
+          + "b20wOgYDVR0fBDMwMTAvoC2gK4YpaHR0cDovL2NybDMuZGlnaWNlcnQuY29tL09t\n"
+          + "bmlyb290MjAyNS5jcmwwbQYDVR0gBGYwZDA3BglghkgBhv1sAQEwKjAoBggrBgEF\n"
+          + "BQcCARYcaHR0cHM6Ly93d3cuZGlnaWNlcnQuY29tL0NQUzALBglghkgBhv1sAQIw\n"
+          + "CAYGZ4EMAQIBMAgGBmeBDAECAjAIBgZngQwBAgMwDQYJKoZIhvcNAQELBQADggEB\n"
+          + "AAUkHd0bsCrrmNaF4zlNXmtXnYJX/OvoMaJXkGUFvhZEOFp3ArnPEELG4ZKk40Un\n"
+          + "+ABHLGioVplTVI+tnkDB0A+21w0LOEhsUCxJkAZbZB2LzEgwLt4I4ptJIsCSDBFe\n"
+          + "lpKU1fwg3FZs5ZKTv3ocwDfjhUkV+ivhdDkYD7fa86JXWGBPzI6UAPxGezQxPk1H\n"
+          + "goE6y/SJXQ7vTQ1unBuCJN0yJV0ReFEQPaA1IwQvZW+cwdFD19Ae8zFnWSfda9J1\n"
+          + "CZMRJCQUzym+5iPDuI9yP+kHyCREU3qzuWFloUwOxkgAyXVjBYdwRVKD05WdRerw\n"
+          + "6DEdfgkfCv4+3ao8XnTSrLE=\n"
+          + "-----END CERTIFICATE-----";
+
+  @Override
+  protected GuiceApplicationBuilder configureApplication(GuiceApplicationBuilder builder) {
+    return super.configureApplication(builder);
+  }
+
+  @Before
+  public void setUp() {
+    customer = ModelFactory.testCustomer();
+    Users user = ModelFactory.testUser(customer, Role.SuperAdmin);
+    authToken = user.createAuthToken();
+    localEBeanServer = Ebean.getDefaultServer();
+    fakeApi = new FakeApi(app, localEBeanServer);
+    platformInstanceClientFactory = app.injector().instanceOf(PlatformInstanceClientFactory.class);
+  }
+
+  @Test
+  public void getDefaultClient() {
+    final PlatformInstanceClient platformInstanceClient =
+        platformInstanceClientFactory.getClient("clusterK$Y", REMOTE_ACME_ORG);
+    assertEquals("Expect default wsClient", mockApiHelper, platformInstanceClient.getApiHelper());
+  }
+
+  @Test
+  public void getCustomClient() {
+    setWsConfig(GOOD_CA_CERT_KEY);
+    final PlatformInstanceClient platformInstanceClient =
+        platformInstanceClientFactory.getClient("clusterK$Y", REMOTE_ACME_ORG);
+    assertNotEquals(
+        "Expect custom wsClient differnt from default",
+        app.injector().instanceOf(WSClient.class),
+        platformInstanceClient.getApiHelper().getWsClient());
+
+    // get client with same config
+    final PlatformInstanceClient platformInstanceClient2 =
+        platformInstanceClientFactory.getClient("clusterK$Y", REMOTE_ACME_ORG);
+
+    assertEquals(
+        "Expect reuse the underlying wsClient",
+        platformInstanceClient.getApiHelper().getWsClient(),
+        platformInstanceClient2.getApiHelper().getWsClient());
+
+    // set new config
+    setWsConfig(GOOD_CA_CERT_KEY);
+    final PlatformInstanceClient platformInstanceClient3 =
+        platformInstanceClientFactory.getClient("clusterK$Y", REMOTE_ACME_ORG);
+    // This should NOT reuse the underlying apiHelper
+    assertNotEquals(platformInstanceClient, platformInstanceClient2);
+    assertNotEquals(
+        platformInstanceClient.getApiHelper().getWsClient(),
+        platformInstanceClient3.getApiHelper().getWsClient());
+  }
+
+  @Test
+  public void setWsConfig_badCert() {
+    assertThrows(RuntimeException.class, () -> setWsConfig(BAD_CA_CERT_KEY));
+  }
+
+  private void setWsConfig(String pemKey) {
+    String wsConfig =
+        String.format(
+            "    {\n"
+                + "      ssl {\n"
+                + "        acceptAnyCert = false\n"
+                + "        trustManager {\n"
+                + "          stores += {\n"
+                + "            type = PEM\n"
+                + "            data = \"\"\"%s\"\"\"\n"
+                + "          }\n"
+                + "        }\n"
+                + "      }\n"
+                + "    }\n",
+            pemKey);
+
+    setConfigKey("yb.ha.ws", wsConfig);
+  }
+
+  private void setConfigKey(String k, String v) {
+    Http.RequestBuilder request =
+        fakeRequest("PUT", String.format(KEY, customer.uuid, GLOBAL_SCOPE_UUID, k))
+            .header("X-AUTH-TOKEN", authToken)
+            .header("content-type", "text/plain")
+            .bodyText(v);
+    fakeApi.route(request);
+  }
+
+  private void notifyConfigChangeListeners(String pathPrefix) {
+    Http.RequestBuilder request =
+        fakeRequest("PUT", String.format(NOTIFY, customer.uuid, GLOBAL_SCOPE_UUID, pathPrefix))
+            .header("X-AUTH-TOKEN", authToken);
+    fakeApi.route(request);
+  }
+}
diff --git a/managed/src/test/java/com/yugabyte/yw/common/ha/PlatformTest.java b/managed/src/test/java/com/yugabyte/yw/common/ha/PlatformTest.java
index c078014d1a..7a6bb67b77 100644
--- a/managed/src/test/java/com/yugabyte/yw/common/ha/PlatformTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/common/ha/PlatformTest.java
@@ -67,6 +67,7 @@ import play.test.Helpers;
 public class PlatformTest extends FakeDBApplication {
   Customer customer;
   Users user;
+  private String authToken;
   String clusterKey;
   Application remoteApp;
   PlatformInstance localInstance;
@@ -85,7 +86,8 @@ public class PlatformTest extends FakeDBApplication {
   @Before
   public void setup() {
     customer = ModelFactory.testCustomer();
-    user = ModelFactory.testUser(customer);
+    user = ModelFactory.testUser(customer, Users.Role.SuperAdmin);
+    authToken = user.createAuthToken();
     localEBeanServer = Ebean.getDefaultServer();
     fakeApi = new FakeApi(app, localEBeanServer);
     clusterKey = createClusterKey();
@@ -142,6 +144,7 @@ public class PlatformTest extends FakeDBApplication {
         app.injector().instanceOf(PlatformReplicationManager.class);
 
     Ebean.register(localEBeanServer, true);
+
     assertTrue("sendBackup failed", replicationManager.sendBackup(remoteInstance));
 
     assertTrue(fakeDump.exists());
@@ -170,7 +173,7 @@ public class PlatformTest extends FakeDBApplication {
             .toFile();
     assertTrue(uploadedFile.exists());
     String uploadedContents = FileUtils.readFileToString(uploadedFile, Charset.defaultCharset());
-    assertTrue(FileUtils.contentEquals(backupFile, uploadedFile));
+    assertTrue("Actual:" + uploadedContents, FileUtils.contentEquals(backupFile, uploadedFile));
   }
 
   private File createFakeDump() throws IOException {
@@ -188,7 +191,6 @@ public class PlatformTest extends FakeDBApplication {
 
   private PlatformInstance createPlatformInstance(
       UUID configUUID, String remoteAcmeOrg, boolean isLocal, boolean isLeader) {
-    String authToken = user.createAuthToken();
     String uri = "/api/settings/ha/config/" + configUUID.toString() + "/instance";
     JsonNode body =
         Json.newObject()
diff --git a/managed/src/test/java/com/yugabyte/yw/common/metrics/MetricServiceTest.java b/managed/src/test/java/com/yugabyte/yw/common/metrics/MetricServiceTest.java
index e13c37d843..f8ece88cc4 100644
--- a/managed/src/test/java/com/yugabyte/yw/common/metrics/MetricServiceTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/common/metrics/MetricServiceTest.java
@@ -1,18 +1,16 @@
 // Copyright (c) YugaByte, Inc.
 package com.yugabyte.yw.common.metrics;
 
+import static com.yugabyte.yw.common.metrics.MetricService.STATUS_NOT_OK;
 import static com.yugabyte.yw.common.metrics.MetricService.buildMetricTemplate;
 import static com.yugabyte.yw.models.helpers.CommonUtils.datePlus;
 import static org.hamcrest.MatcherAssert.assertThat;
-import static org.hamcrest.Matchers.containsInAnyOrder;
+import static org.hamcrest.Matchers.contains;
 import static org.hamcrest.Matchers.empty;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.hasSize;
-import static org.hamcrest.Matchers.not;
 import static org.hamcrest.Matchers.notNullValue;
 import static org.hamcrest.Matchers.nullValue;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
 import com.google.common.collect.ImmutableList;
@@ -25,11 +23,9 @@ import com.yugabyte.yw.models.Universe;
 import com.yugabyte.yw.models.filters.MetricFilter;
 import com.yugabyte.yw.models.helpers.KnownAlertLabels;
 import com.yugabyte.yw.models.helpers.PlatformMetrics;
-import java.time.Instant;
 import java.time.temporal.ChronoUnit;
 import java.util.ArrayList;
 import java.util.Collections;
-import java.util.Date;
 import java.util.List;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.ExecutorService;
@@ -45,8 +41,6 @@ import org.mockito.junit.MockitoJUnitRunner;
 @Slf4j
 public class MetricServiceTest extends FakeDBApplication {
 
-  private final Instant testStart = Instant.now();
-
   private Customer customer;
 
   private Universe universe;
@@ -78,10 +72,9 @@ public class MetricServiceTest extends FakeDBApplication {
 
   @Test
   public void testUpdateAndGetByKey() {
-    metricService.setStatusMetric(
+    metricService.setFailureStatusMetric(
         buildMetricTemplate(PlatformMetrics.ALERT_MANAGER_STATUS, universe)
-            .setKeyLabel(KnownAlertLabels.NODE_NAME, "node1"),
-        "Error");
+            .setKeyLabel(KnownAlertLabels.NODE_NAME, "node1"));
 
     metricService.setOkStatusMetric(
         buildMetricTemplate(PlatformMetrics.ALERT_MANAGER_STATUS, universe)
@@ -92,7 +85,7 @@ public class MetricServiceTest extends FakeDBApplication {
             .customerUuid(customer.getUuid())
             .name(PlatformMetrics.ALERT_MANAGER_STATUS.getMetricName())
             .targetUuid(universe.getUniverseUUID())
-            .sourceLabels("node_name:node1")
+            .sourceLabel("node_name", "node1")
             .build();
     Metric metric = metricService.get(key);
 
@@ -134,7 +127,6 @@ public class MetricServiceTest extends FakeDBApplication {
             buildMetricTemplate(PlatformMetrics.UNIVERSE_UPDATE_IN_PROGRESS, universe)
                 .setValue(1D));
     metricService.save(metrics);
-    metricService.flushMetricsToDb();
 
     List<Metric> updatedMetrics =
         ImmutableList.of(
@@ -146,15 +138,14 @@ public class MetricServiceTest extends FakeDBApplication {
                 .setLabel(KnownAlertLabels.NODE_NAME, "qwerty1"),
             buildMetricTemplate(PlatformMetrics.UNIVERSE_UPDATE_IN_PROGRESS, universe)
                 .setValue(0D)
-                .setLabels(Collections.emptyList()));
-    metricService.save(updatedMetrics);
+                .setLabels(Collections.emptyMap()));
 
-    Future<?> metricFlushFuture = executor.submit(() -> metricService.flushMetricsToDb());
+    Future<?> metricSaveFuture = executor.submit(() -> metricService.save(updatedMetrics));
     Future<?> customerRemovalFuture =
         executor.submit(
             () -> {
               customer.delete();
-              metricService.handleSourceRemoval(customer.getUuid(), null);
+              metricService.markSourceRemoved(customer.getUuid(), null);
             });
     try {
       customerRemovalFuture.get();
@@ -163,57 +154,20 @@ public class MetricServiceTest extends FakeDBApplication {
       fail("Exception occurred in customer removal worker: " + e);
     }
     try {
-      metricFlushFuture.get();
+      metricSaveFuture.get();
     } catch (ExecutionException e) {
-      log.info("Exception occurred in metric flush worker. Will retry:", e);
-      metricService.flushMetricsToDb();
-      log.info("Metrics saved successfully on retry.");
+      log.error("Exception occurred in metric save worker", e);
+      fail("Exception occurred in metric save worker: " + e);
     }
     executor.shutdown();
   }
 
-  @Test
-  public void testMetricStorageInitialization() {
-    List<Metric> metrics =
-        ImmutableList.of(
-            buildMetricTemplate(PlatformMetrics.UNIVERSE_EXISTS, universe).setValue(1D),
-            buildMetricTemplate(PlatformMetrics.UNIVERSE_PAUSED, universe).setValue(1D),
-            buildMetricTemplate(PlatformMetrics.UNIVERSE_UPDATE_IN_PROGRESS, universe)
-                .setValue(1D));
-    metricService.save(metrics);
-
-    List<Metric> inMemoryMetrics = metricService.list(MetricFilter.builder().build());
-    assertThat(inMemoryMetrics, containsInAnyOrder(metrics.toArray(new Metric[0])));
-    List<Metric> persistedMetrics = metricService.list(MetricFilter.builder().build(), true);
-    assertThat(persistedMetrics, empty());
-
-    metricService.flushMetricsToDb();
-
-    inMemoryMetrics = metricService.list(MetricFilter.builder().build());
-    assertThat(inMemoryMetrics, containsInAnyOrder(metrics.toArray(new Metric[0])));
-    persistedMetrics = metricService.list(MetricFilter.builder().build(), true);
-    assertThat(persistedMetrics, containsInAnyOrder(metrics.toArray(new Metric[0])));
-
-    MetricStorage newStorage = new MetricStorage();
-    MetricService newService = new MetricService(newStorage);
-
-    inMemoryMetrics = newService.list(MetricFilter.builder().build());
-    assertThat(inMemoryMetrics, empty());
-
-    newService.initialize();
-
-    inMemoryMetrics = newService.list(MetricFilter.builder().build());
-    assertThat(inMemoryMetrics, hasSize(3));
-    assertThat(inMemoryMetrics, containsInAnyOrder(metrics.toArray(new Metric[0])));
-  }
-
   @Test
   public void testDelete() {
-    metricService.setStatusMetric(
-        buildMetricTemplate(PlatformMetrics.ALERT_MANAGER_STATUS, universe), "Error");
+    metricService.setFailureStatusMetric(
+        buildMetricTemplate(PlatformMetrics.ALERT_MANAGER_STATUS, universe));
     metricService.setOkStatusMetric(
         buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_STATUS, universe));
-    metricService.flushMetricsToDb();
 
     MetricKey keyToDelete =
         MetricKey.builder()
@@ -238,22 +192,71 @@ public class MetricServiceTest extends FakeDBApplication {
 
     assertThat(deletedMetric, empty());
     assertThat(remainingMetric, hasSize(1));
+  }
+
+  @Test
+  public void testMarkSourceInactive() {
+    MetricKey universeExistsMetricKey =
+        MetricKey.from(buildMetricTemplate(PlatformMetrics.UNIVERSE_EXISTS, universe));
+    metricService.setFailureStatusMetric(
+        buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_STATUS, universe));
+    metricService.setOkStatusMetric(buildMetricTemplate(PlatformMetrics.UNIVERSE_EXISTS, universe));
 
-    List<Metric> deletedMetricInDb = metricService.list(filterToDelete, true);
-    List<Metric> remainingMetricInDb = metricService.list(filterRemaining, true);
+    metricService.markSourceInactive(customer.getUuid(), universe.getUniverseUUID());
 
-    // Both metrics are still in DB.
-    assertThat(deletedMetricInDb, hasSize(1));
-    assertThat(remainingMetricInDb, hasSize(1));
+    Metric universeExistsMetric = metricService.get(universeExistsMetricKey);
 
-    metricService.flushMetricsToDb();
+    List<Metric> metricsLeft = metricService.list(MetricFilter.builder().build());
+
+    assertThat(metricsLeft, contains(universeExistsMetric));
+
+    metricService.setOkStatusMetric(
+        buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_STATUS, universe));
+    metricService.setFailureStatusMetric(
+        buildMetricTemplate(PlatformMetrics.UNIVERSE_EXISTS, universe));
+
+    universeExistsMetric = metricService.get(universeExistsMetricKey);
+
+    metricsLeft = metricService.list(MetricFilter.builder().build());
+
+    // Only metrics, valid for INACTIVE state are written.
+    assertThat(metricsLeft, contains(universeExistsMetric));
+    assertThat(universeExistsMetric.getValue(), equalTo(STATUS_NOT_OK));
+
+    metricService.markSourceActive(customer.getUuid(), universe.getUniverseUUID());
+
+    metricService.setOkStatusMetric(
+        buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_STATUS, universe));
+    metricService.setFailureStatusMetric(
+        buildMetricTemplate(PlatformMetrics.UNIVERSE_EXISTS, universe));
+
+    metricsLeft = metricService.list(MetricFilter.builder().build());
+
+    // Writing both metrics after universe unpause is successful.
+    assertThat(metricsLeft, hasSize(2));
+  }
+
+  @Test
+  public void testMarkSourceRemoved() {
+    metricService.setFailureStatusMetric(
+        buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_STATUS, universe));
+    metricService.setOkStatusMetric(buildMetricTemplate(PlatformMetrics.UNIVERSE_EXISTS, universe));
+
+    metricService.markSourceRemoved(customer.getUuid(), universe.getUniverseUUID());
+
+    List<Metric> metricsLeft = metricService.list(MetricFilter.builder().build());
+
+    assertThat(metricsLeft, empty());
+
+    metricService.setOkStatusMetric(
+        buildMetricTemplate(PlatformMetrics.HEALTH_CHECK_STATUS, universe));
+    metricService.setFailureStatusMetric(
+        buildMetricTemplate(PlatformMetrics.UNIVERSE_EXISTS, universe));
 
-    deletedMetricInDb = metricService.list(filterToDelete, true);
-    remainingMetricInDb = metricService.list(filterRemaining, true);
+    metricsLeft = metricService.list(MetricFilter.builder().build());
 
-    // Only remaining metric left in DB.
-    assertThat(deletedMetricInDb, empty());
-    assertThat(remainingMetricInDb, hasSize(1));
+    // No metrics can be written after source is permanently deleted.
+    assertThat(metricsLeft, empty());
   }
 
   @Test
@@ -303,29 +306,16 @@ public class MetricServiceTest extends FakeDBApplication {
 
     assertThat(updatedNode1Metric.getExpireTime(), equalTo(node1Metric.getExpireTime()));
     assertThat(updatedNode2Metric.getValue(), equalTo(3D));
-    assertThat(updatedNode2Metric.getExpireTime(), not(equalTo(node1Metric.getExpireTime())));
+    assertThat(updatedNode2Metric.getExpireTime(), equalTo(node2Metric.getExpireTime()));
     assertThat(updatedNode3Metric.getValue(), equalTo(3D));
-    assertThat(updatedNode3Metric.getExpireTime(), not(equalTo(node1Metric.getExpireTime())));
+    assertThat(updatedNode3Metric.getExpireTime(), equalTo(node3Metric.getExpireTime()));
     assertThat(updatedNode4Metric, nullValue());
   }
 
   private void assertMetric(Metric metric, double value) {
     assertThat(metric.getCreateTime(), notNullValue());
     assertThat(metric.getUpdateTime(), notNullValue());
-    assertFalse(
-        metric
-            .getExpireTime()
-            .before(
-                Date.from(
-                    testStart.plus(
-                        MetricService.DEFAULT_METRIC_EXPIRY_SEC - 1, ChronoUnit.SECONDS))));
-    assertTrue(
-        metric
-            .getExpireTime()
-            .before(
-                Date.from(
-                    testStart.plus(
-                        MetricService.DEFAULT_METRIC_EXPIRY_SEC + 10, ChronoUnit.SECONDS))));
+    assertThat(metric.getExpireTime(), notNullValue());
     assertThat(metric.getCustomerUUID(), equalTo(customer.getUuid()));
     assertThat(metric.getType(), equalTo(Metric.Type.GAUGE));
     assertThat(metric.getName(), equalTo(PlatformMetrics.ALERT_MANAGER_STATUS.getMetricName()));
@@ -339,6 +329,5 @@ public class MetricServiceTest extends FakeDBApplication {
     assertThat(
         metric.getLabelValue(KnownAlertLabels.UNIVERSE_UUID),
         equalTo(universe.getUniverseUUID().toString()));
-    assertThat(metric.getLabelValue(KnownAlertLabels.ERROR_MESSAGE), nullValue());
   }
 }
diff --git a/managed/src/test/java/com/yugabyte/yw/common/metrics/PlatformMetricProcessorTest.java b/managed/src/test/java/com/yugabyte/yw/common/metrics/PlatformMetricProcessorTest.java
deleted file mode 100644
index f6defc6702..0000000000
--- a/managed/src/test/java/com/yugabyte/yw/common/metrics/PlatformMetricProcessorTest.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Copyright 2021 YugaByte, Inc. and Contributors
- *
- * Licensed under the Polyform Free Trial License 1.0.0 (the "License"); you
- * may not use this file except in compliance with the License. You
- * may obtain a copy of the License at
- *
- * http://github.com/YugaByte/yugabyte-db/blob/master/licenses/POLYFORM-FREE-TRIAL-LICENSE-1.0.0.txt
- */
-package com.yugabyte.yw.common.metrics;
-
-import static org.hamcrest.MatcherAssert.assertThat;
-import static org.mockito.ArgumentMatchers.anyList;
-import static org.mockito.Mockito.times;
-import static org.mockito.Mockito.verify;
-
-import com.yugabyte.yw.common.FakeDBApplication;
-import com.yugabyte.yw.common.ModelFactory;
-import com.yugabyte.yw.models.Customer;
-import com.yugabyte.yw.models.Metric;
-import com.yugabyte.yw.models.helpers.CommonUtils;
-import java.util.List;
-import org.hamcrest.Matchers;
-import org.junit.Before;
-import org.junit.Test;
-import org.junit.runner.RunWith;
-import org.mockito.ArgumentCaptor;
-import org.mockito.Captor;
-import org.mockito.Mock;
-import org.mockito.junit.MockitoJUnitRunner;
-
-@RunWith(MockitoJUnitRunner.class)
-public class PlatformMetricProcessorTest extends FakeDBApplication {
-
-  Customer customer;
-  @Mock MetricService metricService;
-  @Captor private ArgumentCaptor<List<Metric>> metricsCaptor;
-
-  PlatformMetricsProcessor platformMetricsProcessor;
-
-  @Before
-  public void setUp() {
-    customer = ModelFactory.testCustomer();
-    platformMetricsProcessor =
-        new PlatformMetricsProcessor(null, null, metricService, new UniverseMetricProvider());
-  }
-
-  @Test
-  public void testMetricsBatching() {
-    for (int i = 0; i < 40; i++) {
-      ModelFactory.createUniverse("testUniverse" + i, customer.getCustomerId());
-    }
-
-    platformMetricsProcessor.scheduleRunner();
-
-    verify(metricService, times(2)).cleanAndSave(metricsCaptor.capture(), anyList());
-
-    for (List<Metric> metrics : metricsCaptor.getAllValues()) {
-      assertThat(metrics.size(), Matchers.lessThanOrEqualTo(CommonUtils.DB_OR_CHAIN_TO_WARN));
-    }
-  }
-}
diff --git a/managed/src/test/java/com/yugabyte/yw/controllers/AlertControllerTest.java b/managed/src/test/java/com/yugabyte/yw/controllers/AlertControllerTest.java
index 616a3668c7..d75771580d 100644
--- a/managed/src/test/java/com/yugabyte/yw/controllers/AlertControllerTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/controllers/AlertControllerTest.java
@@ -64,7 +64,6 @@ import com.yugabyte.yw.models.common.Condition;
 import com.yugabyte.yw.models.common.Unit;
 import com.yugabyte.yw.models.filters.AlertFilter;
 import com.yugabyte.yw.models.helpers.CommonUtils;
-import com.yugabyte.yw.models.helpers.KnownAlertLabels;
 import com.yugabyte.yw.models.helpers.PlatformMetrics;
 import com.yugabyte.yw.models.paging.AlertConfigurationPagedResponse;
 import com.yugabyte.yw.models.paging.AlertPagedResponse;
@@ -321,9 +320,8 @@ public class AlertControllerTest extends FakeDBApplication {
             .setCustomerUUID(customer.getUuid())
             .setSourceUuid(createdChannel.getUuid())
             .setLabels(MetricLabelsBuilder.create().appendSource(createdChannel).getMetricLabels())
-            .setValue(0.0)
-            .setLabel(KnownAlertLabels.ERROR_MESSAGE, "Some error");
-    metricService.cleanAndSave(Collections.singletonList(channelStatus));
+            .setValue(0.0);
+    metricService.save(channelStatus);
 
     Result result =
         doRequestWithAuthToken(
@@ -1038,7 +1036,7 @@ public class AlertControllerTest extends FakeDBApplication {
                 new AlertConfigurationThreshold()
                     .setCondition(Condition.GREATER_THAN)
                     .setThreshold(1D))));
-    assertThat(configuration.getDurationSec(), equalTo(15));
+    assertThat(configuration.getDurationSec(), equalTo(0));
     assertThat(configuration.getDestinationUUID(), equalTo(destination.getUuid()));
   }
 
@@ -1150,7 +1148,9 @@ public class AlertControllerTest extends FakeDBApplication {
               authToken);
       assertThat(result.status(), equalTo(OK));
       JsonNode resultJson = Json.parse(contentAsString(result));
-      assertThat(resultJson.get("message").asText(), equalTo("Alert sent successfully"));
+      assertThat(
+          resultJson.get("message").asText(),
+          equalTo("Result: Some channel - Alert sent successfully"));
       RecordedRequest request = server.takeRequest();
       assertThat(request.getPath(), is("/some/path"));
       assertThat(
diff --git a/managed/src/test/java/com/yugabyte/yw/controllers/ImportControllerTest.java b/managed/src/test/java/com/yugabyte/yw/controllers/ImportControllerTest.java
index 11c638ae67..6be748b7b5 100644
--- a/managed/src/test/java/com/yugabyte/yw/controllers/ImportControllerTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/controllers/ImportControllerTest.java
@@ -19,6 +19,7 @@ import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertThat;
+import static org.junit.Assert.fail;
 import static org.mockito.Matchers.any;
 import static org.mockito.Matchers.anyLong;
 import static org.mockito.Mockito.doNothing;
@@ -84,10 +85,11 @@ public class ImportControllerTest extends CommissionerBaseTest {
     mockTabletSIs.add(si);
     when(mockResponse.getTabletServersList()).thenReturn(mockTabletSIs);
     try {
+      when(mockClient.waitForMaster(any(), anyLong())).thenReturn(true);
       when(mockClient.listTabletServers()).thenReturn(mockResponse);
       doNothing().when(mockClient).waitForMasterLeader(anyLong());
     } catch (Exception e) {
-      e.printStackTrace();
+      fail();
     }
   }
 
@@ -304,8 +306,8 @@ public class ImportControllerTest extends CommissionerBaseTest {
   }
 
   @Test
-  public void testFailedMasterImport() {
-    when(mockClient.waitForServer(any(), anyLong())).thenThrow(IllegalStateException.class);
+  public void testFailedMasterImport() throws Exception {
+    when(mockClient.waitForMaster(any(), anyLong())).thenThrow(IllegalStateException.class);
     String url = "/api/customers/" + customer.uuid + "/universes/import";
     ObjectNode bodyJson =
         Json.newObject().put("universeName", "importUniv").put("masterAddresses", MASTER_ADDRS);
diff --git a/managed/src/test/java/com/yugabyte/yw/controllers/InternalHAControllerTest.java b/managed/src/test/java/com/yugabyte/yw/controllers/InternalHAControllerTest.java
index 66193d4d11..899e616086 100644
--- a/managed/src/test/java/com/yugabyte/yw/controllers/InternalHAControllerTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/controllers/InternalHAControllerTest.java
@@ -12,12 +12,10 @@
 package com.yugabyte.yw.controllers;
 
 import static com.yugabyte.yw.common.AssertHelper.assertBadRequest;
-import static com.yugabyte.yw.common.AssertHelper.assertInternalServerError;
 import static com.yugabyte.yw.common.AssertHelper.assertOk;
 import static com.yugabyte.yw.common.AssertHelper.assertPlatformException;
 import static junit.framework.TestCase.assertEquals;
 import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertThrows;
 import static org.junit.Assert.assertTrue;
 import static play.libs.Files.singletonTemporaryFileCreator;
 import static play.mvc.Http.Status.BAD_REQUEST;
@@ -30,8 +28,7 @@ import com.google.common.collect.ImmutableMap;
 import com.yugabyte.yw.common.FakeApiHelper;
 import com.yugabyte.yw.common.FakeDBApplication;
 import com.yugabyte.yw.common.ModelFactory;
-import com.yugabyte.yw.common.PlatformInstanceClient;
-import com.yugabyte.yw.common.PlatformServiceException;
+import com.yugabyte.yw.common.ha.PlatformInstanceClient;
 import com.yugabyte.yw.common.ha.PlatformReplicationHelper;
 import com.yugabyte.yw.models.Customer;
 import com.yugabyte.yw.models.HighAvailabilityConfig;
diff --git a/managed/src/test/java/com/yugabyte/yw/controllers/PlatformInstanceControllerTest.java b/managed/src/test/java/com/yugabyte/yw/controllers/PlatformInstanceControllerTest.java
index 451c3dc896..d31c34c854 100644
--- a/managed/src/test/java/com/yugabyte/yw/controllers/PlatformInstanceControllerTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/controllers/PlatformInstanceControllerTest.java
@@ -39,6 +39,7 @@ import java.time.Duration;
 import java.util.List;
 import java.util.UUID;
 import java.util.function.Predicate;
+import org.apache.commons.lang3.StringUtils;
 import org.junit.Before;
 import org.junit.Test;
 import play.libs.Json;
@@ -223,7 +224,27 @@ public class PlatformInstanceControllerTest extends FakeDBApplication {
             () -> createPlatformInstance(configUUID, "http://abc.com::abc/", true, false));
     assertBadRequest(createResult, "");
     JsonNode node = Json.parse(contentAsString(createResult));
-    assertErrorNodeValue(node, "address", "Invalid URL provided");
+    assertErrorNodeValue(node, "address", "must be a valid URL");
+  }
+
+  @Test
+  public void testLongAddress() {
+    JsonNode haConfigJson = createHAConfig();
+    UUID configUUID = UUID.fromString(haConfigJson.get("uuid").asText());
+
+    // just within limits. DNS length 254 (which is <= 255)
+    String shortAddress = "http://" + StringUtils.repeat("abcdefghi.", 25) + ".com/";
+    Result createResult = createPlatformInstance(configUUID, shortAddress, true, true);
+    assertOk(createResult);
+
+    // Exceed dns length 264 (total address length is 272 > 263)
+    final String expectedError = "Maximum length is 263";
+    String longAddress = "http://" + StringUtils.repeat("abcdefghi.", 26) + ".com/";
+    createResult =
+        assertPlatformException(() -> createPlatformInstance(configUUID, longAddress, true, false));
+    assertBadRequest(createResult, "");
+    JsonNode node = Json.parse(contentAsString(createResult));
+    assertErrorNodeValue(node, "address", expectedError);
   }
 
   @Test
diff --git a/managed/src/test/java/com/yugabyte/yw/controllers/RuntimeConfControllerTest.java b/managed/src/test/java/com/yugabyte/yw/controllers/RuntimeConfControllerTest.java
index f683c34b9c..ef3101f175 100644
--- a/managed/src/test/java/com/yugabyte/yw/controllers/RuntimeConfControllerTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/controllers/RuntimeConfControllerTest.java
@@ -25,6 +25,8 @@ import static play.test.Helpers.route;
 
 import com.fasterxml.jackson.databind.JsonNode;
 import com.google.common.collect.ImmutableSet;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
 import com.yugabyte.yw.common.FakeDBApplication;
 import com.yugabyte.yw.common.ModelFactory;
 import com.yugabyte.yw.common.config.RuntimeConfigFactory;
@@ -60,6 +62,8 @@ public class RuntimeConfControllerTest extends FakeDBApplication {
   private static final String GET_CONFIG_INCL_INHERITED = GET_CONFIG + "?includeInherited=true";
   private static final String KEY = "/api/customers/%s/runtime_config/%s/key/%s";
   private static final String GC_CHECK_INTERVAL_KEY = "yb.taskGC.gc_check_interval";
+  private static final String EXT_SCRIPT_KEY = "yb.external_script";
+  private static final String EXT_SCRIPT_SCHEDULE_KEY = "yb.external_script.schedule";
 
   private Customer defaultCustomer;
   private Universe defaultUniverse;
@@ -88,7 +92,10 @@ public class RuntimeConfControllerTest extends FakeDBApplication {
     Result result = doRequestWithAuthToken("GET", LIST_KEYS, authToken);
     assertEquals(OK, result.status());
     ImmutableSet<String> expectedKeys =
-        ImmutableSet.of("yb.taskGC.gc_check_interval", "yb.taskGC.task_retention_duration");
+        ImmutableSet.of(
+            "yb.taskGC.gc_check_interval",
+            "yb.taskGC.task_retention_duration",
+            "yb.external_script");
     Set<String> actualKeys =
         ImmutableSet.copyOf(Json.parse(contentAsString(result)).elements())
             .stream()
@@ -129,20 +136,22 @@ public class RuntimeConfControllerTest extends FakeDBApplication {
   public void key() {
     assertEquals(
         NOT_FOUND,
-        assertPlatformException(() -> getGCInterval(defaultUniverse.universeUUID)).status());
+        assertPlatformException(() -> getKey(defaultUniverse.universeUUID, GC_CHECK_INTERVAL_KEY))
+            .status());
     String newInterval = "2 days";
-    Result result = setGCInterval(newInterval, defaultUniverse.universeUUID);
+    assertEquals(OK, setGCInterval(newInterval, defaultUniverse.universeUUID).status());
     RuntimeConfigFactory runtimeConfigFactory =
         app.injector().instanceOf(RuntimeConfigFactory.class);
     Duration duration =
         runtimeConfigFactory.forUniverse(defaultUniverse).getDuration(GC_CHECK_INTERVAL_KEY);
     assertEquals(24 * 60 * 2, duration.toMinutes());
-    assertEquals(OK, result.status());
-    assertEquals(newInterval, contentAsString(getGCInterval(defaultUniverse.universeUUID)));
-    assertEquals(OK, deleteGCInterval(defaultUniverse.universeUUID).status());
+    assertEquals(
+        newInterval, contentAsString(getKey(defaultUniverse.universeUUID, GC_CHECK_INTERVAL_KEY)));
+    assertEquals(OK, deleteKey(defaultUniverse.universeUUID, GC_CHECK_INTERVAL_KEY).status());
     assertEquals(
         NOT_FOUND,
-        assertPlatformException(() -> getGCInterval(defaultUniverse.universeUUID)).status());
+        assertPlatformException(() -> getKey(defaultUniverse.universeUUID, GC_CHECK_INTERVAL_KEY))
+            .status());
   }
 
   private Result setGCInterval(String interval, UUID scopeUUID) {
@@ -154,18 +163,79 @@ public class RuntimeConfControllerTest extends FakeDBApplication {
     return route(app, request);
   }
 
-  private Result getGCInterval(UUID scopeUUID) {
+  @Test
+  public void keyObj() {
+    assertEquals(
+        NOT_FOUND,
+        assertPlatformException(() -> getKey(defaultUniverse.universeUUID, GC_CHECK_INTERVAL_KEY))
+            .status());
+    String newInterval = "2 days";
+    String newRetention = "32 days";
+    assertEquals(
+        OK, setExtScriptObject(newInterval, newRetention, defaultUniverse.universeUUID).status());
+
+    // Now get key internal to the external script object directly (on server side)
+    RuntimeConfigFactory runtimeConfigFactory =
+        app.injector().instanceOf(RuntimeConfigFactory.class);
+    Duration duration =
+        runtimeConfigFactory.forUniverse(defaultUniverse).getDuration(EXT_SCRIPT_SCHEDULE_KEY);
+    assertEquals(24 * 60 * 2, duration.toMinutes());
+
+    // Fetching internal key through API should not work
+    assertEquals(
+        NOT_FOUND,
+        assertPlatformException(() -> getKey(defaultUniverse.universeUUID, EXT_SCRIPT_SCHEDULE_KEY))
+            .status());
+
+    // Fetch whole object through the API parse it and extract the internal key
+    assertEquals(
+        newInterval,
+        ConfigFactory.parseString(
+                contentAsString(getKey(defaultUniverse.universeUUID, EXT_SCRIPT_KEY)))
+            .getString("schedule"));
+
+    assertEquals(
+        "If you set an object deleting its internal key should result in key not found",
+        NOT_FOUND,
+        assertPlatformException(
+                () -> deleteKey(defaultUniverse.universeUUID, EXT_SCRIPT_SCHEDULE_KEY))
+            .status());
+
+    assertEquals(
+        "Delete of the object key should work",
+        OK,
+        deleteKey(defaultUniverse.universeUUID, EXT_SCRIPT_KEY).status());
+
+    assertEquals(
+        "The object was deleted. So expecting NOT_FOUND status",
+        NOT_FOUND,
+        assertPlatformException(() -> getKey(defaultUniverse.universeUUID, EXT_SCRIPT_KEY))
+            .status());
+  }
+
+  private Result setExtScriptObject(String schedule, String content, UUID scopeUUID) {
+    Http.RequestBuilder request =
+        fakeRequest("PUT", String.format(KEY, defaultCustomer.uuid, scopeUUID, EXT_SCRIPT_KEY))
+            .header("X-AUTH-TOKEN", authToken)
+            .bodyText(
+                String.format(
+                    "{"
+                        + "  schedule = %s\n"
+                        + "  params = %s\n"
+                        + "  content = \"the script\"\n"
+                        + "}",
+                    schedule, content));
+    return route(app, request);
+  }
+
+  private Result getKey(UUID scopeUUID, String key) {
     return doRequestWithAuthToken(
-        "GET",
-        String.format(KEY, defaultCustomer.uuid, scopeUUID, GC_CHECK_INTERVAL_KEY),
-        authToken);
+        "GET", String.format(KEY, defaultCustomer.uuid, scopeUUID, key), authToken);
   }
 
-  private Result deleteGCInterval(UUID universeUUID) {
+  private Result deleteKey(UUID universeUUID, String key) {
     return doRequestWithAuthToken(
-        "DELETE",
-        String.format(KEY, defaultCustomer.uuid, universeUUID, GC_CHECK_INTERVAL_KEY),
-        authToken);
+        "DELETE", String.format(KEY, defaultCustomer.uuid, universeUUID, key), authToken);
   }
 
   @Test
@@ -182,11 +252,38 @@ public class RuntimeConfControllerTest extends FakeDBApplication {
 
   @Test
   @Parameters(method = "scopeAndPresetParams")
-  public void getConfig_universe_inherited(ScopeType scopeType, String presetIntervalValue) {
+  public void getConfig_universe_inherited(
+      ScopeType scopeType, String presetIntervalValue, String expectedIntervalValue) {
     UUID scopeUUID = getScopeUUIDForType(scopeType);
     if (!presetIntervalValue.isEmpty()) {
       setGCInterval(presetIntervalValue, scopeUUID);
     }
+    final String actualValue =
+        internal_getConfig_universe_inherited(
+            scopeType, presetIntervalValue, scopeUUID, GC_CHECK_INTERVAL_KEY);
+    compareToExpectedValue(expectedIntervalValue, actualValue, "1 hour");
+  }
+
+  // Same test as above except the config is set as external Script object with retention  key
+  // embeded
+  @Test
+  @Parameters(method = "scopeAndPresetParamsObj")
+  public void getConfig_universe_inherited_obj(
+      ScopeType scopeType, String presetIntervalValue, String expectedIntervalValue) {
+    UUID scopeUUID = getScopeUUIDForType(scopeType);
+    String newRetention = "32 days";
+    if (!presetIntervalValue.isEmpty()) {
+      setExtScriptObject(presetIntervalValue, newRetention, scopeUUID);
+    }
+    final String actualObjValue =
+        internal_getConfig_universe_inherited(
+            scopeType, presetIntervalValue, scopeUUID, EXT_SCRIPT_KEY);
+    final Config configObj = ConfigFactory.parseString(actualObjValue);
+    compareToExpectedValue(expectedIntervalValue, configObj.getValue("schedule").render(), "\"\"");
+  }
+
+  private String internal_getConfig_universe_inherited(
+      ScopeType scopeType, String presetIntervalValue, UUID scopeUUID, String checkKey) {
     Result result =
         doRequestWithAuthToken(
             "GET",
@@ -200,8 +297,7 @@ public class RuntimeConfControllerTest extends FakeDBApplication {
     Map<String, String> configEntriesMap = new HashMap<>();
     while (configEntries.hasNext()) {
       JsonNode entry = configEntries.next();
-      if (GC_CHECK_INTERVAL_KEY.equals(entry.get("key").asText())
-          && !presetIntervalValue.isEmpty()) {
+      if (checkKey.equals(entry.get("key").asText()) && !presetIntervalValue.isEmpty()) {
         assertFalse(contentAsString(result), entry.get("inherited").asBoolean());
       } else {
         assertTrue(contentAsString(result), entry.get("inherited").asBoolean());
@@ -210,10 +306,15 @@ public class RuntimeConfControllerTest extends FakeDBApplication {
     }
     // two taskGC entries (There will be more in future hence >= 2)
     assertTrue(contentAsString(result), configEntriesMap.size() >= 2);
+    return configEntriesMap.get(checkKey);
+  }
+
+  private void compareToExpectedValue(
+      String presetIntervalValue, String value, String defaultValue) {
     if (presetIntervalValue.isEmpty()) {
-      assertEquals("1 hour", configEntriesMap.get(GC_CHECK_INTERVAL_KEY));
+      assertEquals(defaultValue, value);
     } else {
-      assertEquals(presetIntervalValue, configEntriesMap.get(GC_CHECK_INTERVAL_KEY));
+      assertEquals(presetIntervalValue, value);
     }
   }
 
@@ -233,14 +334,49 @@ public class RuntimeConfControllerTest extends FakeDBApplication {
 
   public Object[] scopeAndPresetParams() {
     return new Object[] {
-      new Object[] {ScopeType.GLOBAL, ""},
-      new Object[] {ScopeType.CUSTOMER, ""},
-      new Object[] {ScopeType.PROVIDER, ""},
-      new Object[] {ScopeType.UNIVERSE, ""},
-      new Object[] {ScopeType.GLOBAL, "33 days"},
-      new Object[] {ScopeType.CUSTOMER, "44 seconds"},
-      new Object[] {ScopeType.PROVIDER, "22 hours"},
-      new Object[] {ScopeType.UNIVERSE, "11 minutes"},
+      new Object[] {ScopeType.GLOBAL, "", ""},
+      new Object[] {ScopeType.CUSTOMER, "", ""},
+      new Object[] {ScopeType.PROVIDER, "", ""},
+      new Object[] {ScopeType.UNIVERSE, "", ""},
+      // We will return any strings as unquoted even if they were set as quoted
+      new Object[] {ScopeType.GLOBAL, "\"33 days\"", "33 days"},
+      new Object[] {ScopeType.CUSTOMER, "\"44 seconds\"", "44 seconds"},
+      new Object[] {ScopeType.PROVIDER, "\"22 hours\"", "22 hours"},
+      // Set without quotes should be allowed for string objects backward compatibility
+      // Even when set with quotes we will return string without redundant quotes.
+      // But we will do proper escaping for special characters
+      new Object[] {ScopeType.UNIVERSE, "11\"", "11\\\""},
     };
   }
+
+  public Object[] scopeAndPresetParamsObj() {
+    return new Object[] {
+      new Object[] {ScopeType.GLOBAL, "", ""},
+      new Object[] {ScopeType.CUSTOMER, "", ""},
+      new Object[] {ScopeType.PROVIDER, "", ""},
+      new Object[] {ScopeType.UNIVERSE, "", ""},
+      new Object[] {ScopeType.GLOBAL, "\"33 days\"", "\"33 days\""},
+      new Object[] {ScopeType.CUSTOMER, "\"44 seconds\"", "\"44 seconds\""},
+      new Object[] {ScopeType.PROVIDER, "\"22 hours\"", "\"22 hours\""},
+      // Set without escape quotes should not be allowed within a json object
+      new Object[] {ScopeType.UNIVERSE, "\"11\\\"\"", "\"11\\\"\""},
+    };
+  }
+
+  @Test
+  public void configResolution() {
+    RuntimeConfigFactory runtimeConfigFactory =
+        app.injector().instanceOf(RuntimeConfigFactory.class);
+    assertFalse(runtimeConfigFactory.forUniverse(defaultUniverse).getBoolean("yb.upgrade.vmImage"));
+    setCloudEnabled(defaultUniverse.universeUUID);
+    assertTrue(runtimeConfigFactory.forUniverse(defaultUniverse).getBoolean("yb.upgrade.vmImage"));
+  }
+
+  private void setCloudEnabled(UUID scopeUUID) {
+    Http.RequestBuilder request =
+        fakeRequest("PUT", String.format(KEY, defaultCustomer.uuid, scopeUUID, "yb.cloud.enabled"))
+            .header("X-AUTH-TOKEN", authToken)
+            .bodyText("true");
+    route(app, request);
+  }
 }
diff --git a/managed/src/test/java/com/yugabyte/yw/controllers/SessionControllerTest.java b/managed/src/test/java/com/yugabyte/yw/controllers/SessionControllerTest.java
index 04a927940d..3066b7fbf9 100644
--- a/managed/src/test/java/com/yugabyte/yw/controllers/SessionControllerTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/controllers/SessionControllerTest.java
@@ -9,9 +9,9 @@ import static com.yugabyte.yw.common.AssertHelper.assertConflict;
 import static com.yugabyte.yw.common.AssertHelper.assertForbidden;
 import static com.yugabyte.yw.common.AssertHelper.assertInternalServerError;
 import static com.yugabyte.yw.common.AssertHelper.assertOk;
+import static com.yugabyte.yw.common.AssertHelper.assertPlatformException;
 import static com.yugabyte.yw.common.AssertHelper.assertUnauthorized;
 import static com.yugabyte.yw.common.AssertHelper.assertValue;
-import static com.yugabyte.yw.common.AssertHelper.assertPlatformException;
 import static com.yugabyte.yw.common.FakeApiHelper.routeWithYWErrHandler;
 import static com.yugabyte.yw.common.TestHelper.testDatabase;
 import static com.yugabyte.yw.models.Users.Role;
@@ -57,7 +57,6 @@ import com.yugabyte.yw.models.Universe;
 import com.yugabyte.yw.models.Users;
 import com.yugabyte.yw.models.helpers.NodeDetails;
 import com.yugabyte.yw.scheduler.Scheduler;
-import java.util.Map;
 import java.util.UUID;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.TimeoutException;
diff --git a/managed/src/test/java/com/yugabyte/yw/models/AlertConfigurationTest.java b/managed/src/test/java/com/yugabyte/yw/models/AlertConfigurationTest.java
index 3e47d802c8..ee32b800df 100644
--- a/managed/src/test/java/com/yugabyte/yw/models/AlertConfigurationTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/models/AlertConfigurationTest.java
@@ -1,6 +1,48 @@
 // Copyright (c) YugaByte, Inc.
 package com.yugabyte.yw.models;
 
+import static com.yugabyte.yw.common.AlertTemplate.ALERT_CONFIG_WRITING_FAILED;
+import static com.yugabyte.yw.common.AlertTemplate.ALERT_NOTIFICATION_CHANNEL_ERROR;
+import static com.yugabyte.yw.common.AlertTemplate.ALERT_NOTIFICATION_ERROR;
+import static com.yugabyte.yw.common.AlertTemplate.ALERT_QUERY_FAILED;
+import static com.yugabyte.yw.common.AlertTemplate.BACKUP_FAILURE;
+import static com.yugabyte.yw.common.AlertTemplate.BACKUP_SCHEDULE_FAILURE;
+import static com.yugabyte.yw.common.AlertTemplate.CLIENT_TO_NODE_CA_CERT_EXPIRY;
+import static com.yugabyte.yw.common.AlertTemplate.CLIENT_TO_NODE_CERT_EXPIRY;
+import static com.yugabyte.yw.common.AlertTemplate.CLOCK_SKEW;
+import static com.yugabyte.yw.common.AlertTemplate.DB_COMPACTION_OVERLOAD;
+import static com.yugabyte.yw.common.AlertTemplate.DB_CORE_FILES;
+import static com.yugabyte.yw.common.AlertTemplate.DB_ERROR_LOGS;
+import static com.yugabyte.yw.common.AlertTemplate.DB_FATAL_LOGS;
+import static com.yugabyte.yw.common.AlertTemplate.DB_INSTANCE_DOWN;
+import static com.yugabyte.yw.common.AlertTemplate.DB_INSTANCE_RESTART;
+import static com.yugabyte.yw.common.AlertTemplate.DB_MEMORY_OVERLOAD;
+import static com.yugabyte.yw.common.AlertTemplate.DB_QUEUES_OVERFLOW;
+import static com.yugabyte.yw.common.AlertTemplate.DB_REDIS_CONNECTION;
+import static com.yugabyte.yw.common.AlertTemplate.DB_VERSION_MISMATCH;
+import static com.yugabyte.yw.common.AlertTemplate.DB_YCQL_CONNECTION;
+import static com.yugabyte.yw.common.AlertTemplate.DB_YSQL_CONNECTION;
+import static com.yugabyte.yw.common.AlertTemplate.HEALTH_CHECK_ERROR;
+import static com.yugabyte.yw.common.AlertTemplate.HEALTH_CHECK_NOTIFICATION_ERROR;
+import static com.yugabyte.yw.common.AlertTemplate.HIGH_NUM_YCQL_CONNECTIONS;
+import static com.yugabyte.yw.common.AlertTemplate.HIGH_NUM_YEDIS_CONNECTIONS;
+import static com.yugabyte.yw.common.AlertTemplate.INACTIVE_CRON_NODES;
+import static com.yugabyte.yw.common.AlertTemplate.MEMORY_CONSUMPTION;
+import static com.yugabyte.yw.common.AlertTemplate.NODE_CPU_USAGE;
+import static com.yugabyte.yw.common.AlertTemplate.NODE_DISK_USAGE;
+import static com.yugabyte.yw.common.AlertTemplate.NODE_DOWN;
+import static com.yugabyte.yw.common.AlertTemplate.NODE_FILE_DESCRIPTORS_USAGE;
+import static com.yugabyte.yw.common.AlertTemplate.NODE_OOM_KILLS;
+import static com.yugabyte.yw.common.AlertTemplate.NODE_RESTART;
+import static com.yugabyte.yw.common.AlertTemplate.NODE_TO_NODE_CA_CERT_EXPIRY;
+import static com.yugabyte.yw.common.AlertTemplate.NODE_TO_NODE_CERT_EXPIRY;
+import static com.yugabyte.yw.common.AlertTemplate.REPLICATION_LAG;
+import static com.yugabyte.yw.common.AlertTemplate.YCQL_OP_AVG_LATENCY;
+import static com.yugabyte.yw.common.AlertTemplate.YCQL_OP_P99_LATENCY;
+import static com.yugabyte.yw.common.AlertTemplate.YCQL_THROUGHPUT;
+import static com.yugabyte.yw.common.AlertTemplate.YSQL_OP_AVG_LATENCY;
+import static com.yugabyte.yw.common.AlertTemplate.YSQL_OP_P99_LATENCY;
+import static com.yugabyte.yw.common.AlertTemplate.YSQL_THROUGHPUT;
 import static com.yugabyte.yw.common.TestUtils.replaceFirstChar;
 import static com.yugabyte.yw.common.ThrownMatcher.thrown;
 import static org.hamcrest.CoreMatchers.containsString;
@@ -54,7 +96,6 @@ import java.util.function.Function;
 import java.util.stream.Collectors;
 import javax.persistence.PersistenceException;
 import junitparams.JUnitParamsRunner;
-import junitparams.Parameters;
 import org.apache.commons.lang3.StringUtils;
 import org.junit.Before;
 import org.junit.Rule;
@@ -67,6 +108,164 @@ import play.libs.Json;
 @RunWith(JUnitParamsRunner.class)
 public class AlertConfigurationTest extends FakeDBApplication {
 
+  private static final Map<AlertTemplate, String> TEST_ALERT_MESSAGE =
+      ImmutableMap.<AlertTemplate, String>builder()
+          .put(
+              REPLICATION_LAG,
+              "Average replication lag for universe 'Test Universe'"
+                  + " is above 180000 ms. Current value is 180001 ms")
+          .put(
+              CLOCK_SKEW,
+              "Max clock skew for universe 'Test Universe'"
+                  + " is above 500 ms. Current value is 501 ms")
+          .put(
+              MEMORY_CONSUMPTION,
+              "Average memory usage for universe 'Test Universe'"
+                  + " is above 90%. Current value is 91%")
+          .put(
+              HEALTH_CHECK_ERROR,
+              "Failed to perform health check for universe 'Test Universe'"
+                  + " - check YB Platform logs for details or contact YB support team")
+          .put(
+              HEALTH_CHECK_NOTIFICATION_ERROR,
+              "Failed to perform health check notification"
+                  + " for universe 'Test Universe' - check Health notification settings and"
+                  + " YB Platform logs for details or contact YB support team")
+          .put(
+              BACKUP_FAILURE,
+              "Last backup task for universe 'Test Universe' failed"
+                  + " - check backup task result for more details")
+          .put(
+              BACKUP_SCHEDULE_FAILURE,
+              "Last attempt to run scheduled backup for universe"
+                  + " 'Test Universe' failed due to other backup or universe operation is"
+                  + " in progress.")
+          .put(INACTIVE_CRON_NODES, "1 node(s) has inactive cronjob for universe 'Test Universe'.")
+          .put(
+              ALERT_QUERY_FAILED,
+              "Last alert query for customer 'Customer' failed"
+                  + " - check YB Platform logs for details or contact YB support team")
+          .put(
+              ALERT_CONFIG_WRITING_FAILED,
+              "Last alert rules sync for customer 'Customer' failed"
+                  + " - check YB Platform logs for details or contact YB support team")
+          .put(
+              ALERT_NOTIFICATION_ERROR,
+              "Last attempt to send alert notifications for customer "
+                  + "'Customer' failed - check YB Platform logs for details"
+                  + " or contact YB support team")
+          .put(
+              ALERT_NOTIFICATION_CHANNEL_ERROR,
+              "Last attempt to send alert notifications to"
+                  + " channel 'Some Channel' failed - try sending test alert to get more details")
+          .put(
+              NODE_DOWN,
+              "1 DB node(s) are down for more than 15 minutes" + " for universe 'Test Universe'.")
+          .put(
+              NODE_RESTART,
+              "Universe 'Test Universe' DB node is restarted 3 times" + " during last 30 minutes")
+          .put(
+              NODE_CPU_USAGE,
+              "Average node CPU usage for universe 'Test Universe' is above 95%" + " on 1 node(s).")
+          .put(
+              NODE_DISK_USAGE,
+              "Node disk usage for universe 'Test Universe'" + " is above 70% on 1 node(s).")
+          .put(
+              NODE_FILE_DESCRIPTORS_USAGE,
+              "Node file descriptors usage for universe"
+                  + " 'Test Universe' is above 70% on 1 node(s).")
+          .put(
+              NODE_OOM_KILLS,
+              "More than 3 OOM kills detected for universe 'Test Universe'" + " on 1 node(s).")
+          .put(
+              DB_VERSION_MISMATCH,
+              "Version mismatch detected for universe 'Test Universe'"
+                  + " for 1 Master/TServer instance(s).")
+          .put(
+              DB_INSTANCE_DOWN,
+              "1 DB Master/TServer instance(s) are down for more than"
+                  + " 15 minutes for universe 'Test Universe'.")
+          .put(
+              DB_INSTANCE_RESTART,
+              "Universe 'Test Universe' Master or TServer is restarted"
+                  + " 3 times during last 30 minutes")
+          .put(
+              DB_FATAL_LOGS,
+              "Fatal logs detected for universe 'Test Universe' on "
+                  + "1 Master/TServer instance(s).")
+          .put(
+              DB_ERROR_LOGS,
+              "Error logs detected for universe 'Test Universe' on "
+                  + "1 Master/TServer instance(s).")
+          .put(
+              DB_CORE_FILES,
+              "Core files detected for universe 'Test Universe' on " + "1 TServer instance(s).")
+          .put(
+              DB_YSQL_CONNECTION,
+              "YSQLSH connection failure detected for universe 'Test Universe'"
+                  + " on 1 TServer instance(s).")
+          .put(
+              DB_YCQL_CONNECTION,
+              "CQLSH connection failure detected for universe 'Test Universe'"
+                  + " on 1 TServer instance(s).")
+          .put(
+              DB_REDIS_CONNECTION,
+              "Redis connection failure detected for universe 'Test Universe'"
+                  + " on 1 TServer instance(s).")
+          .put(DB_MEMORY_OVERLOAD, "DB memory rejections detected for universe 'Test Universe'.")
+          .put(
+              DB_COMPACTION_OVERLOAD,
+              "DB compaction rejections detected for universe" + " 'Test Universe'.")
+          .put(DB_QUEUES_OVERFLOW, "DB queues overflow detected for universe 'Test Universe'.")
+          .put(
+              NODE_TO_NODE_CA_CERT_EXPIRY,
+              "Node to node CA certificate for universe"
+                  + " 'Test Universe' will expire in 29 days.")
+          .put(
+              NODE_TO_NODE_CERT_EXPIRY,
+              "Node to node certificate for universe 'Test Universe'" + " will expire in 29 days.")
+          .put(
+              CLIENT_TO_NODE_CA_CERT_EXPIRY,
+              "Client to node CA certificate for universe"
+                  + " 'Test Universe' will expire in 29 days.")
+          .put(
+              CLIENT_TO_NODE_CERT_EXPIRY,
+              "Client to node certificate for universe 'Test Universe'"
+                  + " will expire in 29 days.")
+          .put(
+              YSQL_OP_AVG_LATENCY,
+              "Average YSQL operations latency for universe 'Test Universe'"
+                  + " is above 10000 ms. Current value is 10001 ms")
+          .put(
+              YCQL_OP_AVG_LATENCY,
+              "Average YCQL operations latency for universe 'Test Universe'"
+                  + " is above 10000 ms. Current value is 10001 ms")
+          .put(
+              YSQL_OP_P99_LATENCY,
+              "YSQL P99 latency for universe 'Test Universe'"
+                  + " is above 60000 ms. Current value is 60001 ms")
+          .put(
+              YCQL_OP_P99_LATENCY,
+              "YCQL P99 latency for universe 'Test Universe'"
+                  + " is above 60000 ms. Current value is 60001 ms")
+          .put(
+              HIGH_NUM_YCQL_CONNECTIONS,
+              "Number of YCQL connections for universe"
+                  + " 'Test Universe' is above 1000. Current value is 1001")
+          .put(
+              HIGH_NUM_YEDIS_CONNECTIONS,
+              "Number of YEDIS connections for universe"
+                  + " 'Test Universe' is above 1000. Current value is 1001")
+          .put(
+              YSQL_THROUGHPUT,
+              "Maximum throughput for YSQL operations for universe"
+                  + " 'Test Universe' is above 100000. Current value is 100001")
+          .put(
+              YCQL_THROUGHPUT,
+              "Maximum throughput for YCQL operations for universe"
+                  + " 'Test Universe' is above 100000. Current value is 100001")
+          .build();
+
   @Rule public MockitoRule mockitoRule = MockitoJUnit.rule();
 
   private Customer customer;
@@ -237,7 +436,7 @@ public class AlertConfigurationTest extends FakeDBApplication {
 
     AlertConfiguration configuration2 =
         alertConfigurationService
-            .createConfigurationTemplate(customer, AlertTemplate.HEALTH_CHECK_ERROR)
+            .createConfigurationTemplate(customer, HEALTH_CHECK_ERROR)
             .getDefaultConfiguration();
     AlertConfigurationThreshold warningThreshold =
         new AlertConfigurationThreshold().setCondition(Condition.GREATER_THAN).setThreshold(1D);
@@ -252,7 +451,7 @@ public class AlertConfigurationTest extends FakeDBApplication {
 
     AlertConfiguration platformConfiguration =
         alertConfigurationService
-            .createConfigurationTemplate(customer, AlertTemplate.ALERT_QUERY_FAILED)
+            .createConfigurationTemplate(customer, ALERT_QUERY_FAILED)
             .getDefaultConfiguration();
     platformConfiguration.setDefaultDestination(false);
     alertConfigurationService.save(platformConfiguration);
@@ -262,8 +461,7 @@ public class AlertConfigurationTest extends FakeDBApplication {
     assertFind(filter, configuration, configuration2, platformConfiguration);
 
     // Name filter
-    filter =
-        AlertConfigurationFilter.builder().name(AlertTemplate.MEMORY_CONSUMPTION.getName()).build();
+    filter = AlertConfigurationFilter.builder().name(MEMORY_CONSUMPTION.getName()).build();
     assertFind(filter, configuration);
 
     // Active filter
@@ -302,7 +500,7 @@ public class AlertConfigurationTest extends FakeDBApplication {
     assertFind(filter, configuration, platformConfiguration);
 
     // Template filter
-    filter = AlertConfigurationFilter.builder().template(AlertTemplate.MEMORY_CONSUMPTION).build();
+    filter = AlertConfigurationFilter.builder().template(MEMORY_CONSUMPTION).build();
     assertFind(filter, configuration);
 
     // Severity filter
@@ -342,7 +540,7 @@ public class AlertConfigurationTest extends FakeDBApplication {
     Date now = new Date();
     AlertConfiguration configuration =
         alertConfigurationService
-            .createConfigurationTemplate(customer, AlertTemplate.MEMORY_CONSUMPTION)
+            .createConfigurationTemplate(customer, MEMORY_CONSUMPTION)
             .getDefaultConfiguration();
     configuration.setDestinationUUID(alertDestination.getUuid());
     configuration.setDefaultDestination(false);
@@ -353,7 +551,7 @@ public class AlertConfigurationTest extends FakeDBApplication {
 
     AlertConfiguration configuration2 =
         alertConfigurationService
-            .createConfigurationTemplate(customer, AlertTemplate.HEALTH_CHECK_ERROR)
+            .createConfigurationTemplate(customer, HEALTH_CHECK_ERROR)
             .getDefaultConfiguration();
     AlertConfigurationThreshold warningThreshold =
         new AlertConfigurationThreshold().setCondition(Condition.GREATER_THAN).setThreshold(1D);
@@ -371,7 +569,7 @@ public class AlertConfigurationTest extends FakeDBApplication {
 
     AlertConfiguration platformConfiguration =
         alertConfigurationService
-            .createConfigurationTemplate(customer, AlertTemplate.ALERT_QUERY_FAILED)
+            .createConfigurationTemplate(customer, ALERT_QUERY_FAILED)
             .getDefaultConfiguration();
     platformConfiguration.setDefaultDestination(false);
     platformConfiguration.setCreateTime(now);
@@ -622,7 +820,7 @@ public class AlertConfigurationTest extends FakeDBApplication {
         "errorJson: {\"template\":[\"may not be null\"]}");
 
     testValidationCreate(
-        configuration -> configuration.setTemplate(AlertTemplate.ALERT_CONFIG_WRITING_FAILED),
+        configuration -> configuration.setTemplate(ALERT_CONFIG_WRITING_FAILED),
         "errorJson: {\"\":[\"target type should be consistent with template\"]}");
 
     testValidationCreate(
@@ -710,103 +908,29 @@ public class AlertConfigurationTest extends FakeDBApplication {
   }
 
   @Test
-  // @formatter:off
-  @Parameters({
-    "REPLICATION_LAG|Average replication lag for universe 'Test Universe'"
-        + " is above 180000 ms. Current value is 180001 ms",
-    "CLOCK_SKEW|Max clock skew for universe 'Test Universe'"
-        + " is above 500 ms. Current value is 501 ms",
-    "MEMORY_CONSUMPTION|Average memory usage for universe 'Test Universe'"
-        + " is above 90%. Current value is 91%",
-    "HEALTH_CHECK_ERROR|Failed to perform health check for universe 'Test Universe': "
-        + "Some error occurred",
-    "HEALTH_CHECK_NOTIFICATION_ERROR|Failed to perform health check notification for universe "
-        + "'Test Universe': Some error occurred",
-    "BACKUP_FAILURE|Last backup task for universe 'Test Universe' failed: " + "Some error occurred",
-    "BACKUP_SCHEDULE_FAILURE|Last attempt to run scheduled backup for universe 'Test Universe'"
-        + " failed due to other backup or universe operation is in progress.",
-    "INACTIVE_CRON_NODES|1 node(s) has inactive cronjob for universe 'Test Universe'.",
-    "ALERT_QUERY_FAILED|Last alert query for customer 'Customer' failed: " + "Some error occurred",
-    "ALERT_CONFIG_WRITING_FAILED|Last alert rules sync for customer 'Customer' failed: "
-        + "Some error occurred",
-    "ALERT_NOTIFICATION_ERROR|Last attempt to send alert notifications for customer 'Customer'"
-        + " failed: Some error occurred",
-    "ALERT_NOTIFICATION_CHANNEL_ERROR|Last attempt to send alert notifications to channel "
-        + "'Some Channel' failed: Some error occurred",
-    "NODE_DOWN|1 DB node(s) are down for more than 15 minutes for universe 'Test Universe'.",
-    "NODE_RESTART|Universe 'Test Universe' DB node is restarted 3 times during last 30 minutes",
-    "NODE_CPU_USAGE|Average node CPU usage for universe 'Test Universe' is above 95%"
-        + " on 1 node(s).",
-    "NODE_DISK_USAGE|Node disk usage for universe 'Test Universe' is above 70% on 1 node(s).",
-    "NODE_FILE_DESCRIPTORS_USAGE|Node file descriptors usage for universe 'Test Universe'"
-        + " is above 70% on 1 node(s).",
-    "NODE_OOM_KILLS|More than 3 OOM kills detected for universe 'Test Universe'" + " on 1 node(s).",
-    "DB_VERSION_MISMATCH|Version mismatch detected for universe 'Test Universe'"
-        + " for 1 Master/TServer instance(s).",
-    "DB_INSTANCE_DOWN|1 DB Master/TServer instance(s) are down for more than 15 minutes "
-        + "for universe 'Test Universe'.",
-    "DB_INSTANCE_RESTART|Universe 'Test Universe' Master or TServer is restarted 3 times"
-        + " during last 30 minutes",
-    "DB_FATAL_LOGS|Fatal logs detected for universe 'Test Universe' on "
-        + "1 Master/TServer instance(s).",
-    "DB_ERROR_LOGS|Error logs detected for universe 'Test Universe' on "
-        + "1 Master/TServer instance(s).",
-    "DB_CORE_FILES|Core files detected for universe 'Test Universe' on " + "1 TServer instance(s).",
-    "DB_YSQL_CONNECTION|YSQLSH connection failure detected for universe 'Test Universe'"
-        + " on 1 TServer instance(s).",
-    "DB_YCQL_CONNECTION|CQLSH connection failure detected for universe 'Test Universe'"
-        + " on 1 TServer instance(s).",
-    "DB_REDIS_CONNECTION|Redis connection failure detected for universe 'Test Universe'"
-        + " on 1 TServer instance(s).",
-    "DB_MEMORY_OVERLOAD|DB memory rejections detected for universe 'Test Universe'.",
-    "DB_COMPACTION_OVERLOAD|DB compaction rejections detected for universe 'Test Universe'.",
-    "DB_QUEUES_OVERFLOW|DB queues overflow detected for universe 'Test Universe'.",
-    "NODE_TO_NODE_CA_CERT_EXPIRY|Node to node CA certificate for universe 'Test Universe'"
-        + " will expire in 29 days.",
-    "NODE_TO_NODE_CERT_EXPIRY|Node to node certificate for universe 'Test Universe'"
-        + " will expire in 29 days.",
-    "CLIENT_TO_NODE_CA_CERT_EXPIRY|Client to node CA certificate for universe 'Test Universe'"
-        + " will expire in 29 days.",
-    "CLIENT_TO_NODE_CERT_EXPIRY|Client to node certificate for universe 'Test Universe'"
-        + " will expire in 29 days.",
-    "YSQL_OP_AVG_LATENCY|Average YSQL operations latency for universe 'Test Universe'"
-        + " is above 10000 ms. Current value is 10001 ms",
-    "YCQL_OP_AVG_LATENCY|Average YCQL operations latency for universe 'Test Universe'"
-        + " is above 10000 ms. Current value is 10001 ms",
-    "YSQL_OP_P99_LATENCY|YSQL P99 latency for universe 'Test Universe'"
-        + " is above 60000 ms. Current value is 60001 ms",
-    "YCQL_OP_P99_LATENCY|YCQL P99 latency for universe 'Test Universe'"
-        + " is above 60000 ms. Current value is 60001 ms",
-    "HIGH_NUM_YCQL_CONNECTIONS|Number of YCQL connections for universe 'Test Universe'"
-        + " is above 1000. Current value is 1001",
-    "HIGH_NUM_YEDIS_CONNECTIONS|Number of YEDIS connections for universe 'Test Universe'"
-        + " is above 1000. Current value is 1001",
-    "YSQL_THROUGHPUT|Maximum throughput for YSQL operations for universe 'Test Universe'"
-        + " is above 100000. Current value is 100001",
-    "YCQL_THROUGHPUT|Maximum throughput for YCQL operations for universe 'Test Universe'"
-        + " is above 100000. Current value is 100001"
-  })
-  // @formatter:on
-  public void testTestAlertMessage(AlertTemplate template, String message) {
-    AlertConfiguration configuration =
-        alertConfigurationService
-            .createConfigurationTemplate(customer, template)
-            .getDefaultConfiguration();
-    if (configuration.getTargetType() == TargetType.UNIVERSE) {
-      configuration.setTarget(
-          new AlertConfigurationTarget()
-              .setAll(false)
-              .setUuids(ImmutableSet.of(universe.getUniverseUUID())));
-    }
-    alertConfigurationService.save(configuration);
-    Alert testAlert = alertConfigurationService.createTestAlert(configuration);
-    assertThat(testAlert.getMessage(), equalTo("[TEST ALERT!!!] " + message));
+  public void testTestAlertMessage() {
+    TEST_ALERT_MESSAGE.forEach(
+        (template, message) -> {
+          AlertConfiguration configuration =
+              alertConfigurationService
+                  .createConfigurationTemplate(customer, template)
+                  .getDefaultConfiguration();
+          if (configuration.getTargetType() == TargetType.UNIVERSE) {
+            configuration.setTarget(
+                new AlertConfigurationTarget()
+                    .setAll(false)
+                    .setUuids(ImmutableSet.of(universe.getUniverseUUID())));
+          }
+          alertConfigurationService.save(configuration);
+          Alert testAlert = alertConfigurationService.createTestAlert(configuration);
+          assertThat(testAlert.getMessage(), equalTo("[TEST ALERT!!!] " + message));
+        });
   }
 
   private AlertConfiguration createTestConfiguration() {
     AlertConfiguration configuration =
         alertConfigurationService
-            .createConfigurationTemplate(customer, AlertTemplate.MEMORY_CONSUMPTION)
+            .createConfigurationTemplate(customer, MEMORY_CONSUMPTION)
             .getDefaultConfiguration();
     configuration.setDestinationUUID(alertDestination.getUuid());
     configuration.setDefaultDestination(false);
@@ -814,7 +938,7 @@ public class AlertConfigurationTest extends FakeDBApplication {
   }
 
   private void assertTestConfiguration(AlertConfiguration configuration) {
-    AlertTemplate template = AlertTemplate.MEMORY_CONSUMPTION;
+    AlertTemplate template = MEMORY_CONSUMPTION;
     assertThat(configuration.getCustomerUUID(), equalTo(customer.uuid));
     assertThat(configuration.getName(), equalTo(template.getName()));
     assertThat(configuration.getDescription(), equalTo(template.getDescription()));
diff --git a/managed/src/test/java/com/yugabyte/yw/models/UniverseTest.java b/managed/src/test/java/com/yugabyte/yw/models/UniverseTest.java
index 2918f38c7b..7f132e58bf 100644
--- a/managed/src/test/java/com/yugabyte/yw/models/UniverseTest.java
+++ b/managed/src/test/java/com/yugabyte/yw/models/UniverseTest.java
@@ -708,7 +708,8 @@ public class UniverseTest extends FakeDBApplication {
         if (nodeState == NodeDetails.NodeState.ToBeAdded) {
           assertEquals(ImmutableSet.of(NodeActionType.DELETE), allowedActions);
         } else if (nodeState == NodeDetails.NodeState.Adding) {
-          assertEquals(ImmutableSet.of(NodeActionType.DELETE), allowedActions);
+          assertEquals(
+              ImmutableSet.of(NodeActionType.DELETE, NodeActionType.RELEASE), allowedActions);
         } else if (nodeState == NodeDetails.NodeState.InstanceCreated) {
           assertEquals(ImmutableSet.of(NodeActionType.DELETE), allowedActions);
         } else if (nodeState == NodeDetails.NodeState.ServerSetup) {
diff --git a/managed/src/test/resources/alert/test_alert_definition.yml b/managed/src/test/resources/alert/test_alert_definition.yml
index deaa7fe835..5ae3d8d2e9 100644
--- a/managed/src/test/resources/alert/test_alert_definition.yml
+++ b/managed/src/test/resources/alert/test_alert_definition.yml
@@ -3,7 +3,7 @@ groups:
     rules:
       - alert: alertConfiguration
         expr: query > 1
-        for: 15s
+        for: 0s
         labels:
           configuration_uuid: <configuration_uuid>
           configuration_type: UNIVERSE
@@ -13,9 +13,9 @@ groups:
           severity: SEVERE
           threshold: 1
           universe_uuid: <universe_uuid>
-          universe_name: Test Universe
           source_uuid: <universe_uuid>
-          source_name: Test Universe
           source_type: universe
+          universe_name: Test Universe
+          source_name: Test Universe
         annotations:
           summary: "Average memory usage for universe 'Test Universe' is above 1%. Current value is {{ $value | printf \"%.0f\" }}%"
diff --git a/managed/ui/src/components/alerts/AlertList/AlertsTable.jsx b/managed/ui/src/components/alerts/AlertList/AlertsTable.jsx
index bb6c4c6054..2fc5621543 100644
--- a/managed/ui/src/components/alerts/AlertList/AlertsTable.jsx
+++ b/managed/ui/src/components/alerts/AlertList/AlertsTable.jsx
@@ -12,7 +12,7 @@ import { isAvailable } from '../../../utils/LayoutUtils';
 import './AlertsTable.scss';
 import { toast } from 'react-toastify';
 import { Label } from 'react-bootstrap';
-import {timeFormatter} from "../../../utils/TableFormatters";
+import { timeFormatter } from '../../../utils/TableFormatters';
 
 const DEFAULT_SORT_COLUMN = 'createTime';
 const DEFAULT_SORT_DIRECTION = 'DESC';
@@ -47,7 +47,7 @@ export default function AlertsTable({ filters, customer }) {
         const resp = await api.getAlert(variables.uuid);
 
         queryClient.invalidateQueries('alerts');
-        toast.success('Acknowledged!.');
+        toast.success('Acknowledged!');
         if (alertDetails !== null) {
           setAlertDetails(resp);
         }
@@ -68,7 +68,7 @@ export default function AlertsTable({ filters, customer }) {
 
   const setSortOptions = (sortType, sortDirection) => {
     resetPage();
-    let sortColumn = sortType === 'labels' ? 'sourceName' : sortType;
+    const sortColumn = sortType === 'labels' ? 'sourceName' : sortType;
     setSortType(sortColumn);
     setSortDirection(sortDirection.toUpperCase());
   };
@@ -167,7 +167,7 @@ export default function AlertsTable({ filters, customer }) {
               >
                 Status
               </TableHeaderColumn>
-              { isAvailable(customer.currentCustomer.data.features, 'alert.list.actions') && (
+              {isAvailable(customer.currentCustomer.data.features, 'alert.list.actions') && (
                 <TableHeaderColumn
                   dataField="message"
                   columnClassName="no-border name-column"
diff --git a/managed/ui/src/components/tables/Replication/Replication.js b/managed/ui/src/components/tables/Replication/Replication.js
index 5f24341784..b913789174 100644
--- a/managed/ui/src/components/tables/Replication/Replication.js
+++ b/managed/ui/src/components/tables/Replication/Replication.js
@@ -11,6 +11,8 @@ import { YBLoading } from '../../common/indicators';
 import { YBResourceCount } from '../../common/descriptors';
 import { MetricsPanel } from '../../metrics';
 import { ReplicationAlertModalBtn } from './ReplicationAlertModalBtn';
+import { Dropdown, MenuItem } from 'react-bootstrap';
+import { CustomDatePicker } from '../../metrics/CustomDatePicker/CustomDatePicker';
 import './Replication.scss';
 
 const GRAPH_TYPE = 'replication';
@@ -18,12 +20,27 @@ const METRIC_NAME = 'tserver_async_replication_lag_micros';
 const MILLI_IN_MIN = 60000.0;
 const MILLI_IN_SEC = 1000.0;
 
+const filterTypes = [
+  { label: 'Last 1 hr', type: 'hours', value: '1' },
+  { label: 'Last 6 hrs', type: 'hours', value: '6' },
+  { label: 'Last 12 hrs', type: 'hours', value: '12' },
+  { label: 'Last 24 hrs', type: 'hours', value: '24' },
+  { label: 'Last 7 days', type: 'days', value: '7' },
+  { type: 'divider' },
+  { label: 'Custom', type: 'custom' }
+];
+
 export default class Replication extends Component {
   constructor(props) {
     super(props);
     this.state = {
       graphWidth: props.hideHeader ? window.innerWidth - 300 : 840,
-      intervalId: null
+      intervalId: null,
+      filterLabel: filterTypes[0].label,
+      filterType: filterTypes[0].type,
+      filterValue: filterTypes[0].value,
+      startMoment: moment().subtract(filterTypes[0].value, filterTypes[0].type),
+      endMoment: moment()
     };
   }
 
@@ -32,7 +49,7 @@ export default class Replication extends Component {
   };
 
   componentDidMount() {
-    const {  sourceUniverseUUID } = this.props;
+    const { sourceUniverseUUID } = this.props;
     if (sourceUniverseUUID) {
       this.props.fetchCurrentUniverse(sourceUniverseUUID).then(() => {
         this.queryMetrics();
@@ -57,19 +74,57 @@ export default class Replication extends Component {
       universe: { currentUniverse },
       replicationUUID
     } = this.props;
+    const { startMoment, endMoment, filterValue, filterType, filterLabel } = this.state;
     const universeDetails = getPromiseState(currentUniverse).isSuccess()
       ? currentUniverse.data.universeDetails
       : 'all';
+
     const params = {
       metrics: [METRIC_NAME],
-      start: moment().utc().subtract('1', 'hour').format('X'),
-      end: moment().utc().format('X'),
+      start: startMoment.format('X'),
+      end: endMoment.format('X'),
       nodePrefix: universeDetails.nodePrefix,
       xClusterConfigUuid: replicationUUID
     };
+
+    if (filterLabel !== 'Custom') {
+      params['start'] = moment().subtract(filterValue, filterType).format('X')
+      params['end'] = moment().format('X')
+    }
+
     this.props.queryMetrics(params, GRAPH_TYPE);
   };
 
+  handleFilterChange = (eventKey, event) => {
+
+    const filterInfo = filterTypes[eventKey]
+    const self = this;
+
+    let stateToUpdate = {
+      filterLabel: filterInfo.label,
+      filterType: filterInfo.type,
+      filterValue: filterInfo.value
+    }
+    if (event.target.getAttribute('data-filter-type') !== 'custom') {
+      stateToUpdate = {
+        ...stateToUpdate,
+        endMoment: moment(),
+        startMoment: moment().subtract(filterInfo.value, filterInfo.type)
+      }
+      this.setState(stateToUpdate, () => self.queryMetrics())
+    }
+    else {
+      this.setState(stateToUpdate)
+    }
+  }
+
+  handleStartDateChange = (dateStr) => {
+    this.setState({ startMoment: moment(dateStr) });
+  };
+
+  handleEndDateChange = (dateStr) => {
+    this.setState({ endMoment: moment(dateStr) });
+  };
   render() {
     const {
       universe: { currentUniverse },
@@ -98,7 +153,7 @@ export default class Replication extends Component {
       const replicationNodeMetrics = metrics[GRAPH_TYPE][METRIC_NAME].data.filter(
         (x) => x.name === committedLagName
       )
-      .sort((a, b) => b.x.length - a.x.length);
+        .sort((a, b) => b.x.length - a.x.length);
 
       if (replicationNodeMetrics.length) {
         // Get max-value and avg-value metric array
@@ -168,9 +223,43 @@ export default class Replication extends Component {
       );
     }
 
+    let datePicker = null;
+    if (this.state.filterLabel === 'Custom') {
+      datePicker = (
+        <CustomDatePicker
+          startMoment={this.state.startMoment}
+          endMoment={this.state.endMoment}
+          setStartMoment={this.handleStartDateChange}
+          setEndMoment={this.handleEndDateChange}
+          handleTimeframeChange={this.queryMetrics}
+        />
+      );
+    }
+
+    const self = this;
+
+    const menuItems = filterTypes.map((filter, idx) => {
+      const key = 'graph-filter-' + idx;
+      if (filter.type === 'divider') {
+        return <MenuItem divider key={key} />;
+      }
+
+      return (
+        <MenuItem
+          onSelect={self.handleFilterChange}
+          data-filter-type={filter.type}
+          key={key}
+          eventKey={idx}
+          active={filter.label === self.state.filterLabel}
+        >
+          {filter.label}
+        </MenuItem>
+      );
+    });
+
     // TODO: Make graph resizeable
     return (
-      <div>
+      <div id="replication-tab-panel">
         <YBPanelItem
           header={
             <div className="replication-header">
@@ -188,8 +277,21 @@ export default class Replication extends Component {
               {!hideHeader && infoBlock}
               {!hideHeader && <div className="replication-content-stats">{recentStatBlock}</div>}
               {!showMetrics && <div className="no-data">No data to display.</div>}
+              {
+                showMetrics && <div className={`time-range-option ${!hideHeader ? 'old-view' : ''}`}>
+                  {datePicker}
+                  <Dropdown id="graphFilterDropdown" className="graph-filter-dropdown" pullRight>
+                    <Dropdown.Toggle>
+                      <i className="fa fa-clock-o"></i>&nbsp;
+                      {this.state.filterLabel}
+                    </Dropdown.Toggle>
+                    <Dropdown.Menu>{menuItems}</Dropdown.Menu>
+                  </Dropdown>
+                </div>
+              }
+
               {showMetrics && metrics[GRAPH_TYPE] && (
-                <div className="graph-container">
+                <div className={`graph-container ${!hideHeader ? 'old-view' : ''}`}>
                   <MetricsPanel
                     currentUser={currentUser}
                     metricKey={METRIC_NAME}
diff --git a/managed/ui/src/components/tables/Replication/Replication.scss b/managed/ui/src/components/tables/Replication/Replication.scss
index 1864a04533..12bea28bf4 100644
--- a/managed/ui/src/components/tables/Replication/Replication.scss
+++ b/managed/ui/src/components/tables/Replication/Replication.scss
@@ -2,15 +2,24 @@
 
 @import '../../../_style/colors.scss';
 
-#universe-tab-panel {
+#replication-tab-panel {
   .replication-header {
     display: flex;
     align-items: center;
     justify-content: space-between;
+    padding-left: 25px;
+  }
+
+  .header {
+    margin-bottom: 0 !important;
+  }
+
+  body {
+    margin: 0 !important;
   }
 
   .replication-content {
-    > .replication-content-stats {
+    >.replication-content-stats {
       display: flex;
       margin-left: 7.5px;
     }
@@ -32,7 +41,10 @@
 
     .graph-container {
       width: fit-content;
-      margin-top: 35px;
+      margin-top: 25px;
+      &.old-view{
+        margin-top: 0 !important;
+      }
     }
 
     .info {
@@ -45,7 +57,7 @@
       width: 300px;
       margin: 10px 0 20px 7.5px;
 
-      > div {
+      >div {
         display: inline-block;
 
         h4 {
@@ -60,7 +72,7 @@
       border: 1px solid #c0dfc6;
       background: #f3f9f4;
 
-      > .icon {
+      >.icon {
         color: $YB_SUCCESS_COLOR;
         margin-right: 10px;
         display: inline-block;
@@ -73,9 +85,24 @@
       margin: 30px;
     }
 
-    .metrics-panel > .plot-container {
+    .metrics-panel>.plot-container {
       padding: 30px;
       background: #fff;
     }
+
+    .time-range-option {
+      display: flex;
+      justify-content: flex-end;
+      padding-right: 5px;
+      &.old-view{
+        justify-content: flex-start !important;
+        margin-top: 15px;
+        margin-left: 8px;
+      }
+    }
+
+    .graph-filter-dropdown {
+      margin-right: -15px;
+    }
   }
-}
+}
\ No newline at end of file
diff --git a/managed/ui/src/components/universes/UniverseDetail/UniverseDetail.js b/managed/ui/src/components/universes/UniverseDetail/UniverseDetail.js
index 8e50551c40..d7f7e7add3 100644
--- a/managed/ui/src/components/universes/UniverseDetail/UniverseDetail.js
+++ b/managed/ui/src/components/universes/UniverseDetail/UniverseDetail.js
@@ -576,7 +576,6 @@ class UniverseDetail extends Component {
                           </YBLabelWithIcon>
                         </YBMenuItem>
                       )}
-
                       {!isReadOnlyUniverse &&
                         !universePaused &&
                         isNotHidden(
diff --git a/managed/ui/src/components/universes/UniverseStatus/UniverseStatus.js b/managed/ui/src/components/universes/UniverseStatus/UniverseStatus.js
index 7d1fee2013..845b4050ad 100644
--- a/managed/ui/src/components/universes/UniverseStatus/UniverseStatus.js
+++ b/managed/ui/src/components/universes/UniverseStatus/UniverseStatus.js
@@ -1,39 +1,31 @@
 // Copyright (c) YugaByte, Inc.
 
 import React, { Component } from 'react';
-import './UniverseStatus.scss';
 import { ProgressBar } from 'react-bootstrap';
-import { isNonEmptyObject, isNonEmptyArray, isDefinedNotNull } from '../../../utils/ObjectUtils';
+import { isNonEmptyObject } from '../../../utils/ObjectUtils';
+
 import { YBLoadingCircleIcon } from '../../common/indicators';
+import {
+  getUniversePendingTask,
+  getUniverseStatus,
+  hasPendingTasksForUniverse,
+  universeState
+} from '../helpers/universeHelpers';
 
-export default class UniverseStatus extends Component {
-  hasPendingTasksForUniverse = (customerTaskList) => {
-    const {
-      currentUniverse: { universeUUID }
-    } = this.props;
-    return isNonEmptyArray(customerTaskList)
-      ? customerTaskList.some(function (taskItem) {
-        return (
-          taskItem.targetUUID === universeUUID &&
-          (taskItem.status === 'Running' || taskItem.status === 'Initializing') &&
-          Number(taskItem.percentComplete) !== 100 &&
-          taskItem.target.toLowerCase() !== 'backup'
-        );
-      })
-      : false;
-  };
+import './UniverseStatus.scss';
 
+export default class UniverseStatus extends Component {
   componentDidUpdate(prevProps) {
     const {
-      currentUniverse: { universeDetails },
+      currentUniverse: { universeUUID, universeDetails },
       tasks: { customerTaskList },
       refreshUniverseData
     } = this.props;
 
     if (
-      universeDetails.updateInProgress &&
-      !this.hasPendingTasksForUniverse(customerTaskList) &&
-      this.hasPendingTasksForUniverse(prevProps.tasks.customerTaskList)
+      (universeDetails.updateInProgress || universeDetails.backupInProgress) &&
+      !hasPendingTasksForUniverse(universeUUID, customerTaskList) &&
+      hasPendingTasksForUniverse(universeUUID, prevProps.tasks.customerTaskList)
     ) {
       refreshUniverseData();
     }
@@ -41,109 +33,78 @@ export default class UniverseStatus extends Component {
 
   render() {
     const {
-      currentUniverse: { universeDetails, universeUUID },
+      currentUniverse,
       showLabelText,
       tasks: { customerTaskList }
     } = this.props;
-    const updateInProgress = universeDetails.updateInProgress;
-    const updateSucceeded = universeDetails.updateSucceeded;
-    const universePaused = universeDetails.universePaused;
-    const errorString = universeDetails.errorString;
-    let statusClassName = 'unknown';
-    let statusText = '';
-    const universePendingTask = isNonEmptyArray(customerTaskList)
-      ? customerTaskList.find(function (taskItem) {
-        return (
-          taskItem.targetUUID === universeUUID &&
-          (taskItem.status === 'Running' || taskItem.status === 'Initializing') &&
-          Number(taskItem.percentComplete) !== 100 &&
-          taskItem.target.toLowerCase() !== 'backup'
-        );
-      })
-      : null;
 
-    if (showLabelText) {
-      statusText = 'Loading';
-    }
+    const universeStatus = getUniverseStatus(currentUniverse);
+    const universePendingTask = getUniversePendingTask(
+      currentUniverse.universeUUID,
+      customerTaskList
+    );
+
     let statusDisplay = (
       <div className="status-pending-display-container">
         <YBLoadingCircleIcon size="small" />
-        <span className="status-pending-name">{statusText}</span>
+        <span className="status-pending-name">{showLabelText && universeStatus.state.text}</span>
       </div>
     );
-    if (!isDefinedNotNull(universePendingTask) && updateSucceeded && !universePaused) {
-      statusClassName = 'good';
-      if (showLabelText) {
-        statusText = 'Ready';
-      }
-
+    if (universeStatus.state === universeState.GOOD) {
       statusDisplay = (
         <div>
           <i className="fa fa-check-circle" />
-          {statusText && <span>{statusText}</span>}
+          {showLabelText && universeStatus.state.text && <span>{universeStatus.state.text}</span>}
         </div>
       );
-    } else if (!isDefinedNotNull(universePendingTask) && updateSucceeded && universePaused) {
-      statusClassName = 'paused';
-      if (showLabelText) {
-        statusText = 'Paused';
-      }
-
+    } else if (universeStatus.state === universeState.PAUSED) {
       statusDisplay = (
         <div>
           <i className="fa fa-pause-circle-o" />
-          {statusText && <span>{statusText}</span>}
+          {showLabelText && universeStatus.state.text && <span>{universeStatus.state.text}</span>}
         </div>
       );
-    } else {
-      if (updateInProgress && isNonEmptyObject(universePendingTask)) {
-        if (showLabelText) {
-          statusDisplay = (
-            <div className="status-pending">
-              <div className="status-pending-display-container">
-                <YBLoadingCircleIcon size="small" />
-                <span className="status-pending-name">
-                  Pending&hellip;
-                  {universePendingTask.percentComplete}%
-                </span>
-                <span className="status-pending-progress-container">
-                  <ProgressBar
-                    className={'pending-action-progress'}
-                    now={universePendingTask.percentComplete}
-                  />
-                </span>
-              </div>
-            </div>
-          );
-        } else {
-          statusDisplay = (
-            <div className={'yb-orange'}>
+    } else if (
+      universeStatus.state === universeState.PENDING &&
+      isNonEmptyObject(universePendingTask)
+    ) {
+      if (showLabelText) {
+        statusDisplay = (
+          <div className="status-pending">
+            <div className="status-pending-display-container">
               <YBLoadingCircleIcon size="small" />
+              <span className="status-pending-name">
+                {universeStatus.state.text}&hellip;
+                {universePendingTask.percentComplete}%
+              </span>
+              <span className="status-pending-progress-container">
+                <ProgressBar
+                  className={'pending-action-progress'}
+                  now={universePendingTask.percentComplete}
+                />
+              </span>
             </div>
-          );
-        }
-        statusClassName = 'pending';
-      } else if (!updateInProgress && !updateSucceeded) {
-        if (errorString === 'Preflight checks failed.') {
-          statusClassName = 'warning';
-          if (showLabelText) {
-            statusText = 'Ready';
-          }
-        } else {
-          statusClassName = 'bad';
-          if (showLabelText) {
-            statusText = 'Error';
-          }
-        }
+          </div>
+        );
+      } else {
         statusDisplay = (
-          <div>
-            <i className="fa fa-warning" />
-            {statusText && <span>{statusText}</span>}
+          <div className={'yb-orange'}>
+            <YBLoadingCircleIcon size="small" />
           </div>
         );
       }
+    } else if (
+      universeStatus.state === universeState.BAD ||
+      universeStatus.state === universeState.WARNING
+    ) {
+      statusDisplay = (
+        <div>
+          <i className="fa fa-warning" />
+          {showLabelText && universeStatus.state.text && <span>{universeStatus.state.text}</span>}
+        </div>
+      );
     }
 
-    return <div className={'universe-status ' + statusClassName}>{statusDisplay}</div>;
+    return <div className={'universe-status ' + universeStatus.state.className}>{statusDisplay}</div>;
   }
 }
diff --git a/managed/ui/src/components/universes/UniverseView/UniverseView.js b/managed/ui/src/components/universes/UniverseView/UniverseView.js
index ae96166ac6..022375c6fa 100644
--- a/managed/ui/src/components/universes/UniverseView/UniverseView.js
+++ b/managed/ui/src/components/universes/UniverseView/UniverseView.js
@@ -14,7 +14,7 @@ import {
   getUniverseStatus,
   getUniverseStatusIcon,
   hasPendingTasksForUniverse,
-  status
+  universeState
 } from '../helpers/universeHelpers';
 import {
   YBCost,
@@ -115,10 +115,10 @@ const tableDataValueToKey = {
 
 const toggleTooltip = (view) => <Tooltip id="tooltip">Switch to {view} view.</Tooltip>;
 
-const { UNKNOWN, WARNING, ...filterStatuses } = status;
+const { UNKNOWN, WARNING, ...filterStatuses } = universeState;
 const filterStatusesArr = Object.values(filterStatuses).map((status) => ({
-  value: status.statusText,
-  label: status.statusText
+  value: status.text,
+  label: status.text
 }));
 
 const TABLE_MIN_PAGE_SIZE = 10;
@@ -490,8 +490,8 @@ export const UniverseView = (props) => {
             universe,
             universePendingTasks[universe.universeUUID]
           );
-          universe.status = universeStatus.status;
-          universe.statusText = universeStatus.status.statusText;
+          universe.status = universeStatus.state;
+          universe.statusText = universeStatus.state.text;
           return universe;
         })
       : [];
diff --git a/managed/ui/src/components/universes/helpers/universeHelpers.js b/managed/ui/src/components/universes/helpers/universeHelpers.js
index bc3c30170c..2f22ca82b6 100644
--- a/managed/ui/src/components/universes/helpers/universeHelpers.js
+++ b/managed/ui/src/components/universes/helpers/universeHelpers.js
@@ -1,80 +1,88 @@
 import React from 'react';
-import { isNonEmptyArray, isNonEmptyObject, isDefinedNotNull } from '../../../utils/ObjectUtils';
+import { isNonEmptyArray } from '../../../utils/ObjectUtils';
 import { YBLoadingCircleIcon } from '../../common/indicators';
 
 import _ from 'lodash';
 
-export const status = {
+/**
+ * A mapping from universe state to display text and className.
+ */
+export const universeState = {
   GOOD: {
-    statusText: 'Ready',
-    statusClassName: 'good'
+    text: 'Ready',
+    className: 'good'
   },
   PAUSED: {
-    statusText: 'Paused',
-    statusClassName: 'paused'
+    text: 'Paused',
+    className: 'paused'
   },
   PENDING: {
-    statusText: 'Pending',
-    statusClassName: 'pending'
+    text: 'Pending',
+    className: 'pending'
   },
   WARNING: {
-    statusText: 'Ready',
-    statusClassName: 'warning'
+    text: 'Ready',
+    className: 'warning'
   },
   BAD: {
-    statusText: 'Error',
-    statusClassName: 'bad'
+    text: 'Error',
+    className: 'bad'
   },
   UNKNOWN: {
-    statusText: 'Loading',
-    statusClassName: 'unknown'
+    text: 'Loading',
+    className: 'unknown'
   }
 };
 
-export const getUniverseStatus = (universe, universePendingTask) => {
+/**
+ * Returns a universe status object with:
+ *  - state - A universe state from the universe state mapping {@link universeState}
+ *  - error - The error string from the current universe
+ */
+export const getUniverseStatus = (universe) => {
   const {
     updateInProgress,
+    backupInProgress,
     updateSucceeded,
     universePaused,
     errorString
   } = universe.universeDetails;
 
-  // statusText stores the status for display
-  // warning stores extra information for internal use (ex. warning icons for certain errors)
-  if (!isDefinedNotNull(universePendingTask) && updateSucceeded && !universePaused) {
-    return { status: status.GOOD, warning: '' };
+  const taskInProgress = updateInProgress || backupInProgress;
+  if (!taskInProgress && updateSucceeded && !universePaused) {
+    return { state: universeState.GOOD, error: errorString };
   }
-  if (!isDefinedNotNull(universePendingTask) && updateSucceeded && universePaused) {
-    return { status: status.PAUSED, warning: '' };
+  if (!taskInProgress && updateSucceeded && universePaused) {
+    return { state: universeState.PAUSED, error: errorString };
   }
-  if (updateInProgress && isNonEmptyObject(universePendingTask)) {
-    return { status: status.PENDING, warning: '' };
+  if (taskInProgress) {
+    return { state: universeState.PENDING, error: errorString };
   }
-  if (!updateInProgress && !updateSucceeded) {
+  if (!taskInProgress && !updateSucceeded) {
     return errorString === 'Preflight checks failed.'
-      ? { status: status.WARNING, warning: errorString }
-      : { status: status.BAD, warning: errorString };
+      ? { state: universeState.WARNING, error: errorString }
+      : { state: universeState.BAD, error: errorString };
   }
-  return { status: status.UNKNOWN, warning: '' };
+  return { state: universeState.UNKNOWN, error: errorString };
 };
 
 export const getUniverseStatusIcon = (curStatus) => {
-  if (_.isEqual(curStatus, status.GOOD)) {
+  if (_.isEqual(curStatus, universeState.GOOD)) {
     return <i className="fa fa-check-circle" />;
   }
-  if (_.isEqual(curStatus, status.PAUSED)) {
+  if (_.isEqual(curStatus, universeState.PAUSED)) {
     return <i className="fa fa-pause-circle-o" />;
   }
-  if (_.isEqual(curStatus, status.PENDING)) {
+  if (_.isEqual(curStatus, universeState.PENDING)) {
     return <i className="fa fa-hourglass-half" />;
   }
-  if (_.isEqual(curStatus, status.WARNING)) {
+  if (_.isEqual(curStatus, universeState.WARNING)) {
     return <i className="fa fa-warning" />;
   }
-  if (_.isEqual(curStatus, status.BAD)) {
+  if (_.isEqual(curStatus, universeState.BAD)) {
     return <i className="fa fa-warning" />;
   }
-  if (_.isEqual(curStatus, status.UNKNOWN)) {
+  if (_.isEqual(curStatus, universeState.UNKNOWN)) {
     return <YBLoadingCircleIcon size="small" />;
   }
 };
@@ -83,8 +91,7 @@ export const isPendingUniverseTask = (universeUUID, taskItem) => {
   return (
     taskItem.targetUUID === universeUUID &&
     (taskItem.status === 'Running' || taskItem.status === 'Initializing') &&
-    Number(taskItem.percentComplete) !== 100 &&
-    taskItem.target.toLowerCase() !== 'backup'
+    Number(taskItem.percentComplete) !== 100
   );
 };
 
diff --git a/managed/ui/src/components/users/Users/UsersListContainer.js b/managed/ui/src/components/users/Users/UsersListContainer.js
index 1638c78f10..053b3b6c4a 100644
--- a/managed/ui/src/components/users/Users/UsersListContainer.js
+++ b/managed/ui/src/components/users/Users/UsersListContainer.js
@@ -83,7 +83,8 @@ function mapStateToProps(state) {
   return {
     customer: state.customer.currentCustomer,
     users: state.customer.users,
-    modal: state.modal
+    modal: state.modal,
+    passwordValidationInfo: state.customer.passwordValidationInfo
   };
 }
 
diff --git a/managed/ui/src/components/xcluster/ConfigureReplicationModal.tsx b/managed/ui/src/components/xcluster/ConfigureReplicationModal.tsx
index 2239419107..cfacf1a4e9 100644
--- a/managed/ui/src/components/xcluster/ConfigureReplicationModal.tsx
+++ b/managed/ui/src/components/xcluster/ConfigureReplicationModal.tsx
@@ -1,11 +1,14 @@
 import { Field } from 'formik';
 import React, { useState } from 'react';
 import { Col, Row } from 'react-bootstrap';
-import { YBFormInput, YBFormSelect, YBInputField } from '../common/forms/fields';
+import { useMutation, useQuery, useQueryClient } from 'react-query';
+import * as Yup from 'yup';
+import { toast } from 'react-toastify';
+import _ from 'lodash';
 
+import { YBFormInput, YBFormSelect, YBInputField } from '../common/forms/fields';
 import { YBModalForm } from '../common/forms';
 import { BootstrapTable, TableHeaderColumn } from 'react-bootstrap-table';
-import { IReplicationTable } from './IClusterReplication';
 import { YBLoading } from '../common/indicators';
 import {
   createXClusterReplication,
@@ -13,11 +16,12 @@ import {
   fetchTaskUntilItCompletes,
   fetchUniversesList
 } from '../../actions/xClusterReplication';
-
-import { useMutation, useQuery, useQueryClient } from 'react-query';
-import * as Yup from 'yup';
-import { toast } from 'react-toastify';
 import { YSQL_TABLE_TYPE } from './ReplicationUtils';
+import { isNonEmptyArray } from '../../utils/ObjectUtils';
+import { getUniverseStatus, universeState } from '../universes/helpers/universeHelpers';
+
+import { IReplicationTable } from './IClusterReplication';
+
 import './ConfigureReplicationModal.scss';
 
 const validationSchema = Yup.object().shape({
@@ -44,6 +48,15 @@ interface Props {
   currentUniverseUUID: string;
 }
 
+// TODO: Would be nice to start adding interfaces for all the nested fields in Universe
+//       as when need them and move all the universe types to a common folder
+interface Universe {
+  name: string;
+  universeUUID: string;
+  universeDetails: {};
+  state?: typeof universeState[keyof typeof universeState];
+}
+
 export function ConfigureReplicationModal({ onHide, visible, currentUniverseUUID }: Props) {
   const [currentStep, setCurrentStep] = useState(0);
   const queryClient = useQueryClient();
@@ -62,15 +75,16 @@ export function ConfigureReplicationModal({ onHide, visible, currentUniverseUUID
         fetchTaskUntilItCompletes(resp.data.taskUUID, (err: boolean) => {
           if (err) {
             toast.error(
-              <span className='alertMsg'>
+              <span className="alertMsg">
                 <i className="fa fa-exclamation-circle" />
                 <span>Replication creation failed.</span>
-                <a href={`/tasks/${resp.data.taskUUID}`} rel="noopener noreferrer" target="_blank">View Details</a>
+                <a href={`/tasks/${resp.data.taskUUID}`} rel="noopener noreferrer" target="_blank">
+                  View Details
+                </a>
               </span>
             );
           }
           queryClient.invalidateQueries('universe');
-
         });
       },
       onError: (err: any) => {
@@ -79,8 +93,9 @@ export function ConfigureReplicationModal({ onHide, visible, currentUniverseUUID
     }
   );
 
-  const { data: universeList, isLoading: isUniverseListLoading } = useQuery(['universeList'], () =>
-    fetchUniversesList().then((res) => res.data)
+  const { data: universeList, isLoading: isUniverseListLoading } = useQuery<Universe[]>(
+    ['universeList'],
+    () => fetchUniversesList().then((res) => res.data)
   );
 
   const { data: tables, isLoading: isTablesLoading } = useQuery(
@@ -97,6 +112,13 @@ export function ConfigureReplicationModal({ onHide, visible, currentUniverseUUID
     targetUniverseUUID: undefined,
     tables: []
   };
+  // Add universe status field
+  if (_.isObject(universeList) && isNonEmptyArray(universeList)) {
+    universeList.forEach((universe) => {
+      const universeStatus = getUniverseStatus(universe);
+      universe.state = universeStatus.state;
+    });
+  }
 
   return (
     <YBModalForm
@@ -145,7 +167,7 @@ export function TargetUniverseForm({
   currentUniverseUUID
 }: {
   isEdit: boolean;
-  universeList: { name: string; universeUUID: string }[];
+  universeList: Universe[];
   initialValues: {};
   currentUniverseUUID: string;
 }) {
@@ -174,7 +196,11 @@ export function TargetUniverseForm({
             name="targetUniverseUUID"
             component={YBFormSelect}
             options={universeList
-              .filter((universe) => universe.universeUUID !== currentUniverseUUID)
+              .filter(
+                (universe) =>
+                  universe.universeUUID !== currentUniverseUUID &&
+                  universe.state === universeState.GOOD
+              )
               .map((universe) => {
                 return {
                   label: universe.name,
diff --git a/managed/ui/src/components/xcluster/Replication.tsx b/managed/ui/src/components/xcluster/Replication.tsx
index 7a2d446eff..043a4d4686 100644
--- a/managed/ui/src/components/xcluster/Replication.tsx
+++ b/managed/ui/src/components/xcluster/Replication.tsx
@@ -18,7 +18,7 @@ export default function Replication({ currentUniverseUUID }: { currentUniverseUU
 
   const showConfigureMaxLagTimeModal = () => {
     dispatch(openDialog('configureMaxLagTimeModal'));
-  }
+  };
 
   const hideModal = () => dispatch(closeDialog());
 
@@ -46,7 +46,13 @@ export default function Replication({ currentUniverseUUID }: { currentUniverseUU
             <Row className="cluster_support_text">
               <i className="fa fa-exclamation-circle"></i> For replicating a source universe with
               existing data, please{' '}
-              <a href="https://docs.yugabyte.com/latest/deploy/multi-dc/async-replication/#bootstrapping-a-sink-cluster" target='_blank' rel="noopener noreferrer">contact support</a>
+              <a
+                href="https://docs.yugabyte.com/latest/deploy/multi-dc/async-replication/#bootstrapping-a-sink-cluster"
+                target="_blank"
+                rel="noopener noreferrer"
+              >
+                contact support
+              </a>
             </Row>
           </Row>
         </Col>
diff --git a/managed/ui/src/components/xcluster/ReplicationUtils.tsx b/managed/ui/src/components/xcluster/ReplicationUtils.tsx
index f6ae2ccf67..3610b22fd8 100644
--- a/managed/ui/src/components/xcluster/ReplicationUtils.tsx
+++ b/managed/ui/src/components/xcluster/ReplicationUtils.tsx
@@ -21,7 +21,7 @@ export const getReplicationStatus = (status = IReplicationStatus.INIT) => {
           <i className="fa fa-spinner fa-spin" />
           Updating
         </span>
-      )
+      );
     case IReplicationStatus.RUNNING:
       return (
         <span className="replication-status-text success">
@@ -101,21 +101,20 @@ export const GetCurrentLag = ({
   const nodePrefix = universeInfo?.data?.universeDetails.nodePrefix;
 
   const { data: metricsData, isFetching } = useQuery(
-    [replicationUUID, nodePrefix, 'metric'],
+    ['xcluster-metric', replicationUUID, nodePrefix, 'metric'],
     () => queryLagMetricsForUniverse(nodePrefix, replicationUUID),
     {
-      enabled: !currentUniverseLoading,
-      refetchInterval: 20 * 1000
+      enabled: !currentUniverseLoading
     }
-    );
-    const configurationFilter = {
-      name: ALERT_NAME,
-      targetUuid: sourceUniverseUUID
-    };
-    const { data: configuredThreshold, isLoading: threshholdLoading } = useQuery(
-      ['getConfiguredThreshold', configurationFilter],
-      () => getAlertConfigurations(configurationFilter)
-    );
+  );
+  const configurationFilter = {
+    name: ALERT_NAME,
+    targetUuid: sourceUniverseUUID
+  };
+  const { data: configuredThreshold, isLoading: threshholdLoading } = useQuery(
+    ['getConfiguredThreshold', configurationFilter],
+    () => getAlertConfigurations(configurationFilter)
+  );
 
   if (isFetching || currentUniverseLoading || threshholdLoading) {
     return <i className="fa fa-spinner fa-spin yb-spinner"></i>;
@@ -129,8 +128,25 @@ export const GetCurrentLag = ({
   }
   let maxAcceptableLag = configuredThreshold?.[0]?.thresholds?.SEVERE.threshold || 0;
 
-  const latestLag = metricsData.data.tserver_async_replication_lag_micros.data[0]?.y.pop();
-  return <span className={`replication-lag-value ${maxAcceptableLag < latestLag ? 'above-threshold' : 'below-threshold'}`}>{latestLag || '-'}</span>;
+  const metricAliases = metricsData.data.tserver_async_replication_lag_micros.layout.yaxis.alias;
+  const committedLagName = metricAliases['async_replication_committed_lag_micros'];
+
+  const latestLag = Math.max(
+    ...metricsData.data.tserver_async_replication_lag_micros.data
+      .filter((d: any) => d.name === committedLagName)
+      .map((a: any) => {
+        return a.y.slice(-1);
+      })
+  );
+  return (
+    <span
+      className={`replication-lag-value ${
+        maxAcceptableLag < latestLag ? 'above-threshold' : 'below-threshold'
+      }`}
+    >
+      {latestLag ?? '-'}
+    </span>
+  );
 };
 
 export const GetCurrentLagForTable = ({
@@ -147,11 +163,10 @@ export const GetCurrentLagForTable = ({
   sourceUniverseUUID: string;
 }) => {
   const { data: metricsData, isFetching } = useQuery(
-    [replicationUUID, nodePrefix, tableName, 'metric'],
+    ['xcluster-metric', replicationUUID, nodePrefix, tableName, 'metric'],
     () => queryLagMetricsForTable(tableName, nodePrefix),
     {
-      enabled,
-      refetchInterval: 20 * 1000
+      enabled
     }
   );
 
@@ -176,8 +191,26 @@ export const GetCurrentLagForTable = ({
   }
 
   let maxAcceptableLag = configuredThreshold?.[0]?.thresholds?.SEVERE.threshold || 0;
-  const latestLag = metricsData.data.tserver_async_replication_lag_micros.data[1]?.y[0];
-  return <span className={`replication-lag-value ${maxAcceptableLag < latestLag ? 'above-threshold' : 'below-threshold'}`}>{latestLag || '-'}</span>;
+
+  const metricAliases = metricsData.data.tserver_async_replication_lag_micros.layout.yaxis.alias;
+  const committedLagName = metricAliases['async_replication_committed_lag_micros'];
+
+  const latestLag = Math.max(
+    ...metricsData.data.tserver_async_replication_lag_micros.data
+      .filter((d: any) => d.name === committedLagName)
+      .map((a: any) => {
+        return a.y.slice(-1);
+      })
+  );
+  return (
+    <span
+      className={`replication-lag-value ${
+        maxAcceptableLag < latestLag ? 'above-threshold' : 'below-threshold'
+      }`}
+    >
+      {latestLag ?? '-'}
+    </span>
+  );
 };
 
 export const getMasterNodeAddress = (nodeDetailsSet: Array<any>) => {
@@ -188,11 +221,13 @@ export const getMasterNodeAddress = (nodeDetailsSet: Array<any>) => {
   return '';
 };
 
-export const convertToLocalTime = (time:string, timezone:string) => {
-  return (timezone ?  (moment.utc(time) as any).tz(timezone): moment.utc(time).local()).format('YYYY-MM-DD H:mm:ss')
-}
+export const convertToLocalTime = (time: string, timezone: string) => {
+  return (timezone ? (moment.utc(time) as any).tz(timezone) : moment.utc(time).local()).format(
+    'YYYY-MM-DD H:mm:ss'
+  );
+};
 
-export const formatBytes = function (sizeInBytes:any) {
+export const formatBytes = function (sizeInBytes: any) {
   if (Number.isInteger(sizeInBytes)) {
     const bytes = sizeInBytes;
     const sizes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB'];
@@ -206,4 +241,4 @@ export const formatBytes = function (sizeInBytes:any) {
   } else {
     return '-';
   }
-};
\ No newline at end of file
+};
diff --git a/managed/ui/src/components/xcluster/details/LagGraph.tsx b/managed/ui/src/components/xcluster/details/LagGraph.tsx
index da1a787e19..34340a2e4c 100644
--- a/managed/ui/src/components/xcluster/details/LagGraph.tsx
+++ b/managed/ui/src/components/xcluster/details/LagGraph.tsx
@@ -26,11 +26,10 @@ export const LagGraph: FC<LagGraphProps> = ({ replicationUUID, sourceUniverseUUI
   const nodePrefix = universeInfo?.data?.universeDetails.nodePrefix;
 
   const { data: metrics } = useQuery(
-    [replicationUUID, nodePrefix, 'metric', 'lagGraph'],
+    ['xcluster-metric', replicationUUID, nodePrefix, 'metric', 'lagGraph'],
     () => queryLagMetricsForUniverse(nodePrefix, replicationUUID),
     {
-      enabled: !currentUniverseLoading,
-      refetchInterval: 10 * 1000
+      enabled: !currentUniverseLoading
     }
   );
 
diff --git a/managed/ui/src/components/xcluster/details/ReplicationDetails.tsx b/managed/ui/src/components/xcluster/details/ReplicationDetails.tsx
index cdd772f198..684747822d 100644
--- a/managed/ui/src/components/xcluster/details/ReplicationDetails.tsx
+++ b/managed/ui/src/components/xcluster/details/ReplicationDetails.tsx
@@ -4,7 +4,7 @@ import { useMutation, useQuery, useQueryClient } from 'react-query';
 import { useDispatch, useSelector } from 'react-redux';
 import { Link } from 'react-router';
 import { toast } from 'react-toastify';
-import { useMount } from 'react-use';
+import { useInterval, useMount } from 'react-use';
 import { IReplicationStatus } from '..';
 import { closeDialog, openDialog } from '../../../actions/modal';
 import { fetchUniverseList } from '../../../actions/universe';
@@ -56,6 +56,11 @@ export function ReplicationDetails({ params }: Props) {
     setUniversesList((await resp.payload).data);
   });
 
+  //refresh metrics for every 20 seconds
+  useInterval(() => {
+    queryClient.invalidateQueries('xcluster-metric');
+  }, 20_000);
+
   const switchReplicationStatus = useMutation(
     (replication: IReplication) => {
       return changeXClusterStatus(
diff --git a/src/postgres/src/backend/access/transam/xact.c b/src/postgres/src/backend/access/transam/xact.c
index df21b68b9e..0ee557a397 100644
--- a/src/postgres/src/backend/access/transam/xact.c
+++ b/src/postgres/src/backend/access/transam/xact.c
@@ -1906,6 +1906,28 @@ YBStartTransaction(TransactionState s)
 	}
 }
 
+/*
+ * The isolation level in Postgres code (i.e., XactIsoLevel) maps to a certain
+ * isolation level as seen by pggate. This function returns the mapped isolation
+ * level that pggate layer is supposed to see.
+ */
+int YBGetEffectivePggateIsolationLevel() {
+	int mapped_pg_isolation_level = XactIsoLevel;
+
+	// For the txn manager, logic for XACT_READ_UNCOMMITTED is same as
+	// XACT_READ_COMMITTED.
+	if (mapped_pg_isolation_level == XACT_READ_UNCOMMITTED)
+		mapped_pg_isolation_level = XACT_READ_COMMITTED;
+
+	// If READ COMMITTED mode is not on, XACT_READ_COMMITTED maps to
+	// XACT_REPEATABLE_READ.
+	if ((mapped_pg_isolation_level == XACT_READ_COMMITTED) &&
+			!IsYBReadCommitted())
+		mapped_pg_isolation_level = XACT_REPEATABLE_READ;
+
+	return mapped_pg_isolation_level;
+}
+
 void
 YBInitializeTransaction(void)
 {
@@ -1913,15 +1935,8 @@ YBInitializeTransaction(void)
 	{
 		HandleYBStatus(YBCPgBeginTransaction());
 
-		int	pg_isolation_level = XactIsoLevel;
-
-		if (pg_isolation_level == XACT_READ_UNCOMMITTED)
-			pg_isolation_level = XACT_READ_COMMITTED;
-
-		if ((pg_isolation_level == XACT_READ_COMMITTED) && !IsYBReadCommitted())
-			pg_isolation_level = XACT_REPEATABLE_READ;
-
-		HandleYBStatus(YBCPgSetTransactionIsolationLevel(pg_isolation_level));
+		HandleYBStatus(
+			YBCPgSetTransactionIsolationLevel(YBGetEffectivePggateIsolationLevel()));
 		HandleYBStatus(YBCPgEnableFollowerReads(YBReadFromFollowersEnabled(), YBFollowerReadStalenessMs()));
 		HandleYBStatus(YBCPgSetTransactionReadOnly(XactReadOnly));
 		HandleYBStatus(YBCPgSetTransactionDeferrable(XactDeferrable));
diff --git a/src/postgres/src/backend/access/yb_access/yb_lsm.c b/src/postgres/src/backend/access/yb_access/yb_lsm.c
index 521151790f..f62d5ba30c 100644
--- a/src/postgres/src/backend/access/yb_access/yb_lsm.c
+++ b/src/postgres/src/backend/access/yb_access/yb_lsm.c
@@ -23,6 +23,7 @@
  */
 
 #include "postgres.h"
+#include "pgstat.h"
 
 #include "miscadmin.h"
 #include "access/nbtree.h"
@@ -125,16 +126,17 @@ bindColumn(YBCPgStatement stmt,
  * Utility method to set binds for index write statement.
  */
 static void
-doBindsForWrite(YBCPgStatement stmt,
-				void *indexstate,
-				Relation index,
-				Datum *values,
-				bool *isnull,
-				int natts,
-				Datum ybbasectid,
-				bool ybctid_as_value)
+doBindsForIdxWrite(YBCPgStatement stmt,
+				   void *indexstate,
+				   Relation index,
+				   Datum *values,
+				   bool *isnull,
+				   int n_bound_atts,
+				   Datum ybbasectid,
+				   bool ybctid_as_value)
 {
-	TupleDesc tupdesc = RelationGetDescr(index);
+	TupleDesc tupdesc		= RelationGetDescr(index);
+	int		  indnkeyatts	= IndexRelationGetNumberOfKeyAttributes(index);
 
 	if (ybbasectid == 0)
 	{
@@ -144,7 +146,7 @@ doBindsForWrite(YBCPgStatement stmt,
 	}
 
 	bool has_null_attr = false;
-	for (AttrNumber attnum = 1; attnum <= natts; ++attnum)
+	for (AttrNumber attnum = 1; attnum <= n_bound_atts; ++attnum)
 	{
 		Oid			type_id = GetTypeId(attnum, tupdesc);
 		Oid			collation_id = YBEncodingCollation(stmt, attnum,
@@ -152,8 +154,14 @@ doBindsForWrite(YBCPgStatement stmt,
 		Datum		value   = values[attnum - 1];
 		bool		is_null = isnull[attnum - 1];
 
-		has_null_attr = has_null_attr || is_null;
 		bindColumn(stmt, attnum, type_id, collation_id, value, is_null);
+
+		/*
+		 * If any of the indexed columns is null, we need to take case of
+		 * SQL null != null semantics.
+		 * For details, see comment on kYBUniqueIdxKeySuffix.
+		 */
+		has_null_attr = has_null_attr || (is_null && attnum <= indnkeyatts);
 	}
 
 	const bool unique_index = index->rd_index->indisunique;
@@ -197,7 +205,7 @@ ybcinbuildCallback(Relation index, HeapTuple heapTuple, Datum *values, bool *isn
 							  isnull,
 							  heapTuple->t_ybctid,
 							  buildstate->backfill_write_time,
-							  doBindsForWrite,
+							  doBindsForIdxWrite,
 							  NULL /* indexstate */);
 
 	buildstate->index_tuples += 1;
@@ -293,7 +301,7 @@ ybcininsert(Relation index, Datum *values, bool *isnull, Datum ybctid, Relation
 										   isnull,
 										   ybctid,
 										   NULL /* backfill_write_time */,
-										   doBindsForWrite,
+										   doBindsForIdxWrite,
 										   NULL /* indexstate */);
 			}
 			YB_FOR_EACH_DB_END;
@@ -304,7 +312,7 @@ ybcininsert(Relation index, Datum *values, bool *isnull, Datum ybctid, Relation
 								  isnull,
 								  ybctid,
 								  NULL /* backfill_write_time */,
-								  doBindsForWrite,
+								  doBindsForIdxWrite,
 								  NULL /* indexstate */);
 	}
 
@@ -317,7 +325,7 @@ ybcindelete(Relation index, Datum *values, bool *isnull, Datum ybctid, Relation
 {
 	if (!index->rd_index->indisprimary)
 		YBCExecuteDeleteIndex(index, values, isnull, ybctid,
-							  doBindsForWrite, NULL /* indexstate */);
+							  doBindsForIdxWrite, NULL /* indexstate */);
 }
 
 IndexBulkDeleteResult *
@@ -390,7 +398,7 @@ ybcinbeginscan(Relation rel, int nkeys, int norderbys)
 	/* get the scan */
 	scan = RelationGetIndexScan(rel, nkeys, norderbys);
 	scan->opaque = NULL;
-
+	pgstat_count_index_scan(rel);
 	return scan;
 }
 
diff --git a/src/postgres/src/backend/access/ybgin/ybginwrite.c b/src/postgres/src/backend/access/ybgin/ybginwrite.c
index 260ad52c1d..42964df837 100644
--- a/src/postgres/src/backend/access/ybgin/ybginwrite.c
+++ b/src/postgres/src/backend/access/ybgin/ybginwrite.c
@@ -106,14 +106,14 @@ bindColumn(YBCPgStatement stmt,
  * Utility method to set binds for index write statement.
  */
 static void
-doBindsForWrite(YBCPgStatement stmt,
-				void *indexstate,
-				Relation index,
-				Datum *values,
-				bool *isnull,
-				int natts,
-				Datum ybbasectid,
-				bool ybctid_as_value)
+doBindsForIdxWrite(YBCPgStatement stmt,
+				   void *indexstate,
+				   Relation index,
+				   Datum *values,
+				   bool *isnull,
+				   int n_bound_atts,
+				   Datum ybbasectid,
+				   bool ybctid_as_value)
 {
 	GinState *ginstate = (GinState *) indexstate;
 	TupleDesc tupdesc = RelationGetDescr(index);
@@ -123,7 +123,7 @@ doBindsForWrite(YBCPgStatement stmt,
 				(errcode(ERRCODE_INTERNAL_ERROR),
 				 errmsg("missing base table ybctid in index write request")));
 
-	for (AttrNumber attnum = 1; attnum <= natts; ++attnum)
+	for (AttrNumber attnum = 1; attnum <= n_bound_atts; ++attnum)
 	{
 		Oid			type_id = GetTypeId(attnum, tupdesc);
 		Oid			collation_id = YBEncodingCollation(stmt, attnum,
@@ -183,12 +183,12 @@ ybginTupleWrite(GinState *ginstate, OffsetNumber attnum,
 		if (isinsert)
 			YBCExecuteInsertIndex(index, &entries[i], &isnull, ybctid,
 								  backfilltime /* backfill_write_time */,
-								  doBindsForWrite, (void *) ginstate);
+								  doBindsForIdxWrite, (void *) ginstate);
 		else
 		{
 			Assert(!backfilltime);
 			YBCExecuteDeleteIndex(index, &entries[i], &isnull, ybctid,
-								  doBindsForWrite, (void *) ginstate);
+								  doBindsForIdxWrite, (void *) ginstate);
 		}
 	}
 
diff --git a/src/postgres/src/backend/bootstrap/ybcbootstrap.c b/src/postgres/src/backend/bootstrap/ybcbootstrap.c
index 1a442a5e12..257af25f21 100644
--- a/src/postgres/src/backend/bootstrap/ybcbootstrap.c
+++ b/src/postgres/src/backend/bootstrap/ybcbootstrap.c
@@ -142,6 +142,7 @@ void YBCCreateSysCatalogTable(const char *table_name,
 	                                   InvalidOid /* tablegroup_oid */,
 	                                   InvalidOid /* colocation_id */,
 	                                   InvalidOid /* tablespace_oid */,
+	                                   false /* is_matview */,
 	                                   InvalidOid /* matviewPgTableId */,
 	                                   &yb_stmt));
 
diff --git a/src/postgres/src/backend/catalog/aclchk.c b/src/postgres/src/backend/catalog/aclchk.c
index d0ba7f4d28..cdf23a1d22 100644
--- a/src/postgres/src/backend/catalog/aclchk.c
+++ b/src/postgres/src/backend/catalog/aclchk.c
@@ -4349,7 +4349,7 @@ pg_namespace_aclmask(Oid nsp_oid, Oid roleid,
 	Oid			ownerId;
 
 	/* Superusers bypass all permission checking. */
-	if (superuser_arg(roleid))
+	if (superuser_arg(roleid) || (IsYbExtensionUser(roleid) && creating_extension))
 		return mask;
 
 	/*
diff --git a/src/postgres/src/backend/catalog/indexing.c b/src/postgres/src/backend/catalog/indexing.c
index 8e3eaf1b94..58c4bc3f0a 100644
--- a/src/postgres/src/backend/catalog/indexing.c
+++ b/src/postgres/src/backend/catalog/indexing.c
@@ -286,6 +286,9 @@ YBCatalogTupleInsert(Relation heapRel, HeapTuple tup, bool yb_shared_insert)
 
 	if (IsYugaByteEnabled())
 	{
+		/* Keep ybctid consistent across all databases. */
+		Datum ybctid = 0;
+
 		if (yb_shared_insert)
 		{
 			if (!IsYsqlUpgrade)
@@ -300,14 +303,19 @@ YBCatalogTupleInsert(Relation heapRel, HeapTuple tup, bool yb_shared_insert)
 				 */
 				if (dboid == YBCGetDatabaseOid(heapRel))
 					continue; /* Will be done after the loop. */
-				YBCExecuteInsertForDb(dboid, heapRel, RelationGetDescr(heapRel), tup);
+				YBCExecuteInsertForDb(dboid,
+									  heapRel,
+									  RelationGetDescr(heapRel),
+									  tup,
+									  &ybctid);
 			}
 			YB_FOR_EACH_DB_END;
 		}
 		oid = YBCExecuteInsertForDb(YBCGetDatabaseOid(heapRel),
 									heapRel,
 									RelationGetDescr(heapRel),
-									tup);
+									tup,
+									&ybctid);
 		/* Update the local cache automatically */
 		YBSetSysCacheTuple(heapRel, tup);
 	}
@@ -345,6 +353,9 @@ CatalogTupleInsertWithInfo(Relation heapRel, HeapTuple tup,
 
 	if (IsYugaByteEnabled())
 	{
+		/* Keep ybctid consistent across all databases. */
+		Datum ybctid = 0;
+
 		if (yb_shared_insert)
 		{
 			if (!IsYsqlUpgrade)
@@ -359,14 +370,19 @@ CatalogTupleInsertWithInfo(Relation heapRel, HeapTuple tup,
 				 */
 				if (dboid == YBCGetDatabaseOid(heapRel))
 					continue; /* Will be done after the loop. */
-				YBCExecuteInsertForDb(dboid, heapRel, RelationGetDescr(heapRel), tup);
+				YBCExecuteInsertForDb(dboid,
+									  heapRel,
+									  RelationGetDescr(heapRel),
+									  tup,
+									  &ybctid);
 			}
 			YB_FOR_EACH_DB_END;
 		}
 		oid = YBCExecuteInsertForDb(YBCGetDatabaseOid(heapRel),
 									heapRel,
 									RelationGetDescr(heapRel),
-									tup);
+									tup,
+									&ybctid);
 		/* Update the local cache automatically */
 		YBSetSysCacheTuple(heapRel, tup);
 	}
diff --git a/src/postgres/src/backend/commands/cluster.c b/src/postgres/src/backend/commands/cluster.c
index 6f9424b52f..bf1a62cee8 100644
--- a/src/postgres/src/backend/commands/cluster.c
+++ b/src/postgres/src/backend/commands/cluster.c
@@ -703,9 +703,17 @@ make_new_heap(Oid OIDOldHeap, Oid NewTableSpace, char relpersistence,
 	if (IsYugaByteEnabled() && relpersistence != RELPERSISTENCE_TEMP)
 	{
 		CreateStmt *dummyStmt = makeNode(CreateStmt);
-		dummyStmt->relation = makeRangeVar(NULL, NewHeapName, -1);
-		YBCCreateTable(dummyStmt, RELKIND_RELATION, OldHeapDesc, OIDNewHeap, namespaceid,
-					   InvalidOid, InvalidOid, NewTableSpace, OIDOldHeap);
+		dummyStmt->relation   = makeRangeVar(NULL, NewHeapName, -1);
+		char relkind          = RELKIND_RELATION;
+		Oid matviewPgTableId  = InvalidOid;
+
+		if (OldHeap->rd_rel->relkind == RELKIND_MATVIEW) {
+			relkind = RELKIND_MATVIEW;
+			matviewPgTableId = OIDOldHeap;
+		}
+
+		YBCCreateTable(dummyStmt, relkind, OldHeapDesc, OIDNewHeap, namespaceid,
+					   InvalidOid, InvalidOid, NewTableSpace, matviewPgTableId);
 	}
 
 	ReleaseSysCache(tuple);
diff --git a/src/postgres/src/backend/commands/collationcmds.c b/src/postgres/src/backend/commands/collationcmds.c
index e36fc3026c..a74db4179f 100644
--- a/src/postgres/src/backend/commands/collationcmds.c
+++ b/src/postgres/src/backend/commands/collationcmds.c
@@ -117,8 +117,13 @@ DefineCollation(ParseState *pstate, List *names, List *parameters, bool if_not_e
 	{
 		Oid			collid;
 		HeapTuple	tp;
+		char		*schemaname;
+		char		*collation_name;
+		List		*name;
 
-		collid = get_collation_oid(defGetQualifiedName(fromEl), false);
+		name = defGetQualifiedName(fromEl);
+		collid = get_collation_oid(name, false);
+		DeconstructQualifiedName(name, &schemaname, &collation_name);
 		tp = SearchSysCache1(COLLOID, ObjectIdGetDatum(collid));
 		if (!HeapTupleIsValid(tp))
 			elog(ERROR, "cache lookup failed for collation %u", collid);
@@ -130,6 +135,10 @@ DefineCollation(ParseState *pstate, List *names, List *parameters, bool if_not_e
 
 		ReleaseSysCache(tp);
 
+		if (IsYugaByteEnabled() && YBIsDeprecatedLibICUCollation(collation_name))
+			ereport(ERROR,
+					(errcode(ERRCODE_INVALID_OBJECT_DEFINITION),
+					 errmsg("cannot create collation based on deprecated collation")));
 		/*
 		 * Copying the "default" collation is not allowed because most code
 		 * checks for DEFAULT_COLLATION_OID instead of COLLPROVIDER_DEFAULT,
diff --git a/src/postgres/src/backend/commands/dropcmds.c b/src/postgres/src/backend/commands/dropcmds.c
index 4a7cfa3a8e..255b099adb 100644
--- a/src/postgres/src/backend/commands/dropcmds.c
+++ b/src/postgres/src/backend/commands/dropcmds.c
@@ -30,6 +30,8 @@
 #include "utils/lsyscache.h"
 #include "utils/syscache.h"
 
+#include "pg_yb_utils.h"
+
 
 static void does_not_exist_skipping(ObjectType objtype,
 						Node *object);
@@ -57,6 +59,9 @@ RemoveObjects(DropStmt *stmt)
 {
 	ObjectAddresses *objects;
 	ListCell   *cell1;
+	bool is_yb_db_admin_droppable_object =
+		stmt->removeType == OBJECT_FUNCTION
+		&& IsYbDbAdminUser(GetUserId());
 
 	objects = new_object_addresses();
 
@@ -104,7 +109,8 @@ RemoveObjects(DropStmt *stmt)
 		/* Check permissions. */
 		namespaceId = get_object_namespace(&address);
 		if (!OidIsValid(namespaceId) ||
-			!pg_namespace_ownercheck(namespaceId, GetUserId()))
+			(!pg_namespace_ownercheck(namespaceId, GetUserId()) &&
+			 !is_yb_db_admin_droppable_object))
 			check_object_ownership(GetUserId(), stmt->removeType, address,
 								   object, relation);
 
diff --git a/src/postgres/src/backend/commands/event_trigger.c b/src/postgres/src/backend/commands/event_trigger.c
index 8170528681..e421d030e1 100644
--- a/src/postgres/src/backend/commands/event_trigger.c
+++ b/src/postgres/src/backend/commands/event_trigger.c
@@ -187,7 +187,7 @@ CreateEventTrigger(CreateEventTrigStmt *stmt)
 				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
 				 errmsg("permission denied to create event trigger \"%s\"",
 						stmt->trigname),
-				 errhint("Must be superuser to create an event trigger.")));
+				 errhint("Must be superuser or yb_db_admin role member to create an event trigger.")));
 
 	/* Validate event name. */
 	if (strcmp(stmt->eventname, "ddl_command_start") != 0 &&
@@ -620,7 +620,8 @@ AlterEventTriggerOwner_internal(Relation rel, HeapTuple tup, Oid newOwnerId)
 				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
 				 errmsg("permission denied to change owner of event trigger \"%s\"",
 						NameStr(form->evtname)),
-				 errhint("The owner of an event trigger must be a superuser.")));
+				 errhint("The owner of an event trigger must be a superuser "
+				 		 "or yb_db_admin role member.")));
 
 	form->evtowner = newOwnerId;
 	CatalogTupleUpdate(rel, &tup->t_self, tup);
diff --git a/src/postgres/src/backend/commands/extension.c b/src/postgres/src/backend/commands/extension.c
index 20a6781ed6..b8c175a00c 100644
--- a/src/postgres/src/backend/commands/extension.c
+++ b/src/postgres/src/backend/commands/extension.c
@@ -808,13 +808,15 @@ execute_extension_script(Oid extensionOid, ExtensionControlFile *control,
 					(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
 					 errmsg("permission denied to create extension \"%s\"",
 							control->name),
-					 errhint("Must be superuser to create this extension.")));
+					 errhint("Must be superuser or yb_extension role member "
+					 		 "to create this extension.")));
 		else
 			ereport(ERROR,
 					(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
 					 errmsg("permission denied to update extension \"%s\"",
 							control->name),
-					 errhint("Must be superuser to update this extension.")));
+					 errhint("Must be superuser or yb_extension role member "
+					 		 "to create this extension.")));
 	}
 
 	filename = get_extension_script_filename(control, from_version, version);
diff --git a/src/postgres/src/backend/commands/foreigncmds.c b/src/postgres/src/backend/commands/foreigncmds.c
index 750fd91ba6..a1d01daa19 100644
--- a/src/postgres/src/backend/commands/foreigncmds.c
+++ b/src/postgres/src/backend/commands/foreigncmds.c
@@ -222,7 +222,8 @@ AlterForeignDataWrapperOwner_internal(Relation rel, HeapTuple tup, Oid newOwnerI
 				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
 				 errmsg("permission denied to change owner of foreign-data wrapper \"%s\"",
 						NameStr(form->fdwname)),
-				 errhint("Must be superuser to change owner of a foreign-data wrapper.")));
+				 errhint("Must be superuser or yb_fdw role member to change "
+				 		 "owner of a foreign-data wrapper.")));
 
 	/* New owner must also be a superuser */
 	if (!IsYbFdwUser(newOwnerId) && !superuser_arg(newOwnerId))
@@ -230,7 +231,8 @@ AlterForeignDataWrapperOwner_internal(Relation rel, HeapTuple tup, Oid newOwnerI
 				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
 				 errmsg("permission denied to change owner of foreign-data wrapper \"%s\"",
 						NameStr(form->fdwname)),
-				 errhint("The owner of a foreign-data wrapper must be a superuser.")));
+				 errhint("Must be superuser or yb_fdw role member to change "
+				 		 "owner of a foreign-data wrapper.")));
 
 	if (form->fdwowner != newOwnerId)
 	{
@@ -581,7 +583,8 @@ CreateForeignDataWrapper(CreateFdwStmt *stmt)
 				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
 				 errmsg("permission denied to create foreign-data wrapper \"%s\"",
 						stmt->fdwname),
-				 errhint("Must be superuser to create a foreign-data wrapper.")));
+				 errhint("Must be superuser or yb_fdw role member to create a "
+				 		 "foreign-data wrapper.")));
 
 	/* For now the owner cannot be specified on create. Use effective user ID. */
 	ownerId = GetUserId();
@@ -695,7 +698,8 @@ AlterForeignDataWrapper(AlterFdwStmt *stmt)
 				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
 				 errmsg("permission denied to alter foreign-data wrapper \"%s\"",
 						stmt->fdwname),
-				 errhint("Must be superuser to alter a foreign-data wrapper.")));
+				 errhint("Must be superuser or yb_fdw role member to alter a "
+				 		 "foreign-data wrapper.")));
 
 	tp = SearchSysCacheCopy1(FOREIGNDATAWRAPPERNAME,
 							 CStringGetDatum(stmt->fdwname));
diff --git a/src/postgres/src/backend/commands/functioncmds.c b/src/postgres/src/backend/commands/functioncmds.c
index bfe7aa1a8a..94ad097d80 100644
--- a/src/postgres/src/backend/commands/functioncmds.c
+++ b/src/postgres/src/backend/commands/functioncmds.c
@@ -906,7 +906,7 @@ CreateFunction(ParseState *pstate, CreateFunctionStmt *stmt)
 
 	/* Check we have creation rights in target namespace */
 	aclresult = pg_namespace_aclcheck(namespaceId, GetUserId(), ACL_CREATE);
-	if (aclresult != ACLCHECK_OK)
+	if (aclresult != ACLCHECK_OK && !IsYbDbAdminUser(GetUserId()))
 		aclcheck_error(aclresult, OBJECT_SCHEMA,
 					   get_namespace_name(namespaceId));
 
@@ -948,7 +948,7 @@ CreateFunction(ParseState *pstate, CreateFunctionStmt *stmt)
 		AclResult	aclresult;
 
 		aclresult = pg_language_aclcheck(languageOid, GetUserId(), ACL_USAGE);
-		if (aclresult != ACLCHECK_OK)
+		if (aclresult != ACLCHECK_OK && !IsYbDbAdminUser(GetUserId()))
 			aclcheck_error(aclresult, OBJECT_LANGUAGE,
 						   NameStr(languageStruct->lanname));
 	}
@@ -958,7 +958,8 @@ CreateFunction(ParseState *pstate, CreateFunctionStmt *stmt)
 		 * If untrusted language, must be superuser, or someone with the
 		 * yb_extension role in the midst of creating an extension.
 		 */
-		if (!(IsYbExtensionUser(GetUserId()) && creating_extension) && !superuser())
+		if (!(IsYbExtensionUser(GetUserId()) && creating_extension) &&
+			!superuser())
 			aclcheck_error(ACLCHECK_NO_PRIV, OBJECT_LANGUAGE,
 						   NameStr(languageStruct->lanname));
 	}
@@ -1206,7 +1207,8 @@ AlterFunction(ParseState *pstate, AlterFunctionStmt *stmt)
 	procForm = (Form_pg_proc) GETSTRUCT(tup);
 
 	/* Permission check: must own function */
-	if (!pg_proc_ownercheck(funcOid, GetUserId()) && !IsYbDbAdminUser(GetUserId()))
+	if (!pg_proc_ownercheck(funcOid, GetUserId()) &&
+		!IsYbDbAdminUser(GetUserId()))
 		aclcheck_error(ACLCHECK_NOT_OWNER, stmt->objtype,
 					   NameListToString(stmt->func->objname));
 
@@ -1562,7 +1564,7 @@ CreateCast(CreateCastStmt *stmt)
 		 * Must be superuser to create binary-compatible casts, since
 		 * erroneous casts can easily crash the backend.
 		 */
-		if (!superuser())
+		if (!superuser() && !(IsYbExtensionUser(GetUserId()) && creating_extension))
 			ereport(ERROR,
 					(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
 					 errmsg("must be superuser to create a cast WITHOUT FUNCTION")));
diff --git a/src/postgres/src/backend/commands/opclasscmds.c b/src/postgres/src/backend/commands/opclasscmds.c
index 67c982b7a7..7032b2d6ee 100644
--- a/src/postgres/src/backend/commands/opclasscmds.c
+++ b/src/postgres/src/backend/commands/opclasscmds.c
@@ -409,7 +409,8 @@ DefineOpClass(CreateOpClassStmt *stmt)
 	if (!(IsYbExtensionUser(GetUserId()) && creating_extension) && !superuser())
 		ereport(ERROR,
 				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
-				 errmsg("must be superuser to create an operator class")));
+				 errmsg("must be superuser or yb_extension role member to "
+				 		"create an operator class")));
 
 	/* Look up the datatype */
 	typeoid = typenameTypeId(NULL, stmt->datatype);
@@ -807,10 +808,11 @@ AlterOpFamily(AlterOpFamilyStmt *stmt)
 	 *
 	 * XXX re-enable NOT_USED code sections below if you remove this test.
 	 */
-	if (!superuser())
+	if (!superuser() && !(IsYbExtensionUser(GetUserId()) && creating_extension))
 		ereport(ERROR,
 				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
-				 errmsg("must be superuser to alter an operator family")));
+				 errmsg("must be superuser or yb_extension role member to "
+				 		"alter an operator family")));
 
 	/*
 	 * ADD and DROP cases need separate code from here on down.
diff --git a/src/postgres/src/backend/commands/tablecmds.c b/src/postgres/src/backend/commands/tablecmds.c
index e5a3fcf935..120f58b23e 100644
--- a/src/postgres/src/backend/commands/tablecmds.c
+++ b/src/postgres/src/backend/commands/tablecmds.c
@@ -3378,7 +3378,9 @@ RenameRelation(RenameStmt *stmt)
 	/* YB rename is not needed for a primary key dummy index. */
 	rel             = RelationIdGetRelation(relid);
 	needs_yb_rename = IsYBRelation(rel) &&
-	                  !(rel->rd_rel->relkind == RELKIND_INDEX && rel->rd_index->indisprimary);
+					  !(rel->rd_rel->relkind == RELKIND_INDEX &&
+						rel->rd_index->indisprimary) &&
+					  rel->rd_rel->relkind != RELKIND_PARTITIONED_INDEX;
 	RelationClose(rel);
 
 	/* Do the work */
@@ -16880,10 +16882,10 @@ CloneRowTriggersToPartition(Relation parent, Relation partition)
 		trigStmt->initdeferred = trigForm->tginitdeferred;
 		trigStmt->constrrel = NULL; /* passed separately */
 
-		CreateTrigger(trigStmt, NULL, RelationGetRelid(partition),
-					  trigForm->tgconstrrelid, InvalidOid, InvalidOid,
-					  trigForm->tgfoid, HeapTupleGetOid(tuple), qual,
-					  false, true);
+		CreateTriggerFiringOn(trigStmt, NULL, RelationGetRelid(partition),
+							  trigForm->tgconstrrelid, InvalidOid, InvalidOid,
+							  trigForm->tgfoid, HeapTupleGetOid(tuple), qual,
+							  false, true, trigForm->tgenabled);
 
 		MemoryContextReset(perTupCxt);
 	}
diff --git a/src/postgres/src/backend/commands/tablespace.c b/src/postgres/src/backend/commands/tablespace.c
index 2c09ed4d91..71af63b86f 100644
--- a/src/postgres/src/backend/commands/tablespace.c
+++ b/src/postgres/src/backend/commands/tablespace.c
@@ -377,7 +377,8 @@ CreateTableSpace(CreateTableSpaceStmt *stmt)
 				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
 				 errmsg("permission denied to create tablespace \"%s\"",
 						stmt->tablespacename),
-				 errhint("Must be superuser or yb_db_admin role to create a tablespace.")));
+				 errhint("Must be superuser or yb_db_admin role member to "
+				 		 "create a tablespace.")));
 
 	/* However, the eventual owner of the tablespace need not be */
 	if (stmt->owner)
diff --git a/src/postgres/src/backend/commands/trigger.c b/src/postgres/src/backend/commands/trigger.c
index decaf2ae6b..1624c06d82 100644
--- a/src/postgres/src/backend/commands/trigger.c
+++ b/src/postgres/src/backend/commands/trigger.c
@@ -161,6 +161,24 @@ CreateTrigger(CreateTrigStmt *stmt, const char *queryString,
 			  Oid relOid, Oid refRelOid, Oid constraintOid, Oid indexOid,
 			  Oid funcoid, Oid parentTriggerOid, Node *whenClause,
 			  bool isInternal, bool in_partition)
+{
+	return
+		CreateTriggerFiringOn(stmt, queryString, relOid, refRelOid,
+							  constraintOid, indexOid, funcoid,
+							  parentTriggerOid, whenClause, isInternal,
+							  in_partition, TRIGGER_FIRES_ON_ORIGIN);
+}
+
+/*
+ * Like the above; additionally the firing condition
+ * (always/origin/replica/disabled) can be specified.
+ */
+ObjectAddress
+CreateTriggerFiringOn(CreateTrigStmt *stmt, const char *queryString,
+					  Oid relOid, Oid refRelOid, Oid constraintOid,
+					  Oid indexOid, Oid funcoid, Oid parentTriggerOid,
+					  Node *whenClause, bool isInternal, bool in_partition,
+					  char trigger_fires_when)
 {
 	int16		tgtype;
 	int			ncolumns;
@@ -829,7 +847,7 @@ CreateTrigger(CreateTrigStmt *stmt, const char *queryString,
 															 CStringGetDatum(trigname));
 	values[Anum_pg_trigger_tgfoid - 1] = ObjectIdGetDatum(funcoid);
 	values[Anum_pg_trigger_tgtype - 1] = Int16GetDatum(tgtype);
-	values[Anum_pg_trigger_tgenabled - 1] = CharGetDatum(TRIGGER_FIRES_ON_ORIGIN);
+	values[Anum_pg_trigger_tgenabled - 1] = trigger_fires_when;
 	values[Anum_pg_trigger_tgisinternal - 1] = BoolGetDatum(isInternal || in_partition);
 	values[Anum_pg_trigger_tgconstrrelid - 1] = ObjectIdGetDatum(constrrelid);
 	values[Anum_pg_trigger_tgconstrindid - 1] = ObjectIdGetDatum(indexOid);
@@ -1172,11 +1190,11 @@ CreateTrigger(CreateTrigStmt *stmt, const char *queryString,
 			if (found_whole_row)
 				elog(ERROR, "unexpected whole-row reference found in trigger WHEN clause");
 
-			CreateTrigger(childStmt, queryString,
-						  partdesc->oids[i], refRelOid,
-						  InvalidOid, indexOnChild,
-						  funcoid, trigoid, qual,
-						  isInternal, true);
+			CreateTriggerFiringOn(childStmt, queryString,
+								  partdesc->oids[i], refRelOid,
+								  InvalidOid, indexOnChild,
+								  funcoid, trigoid, qual,
+								  isInternal, true, trigger_fires_when);
 
 			heap_close(childTbl, NoLock);
 
diff --git a/src/postgres/src/backend/commands/typecmds.c b/src/postgres/src/backend/commands/typecmds.c
index 0234ff247c..b1f7dfc8d1 100644
--- a/src/postgres/src/backend/commands/typecmds.c
+++ b/src/postgres/src/backend/commands/typecmds.c
@@ -183,7 +183,8 @@ DefineType(ParseState *pstate, List *names, List *parameters)
 	if (!(IsYbExtensionUser(GetUserId()) && creating_extension) && !superuser())
 		ereport(ERROR,
 				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
-				 errmsg("must be superuser to create a base type")));
+				 errmsg("must be superuser or yb_extension role member to "
+				 		"create a base type")));
 
 	/* Convert list of names to a name and namespace */
 	typeNamespace = QualifiedNameGetCreationNamespace(names, &typeName);
diff --git a/src/postgres/src/backend/commands/user.c b/src/postgres/src/backend/commands/user.c
index 30eb5a24fc..2f77f709fa 100644
--- a/src/postgres/src/backend/commands/user.c
+++ b/src/postgres/src/backend/commands/user.c
@@ -307,7 +307,8 @@ CreateRole(ParseState *pstate, CreateRoleStmt *stmt)
 		if (!superuser() && !IsYbDbAdminUser(GetUserId()))
 			ereport(ERROR,
 					(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
-					 errmsg("must be superuser or yb_db_admin role to change bypassrls attribute")));
+					 errmsg("must be superuser or yb_db_admin role member to "
+					 		"change bypassrls attribute")));
 	}
 	else
 	{
@@ -707,7 +708,8 @@ AlterRole(AlterRoleStmt *stmt)
 		if (!superuser() && !IsYbDbAdminUser(GetUserId()))
 			ereport(ERROR,
 					(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
-					 errmsg("must be superuser or yb_db_admin role to change bypassrls attribute")));
+					 errmsg("must be superuser or yb_db_admin role member "
+					 		"to change bypassrls attribute")));
 	}
 	else if (!have_createrole_privilege())
 	{
@@ -1390,16 +1392,24 @@ ReassignOwnedObjects(ReassignOwnedStmt *stmt)
 	{
 		Oid			roleid = lfirst_oid(cell);
 
-		if (!has_privs_of_role(GetUserId(), roleid))
+		if (!has_privs_of_role(GetUserId(), roleid) &&
+			!IsYbDbAdminUser(GetUserId()))
 			ereport(ERROR,
 					(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
 					 errmsg("permission denied to reassign objects")));
+
+		if (superuser_arg(roleid) && !superuser())
+			ereport(ERROR,
+					(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
+					 errmsg("non-superuser cannot reassign objects "
+					 		"from superuser")));
 	}
 
 	/* Must have privileges on the receiving side too */
 	newrole = get_rolespec_oid(stmt->newrole, false);
 
-	if (!has_privs_of_role(GetUserId(), newrole))
+	if (!has_privs_of_role(GetUserId(), newrole) &&
+		!IsYbDbAdminUser(GetUserId()))
 		ereport(ERROR,
 				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
 				 errmsg("permission denied to reassign objects")));
diff --git a/src/postgres/src/backend/commands/variable.c b/src/postgres/src/backend/commands/variable.c
index cad5f83a8c..feef5865df 100644
--- a/src/postgres/src/backend/commands/variable.c
+++ b/src/postgres/src/backend/commands/variable.c
@@ -600,7 +600,8 @@ assign_XactIsoLevel(const char *newval, void *extra)
 	XactIsoLevel = *((int *) extra);
 	if (YBTransactionsEnabled())
 	{
-		HandleYBStatus(YBCPgSetTransactionIsolationLevel(XactIsoLevel));
+		HandleYBStatus(
+			YBCPgSetTransactionIsolationLevel(YBGetEffectivePggateIsolationLevel()));
 	}
 }
 
diff --git a/src/postgres/src/backend/commands/ybccmds.c b/src/postgres/src/backend/commands/ybccmds.c
index e4d0b41eab..5b5b7e4cd9 100644
--- a/src/postgres/src/backend/commands/ybccmds.c
+++ b/src/postgres/src/backend/commands/ybccmds.c
@@ -468,6 +468,7 @@ YBCCreateTable(CreateStmt *stmt, char relkind, TupleDesc desc,
 	ListCell       *listptr;
 	bool           is_shared_relation = tablespaceId == GLOBALTABLESPACE_OID;
 	Oid            databaseId         = YBCGetDatabaseOidFromShared(is_shared_relation);
+	bool           is_matview         = relkind == RELKIND_MATVIEW;
 
 	char *db_name = get_database_name(databaseId);
 	char *schema_name = stmt->relation->schemaname;
@@ -612,6 +613,7 @@ YBCCreateTable(CreateStmt *stmt, char relkind, TupleDesc desc,
 									   tablegroupId,
 									   colocationId,
 									   tablespaceId,
+									   is_matview,
 									   matviewPgTableId,
 									   &handle));
 
@@ -631,10 +633,10 @@ YBCCreateTable(CreateStmt *stmt, char relkind, TupleDesc desc,
 void
 YBCDropTable(Oid relationId)
 {
-	YBCPgStatement  handle     = NULL;
-	Oid             databaseId = YBCGetDatabaseOidByRelid(relationId);
-	// TODO(alex): Rename to disambiguate colocation via DB vs via tablegroup
-	bool            colocated  = false;
+	YBCPgStatement handle = NULL;
+	Oid			databaseId = YBCGetDatabaseOidByRelid(relationId);
+	/* TODO(alex): Rename to disambiguate colocation via DB vs via tablegroup */
+	bool		colocated = false;
 
 	/* Determine if table is colocated */
 	if (MyDatabaseColocated)
@@ -646,7 +648,7 @@ YBCDropTable(Oid relationId)
 									 &not_found);
 	}
 
-	/* Create table-level tombstone for colocated tables / tables in a tablegroup */
+	/* Create table-level tombstone for colocated/tablegroup tables */
 	Oid tablegroupId = InvalidOid;
 	if (YbTablegroupCatalogExists)
 		tablegroupId = get_tablegroup_oid_by_table_oid(relationId);
@@ -658,7 +660,8 @@ YBCDropTable(Oid relationId)
 															   false,
 															   &handle),
 									 &not_found);
-		/* Since the creation of the handle could return a 'NotFound' error,
+		/*
+		 * Since the creation of the handle could return a 'NotFound' error,
 		 * execute the statement only if the handle is valid.
 		 */
 		const bool valid_handle = !not_found;
@@ -694,12 +697,13 @@ YBCDropTable(Oid relationId)
 }
 
 void
-YBCTruncateTable(Relation rel) {
-	YBCPgStatement  handle;
-	Oid             relationId = RelationGetRelid(rel);
-	Oid             databaseId = YBCGetDatabaseOid(rel);
-	// TODO(alex): Rename to disambiguate colocation via DB vs via tablegroup
-	bool            colocated = false;
+YBCTruncateTable(Relation rel)
+{
+	YBCPgStatement handle;
+	Oid			relationId = RelationGetRelid(rel);
+	Oid			databaseId = YBCGetDatabaseOid(rel);
+	/* TODO(alex): Rename to disambiguate colocation via DB vs via tablegroup */
+	bool		colocated = false;
 
 	/* Determine if table is colocated */
 	if (MyDatabaseColocated)
@@ -711,7 +715,7 @@ YBCTruncateTable(Relation rel) {
 		tablegroupId = get_tablegroup_oid_by_table_oid(relationId);
 	if (colocated || tablegroupId != InvalidOid)
 	{
-		/* Create table-level tombstone for colocated tables / tables in tablegroups */
+		/* Create table-level tombstone for colocated/tablegroup tables */
 		HandleYBStatus(YBCPgNewTruncateColocated(databaseId,
 												 relationId,
 												 false,
@@ -735,42 +739,21 @@ YBCTruncateTable(Relation rel) {
 	/* Truncate the associated secondary indexes */
 	List	 *indexlist = RelationGetIndexList(rel);
 	ListCell *lc;
-
 	foreach(lc, indexlist)
 	{
 		Oid indexId = lfirst_oid(lc);
 
+		/* PK index is not secondary index, skip */
 		if (indexId == rel->rd_pkindex)
 			continue;
 
-		/* Determine if index is colocated */
-		if (MyDatabaseColocated)
-			HandleYBStatus(YBCPgIsTableColocated(databaseId,
-												indexId,
-												&colocated));
-
-		tablegroupId = InvalidOid;
-		if (YbTablegroupCatalogExists)
-			tablegroupId = get_tablegroup_oid_by_table_oid(indexId);
-		if (colocated || tablegroupId != InvalidOid)
-		{
-			/* Create index-level tombstone for colocated indexes / indexes in tablegroups */
-			HandleYBStatus(YBCPgNewTruncateColocated(databaseId,
-													 indexId,
-													 false,
-													 &handle));
-			HandleYBStatus(YBCPgDmlBindTable(handle));
-			int rows_affected_count = 0;
-			HandleYBStatus(YBCPgDmlExecWriteOp(handle, &rows_affected_count));
-		}
-		else
-		{
-			/* Send truncate table RPC to master for non-colocated tables */
-			HandleYBStatus(YBCPgNewTruncateTable(databaseId,
-												 indexId,
-												 &handle));
-			HandleYBStatus(YBCPgExecTruncateTable(handle));
-		}
+		/*
+		 * Lock level doesn't fully work in YB.  Since YB TRUNCATE is already
+		 * considered to not be transaction-safe, it doesn't really matter.
+		 */
+		Relation indexRel = index_open(indexId, AccessExclusiveLock);
+		YBCTruncateTable(indexRel);
+		index_close(indexRel, AccessExclusiveLock);
 	}
 
 	list_free(indexlist);
@@ -950,13 +933,13 @@ YBCPrepareAlterTableCmd(AlterTableCmd* cmd, Relation rel, YBCPgStatement handle,
 
 			typeTuple = typenameType(NULL, colDef->typeName, &typmod);
 			typeOid = HeapTupleGetOid(typeTuple);
+			ReleaseSysCache(typeTuple);
 			order = RelationGetNumberOfAttributes(rel) + *col;
 			const YBCPgTypeEntity *col_type = YbDataTypeFromOidMod(order, typeOid);
 
 			HandleYBStatus(YBCPgAlterTableAddColumn(handle, colDef->colname,
 						   order, col_type));
 			++(*col);
-			ReleaseSysCache(typeTuple);
 			*needsYBAlter = true;
 
 			/*
@@ -1192,6 +1175,7 @@ YBCRename(RenameStmt *stmt, Oid relationId)
 
 	switch (stmt->renameType)
 	{
+		case OBJECT_INDEX:
 		case OBJECT_TABLE:
 			HandleYBStatus(YBCPgNewAlterTable(databaseId,
 											  relationId,
@@ -1221,10 +1205,10 @@ YBCRename(RenameStmt *stmt, Oid relationId)
 void
 YBCDropIndex(Oid relationId)
 {
-	YBCPgStatement	handle;
-	// TODO(alex): Rename to disambiguate colocation via DB vs via tablegroup
-	bool			colocated  = false;
-	Oid				databaseId = YBCGetDatabaseOidByRelid(relationId);
+	YBCPgStatement handle;
+	/* TODO(alex): Rename to disambiguate colocation via DB vs via tablegroup */
+	bool		colocated = false;
+	Oid			databaseId = YBCGetDatabaseOidByRelid(relationId);
 
 	/* Determine if table is colocated */
 	if (MyDatabaseColocated)
@@ -1236,7 +1220,7 @@ YBCDropIndex(Oid relationId)
 									 &not_found);
 	}
 
-	/* Create table-level tombstone for colocated indexes / indexes in a tablegroup */
+	/* Create table-level tombstone for colocated/tablegroup indexes */
 	Oid tablegroupId = InvalidOid;
 	if (YbTablegroupCatalogExists)
 		tablegroupId = get_tablegroup_oid_by_table_oid(relationId);
@@ -1249,7 +1233,8 @@ YBCDropIndex(Oid relationId)
 															   &handle),
 									 &not_found);
 		const bool valid_handle = !not_found;
-		if (valid_handle) {
+		if (valid_handle)
+		{
 			HandleYBStatusIgnoreNotFound(YBCPgDmlBindTable(handle), &not_found);
 			int rows_affected_count = 0;
 			HandleYBStatusIgnoreNotFound(YBCPgDmlExecWriteOp(handle, &rows_affected_count),
@@ -1266,7 +1251,8 @@ YBCDropIndex(Oid relationId)
 													   &handle),
 									 &not_found);
 		const bool valid_handle = !not_found;
-		if (valid_handle) {
+		if (valid_handle)
+		{
 			/*
 			 * We cannot abort drop in DocDB so postpone the execution until
 			 * the rest of the statement/txn is finished executing.
diff --git a/src/postgres/src/backend/executor/execMain.c b/src/postgres/src/backend/executor/execMain.c
index 3b0da31823..a0535842c9 100644
--- a/src/postgres/src/backend/executor/execMain.c
+++ b/src/postgres/src/backend/executor/execMain.c
@@ -43,6 +43,7 @@
 #include "access/xact.h"
 #include "catalog/namespace.h"
 #include "catalog/pg_publication.h"
+#include "commands/extension.h"
 #include "commands/matview.h"
 #include "commands/trigger.h"
 #include "executor/execdebug.h"
@@ -825,7 +826,10 @@ InitPlan(QueryDesc *queryDesc, int eflags)
 	/*
 	 * Do permissions checks
 	 */
-	ExecCheckRTPerms(rangeTable, true);
+	if (!(IsYbExtensionUser(GetUserId()) && creating_extension))
+	{
+		ExecCheckRTPerms(rangeTable, true);
+	}
 
 	/*
 	 * initialize the node's execution state
diff --git a/src/postgres/src/backend/executor/nodeIndexonlyscan.c b/src/postgres/src/backend/executor/nodeIndexonlyscan.c
index 3bee737614..90baea1113 100644
--- a/src/postgres/src/backend/executor/nodeIndexonlyscan.c
+++ b/src/postgres/src/backend/executor/nodeIndexonlyscan.c
@@ -126,6 +126,8 @@ IndexOnlyNext(IndexOnlyScanState *node)
 	/*
 	 * OK, now that we have what we need, fetch the next tuple.
 	 */
+	MemoryContext oldcontext;
+	oldcontext = MemoryContextSwitchTo(node->ss.ps.ps_ExprContext->ecxt_per_tuple_memory);
 	while ((tid = index_getnext_tid(scandesc, direction)) != NULL)
 	{
 		HeapTuple	tuple = NULL;
@@ -228,9 +230,10 @@ IndexOnlyNext(IndexOnlyScanState *node)
 		if (scandesc->xs_recheck)
 		{
 			econtext->ecxt_scantuple = slot;
-			if (!ExecQualAndReset(node->indexqual, econtext))
+			if (!ExecQual(node->indexqual, econtext))
 			{
 				/* Fails recheck, so drop it and loop back for another */
+				ResetExprContext(econtext);
 				InstrCountFiltered2(node, 1);
 				continue;
 			}
@@ -262,9 +265,10 @@ IndexOnlyNext(IndexOnlyScanState *node)
 			PredicateLockPage(scandesc->heapRelation,
 							  ItemPointerGetBlockNumber(tid),
 							  estate->es_snapshot);
-
+		MemoryContextSwitchTo(oldcontext);
 		return slot;
 	}
+	MemoryContextSwitchTo(oldcontext);
 
 	/*
 	 * if we get here it means the index scan failed so we are at the end of
diff --git a/src/postgres/src/backend/executor/nodeIndexscan.c b/src/postgres/src/backend/executor/nodeIndexscan.c
index e08c6e9abd..747d7c0f67 100644
--- a/src/postgres/src/backend/executor/nodeIndexscan.c
+++ b/src/postgres/src/backend/executor/nodeIndexscan.c
@@ -168,6 +168,9 @@ IndexNext(IndexScanState *node)
 	/*
 	 * ok, now that we have what we need, fetch the next tuple.
 	 */
+	MemoryContext oldcontext;
+	oldcontext = MemoryContextSwitchTo(
+		node->ss.ps.ps_ExprContext->ecxt_per_tuple_memory);
 	while ((tuple = index_getnext(scandesc, direction)) != NULL)
 	{
 		CHECK_FOR_INTERRUPTS();
@@ -189,14 +192,15 @@ IndexNext(IndexScanState *node)
 		if (scandesc->xs_recheck)
 		{
 			econtext->ecxt_scantuple = slot;
-			if (!ExecQualAndReset(node->indexqualorig, econtext))
+			if (!ExecQual(node->indexqualorig, econtext))
 			{
+				ResetExprContext(econtext);
 				/* Fails recheck, so drop it and loop back for another */
 				InstrCountFiltered2(node, 1);
 				continue;
 			}
 		}
-
+		MemoryContextSwitchTo(oldcontext);
 		return slot;
 	}
 
@@ -205,6 +209,7 @@ IndexNext(IndexScanState *node)
 	 * the scan..
 	 */
 	node->iss_ReachedEnd = true;
+	MemoryContextSwitchTo(oldcontext);
 	return ExecClearTuple(slot);
 }
 
diff --git a/src/postgres/src/backend/executor/ybcModifyTable.c b/src/postgres/src/backend/executor/ybcModifyTable.c
index f995451e38..a3674ae83e 100644
--- a/src/postgres/src/backend/executor/ybcModifyTable.c
+++ b/src/postgres/src/backend/executor/ybcModifyTable.c
@@ -211,7 +211,8 @@ static Oid YBCExecuteInsertInternal(Oid dboid,
                                     Relation rel,
                                     TupleDesc tupleDesc,
                                     HeapTuple tuple,
-                                    bool is_single_row_txn)
+                                    bool is_single_row_txn,
+                                    Datum *ybctid)
 {
 	Oid            relid    = RelationGetRelid(rel);
 	AttrNumber     minattr  = YBGetFirstLowInvalidAttributeNumber(rel);
@@ -234,9 +235,17 @@ static Oid YBCExecuteInsertInternal(Oid dboid,
 	                              &insert_stmt));
 
 	/* Get the ybctid for the tuple and bind to statement */
-	tuple->t_ybctid = YBCGetYBTupleIdFromTuple(rel, tuple, tupleDesc);
+	tuple->t_ybctid =
+		ybctid != NULL && *ybctid != 0 ? *ybctid
+		                               : YBCGetYBTupleIdFromTuple(rel, tuple, tupleDesc);
+
 	YBCBindTupleId(insert_stmt, tuple->t_ybctid);
 
+	if (ybctid != NULL)
+	{
+		*ybctid = tuple->t_ybctid;
+	}
+
 	for (AttrNumber attnum = minattr; attnum <= natts; attnum++)
 	{
 		/* Skip virtual (system) and dropped columns */
@@ -312,20 +321,23 @@ Oid YBCExecuteInsert(Relation rel,
 	return YBCExecuteInsertForDb(YBCGetDatabaseOid(rel),
 	                             rel,
 	                             tupleDesc,
-	                             tuple);
+	                             tuple,
+	                             NULL /* ybctid */);
 }
 
 Oid YBCExecuteInsertForDb(Oid dboid,
                           Relation rel,
                           TupleDesc tupleDesc,
-                          HeapTuple tuple)
+                          HeapTuple tuple,
+                          Datum *ybctid)
 {
 	bool non_transactional = !IsSystemRelation(rel) && yb_disable_transactional_writes;
 	return YBCExecuteInsertInternal(dboid,
 	                                rel,
 	                                tupleDesc,
 	                                tuple,
-	                                non_transactional);
+	                                non_transactional,
+	                                ybctid);
 }
 
 Oid YBCExecuteNonTxnInsert(Relation rel,
@@ -335,19 +347,22 @@ Oid YBCExecuteNonTxnInsert(Relation rel,
 	return YBCExecuteNonTxnInsertForDb(YBCGetDatabaseOid(rel),
 	                                   rel,
 	                                   tupleDesc,
-	                                   tuple);
+	                                   tuple,
+	                                   NULL /* ybctid */);
 }
 
 Oid YBCExecuteNonTxnInsertForDb(Oid dboid,
                                 Relation rel,
                                 TupleDesc tupleDesc,
-                                HeapTuple tuple)
+                                HeapTuple tuple,
+                                Datum *ybctid)
 {
 	return YBCExecuteInsertInternal(dboid,
 	                                rel,
 	                                tupleDesc,
 	                                tuple,
-	                                true /* is_single_row_txn */);
+	                                true /* is_single_row_txn */,
+	                                ybctid);
 }
 
 Oid YBCHeapInsert(TupleTableSlot *slot,
@@ -355,13 +370,14 @@ Oid YBCHeapInsert(TupleTableSlot *slot,
                   EState *estate)
 {
 	Oid dboid = YBCGetDatabaseOid(estate->es_result_relation_info->ri_RelationDesc);
-	return YBCHeapInsertForDb(dboid, slot, tuple, estate);
+	return YBCHeapInsertForDb(dboid, slot, tuple, estate, NULL /* ybctid */);
 }
 
 Oid YBCHeapInsertForDb(Oid dboid,
                        TupleTableSlot* slot,
                        HeapTuple tuple,
-                       EState* estate)
+                       EState* estate,
+                       Datum* ybctid)
 {
 	/*
 	 * get information on the (current) result relation
@@ -381,14 +397,16 @@ Oid YBCHeapInsertForDb(Oid dboid,
 		return YBCExecuteNonTxnInsertForDb(dboid,
 		                                   resultRelationDesc,
 		                                   slot->tts_tupleDescriptor,
-		                                   tuple);
+		                                   tuple,
+		                                   ybctid);
 	}
 	else
 	{
 		return YBCExecuteInsertForDb(dboid,
 		                             resultRelationDesc,
 		                             slot->tts_tupleDescriptor,
-		                             tuple);
+		                             tuple,
+		                             ybctid);
 	}
 }
 
diff --git a/src/postgres/src/backend/nodes/copyfuncs.c b/src/postgres/src/backend/nodes/copyfuncs.c
index 2d52ef4e56..33f66949b5 100644
--- a/src/postgres/src/backend/nodes/copyfuncs.c
+++ b/src/postgres/src/backend/nodes/copyfuncs.c
@@ -102,6 +102,7 @@ _copyPlannedStmt(const PlannedStmt *from)
 	COPY_NODE_FIELD(utilityStmt);
 	COPY_LOCATION_FIELD(stmt_location);
 	COPY_LOCATION_FIELD(stmt_len);
+	COPY_SCALAR_FIELD(yb_num_referenced_relations);
 
 	return newnode;
 }
diff --git a/src/postgres/src/backend/nodes/outfuncs.c b/src/postgres/src/backend/nodes/outfuncs.c
index 6ee100d330..cb143380fd 100644
--- a/src/postgres/src/backend/nodes/outfuncs.c
+++ b/src/postgres/src/backend/nodes/outfuncs.c
@@ -292,6 +292,7 @@ _outPlannedStmt(StringInfo str, const PlannedStmt *node)
 	WRITE_NODE_FIELD(utilityStmt);
 	WRITE_LOCATION_FIELD(stmt_location);
 	WRITE_LOCATION_FIELD(stmt_len);
+	WRITE_INT_FIELD(yb_num_referenced_relations);
 }
 
 /*
diff --git a/src/postgres/src/backend/nodes/readfuncs.c b/src/postgres/src/backend/nodes/readfuncs.c
index b24d8623e2..0c62778107 100644
--- a/src/postgres/src/backend/nodes/readfuncs.c
+++ b/src/postgres/src/backend/nodes/readfuncs.c
@@ -1504,6 +1504,7 @@ _readPlannedStmt(void)
 	READ_NODE_FIELD(utilityStmt);
 	READ_LOCATION_FIELD(stmt_location);
 	READ_LOCATION_FIELD(stmt_len);
+	READ_INT_FIELD(yb_num_referenced_relations);
 
 	READ_DONE();
 }
diff --git a/src/postgres/src/backend/optimizer/path/allpaths.c b/src/postgres/src/backend/optimizer/path/allpaths.c
index b3d45e2157..4a70747a34 100644
--- a/src/postgres/src/backend/optimizer/path/allpaths.c
+++ b/src/postgres/src/backend/optimizer/path/allpaths.c
@@ -1287,6 +1287,9 @@ set_append_rel_size(PlannerInfo *root, RelOptInfo *rel,
 		/* We have at least one live child. */
 		has_live_children = true;
 
+		if (!childRTE->inh)
+			root->yb_num_referenced_relations++;
+
 		/*
 		 * If any live child is not parallel-safe, treat the whole appendrel
 		 * as not parallel-safe.  In future we might be able to generate plans
diff --git a/src/postgres/src/backend/optimizer/plan/planner.c b/src/postgres/src/backend/optimizer/plan/planner.c
index 7cd7393dc3..7e997d5340 100644
--- a/src/postgres/src/backend/optimizer/plan/planner.c
+++ b/src/postgres/src/backend/optimizer/plan/planner.c
@@ -532,6 +532,7 @@ standard_planner(Query *parse, int cursorOptions, ParamListInfo boundParams)
 	result->utilityStmt = parse->utilityStmt;
 	result->stmt_location = parse->stmt_location;
 	result->stmt_len = parse->stmt_len;
+	result->yb_num_referenced_relations = root->yb_num_referenced_relations;
 
 	result->jitFlags = PGJIT_NONE;
 	if (jit_enabled && jit_above_cost >= 0 &&
@@ -631,6 +632,7 @@ subquery_planner(PlannerGlobal *glob, Query *parse,
 		root->wt_param_id = -1;
 	root->non_recursive_path = NULL;
 	root->partColsUpdated = false;
+	root->yb_num_referenced_relations = 0;
 
 	/*
 	 * If there is a WITH list, process each WITH query and build an initplan
@@ -987,6 +989,14 @@ subquery_planner(PlannerGlobal *glob, Query *parse,
 	 */
 	set_cheapest(final_rel);
 
+	/*
+	 * For the top-level query, parent_root is NULL. In all other cases,
+	 * update the number of relations that survived constraint exclusion
+	 * and partition pruning.
+	 */
+	if (parent_root)
+		parent_root->yb_num_referenced_relations +=
+			root->yb_num_referenced_relations;
 	return root;
 }
 
@@ -1498,6 +1508,8 @@ inheritance_planner(PlannerInfo *root)
 		if (IS_DUMMY_PATH(subpath))
 			continue;
 
+		root->yb_num_referenced_relations++;
+
 		/*
 		 * Add the current parent's RT index to the partitioned_relids set if
 		 * we're creating the ModifyTable path for a partitioned root table.
diff --git a/src/postgres/src/backend/parser/gram.y b/src/postgres/src/backend/parser/gram.y
index 21a1f199a1..788f0c0e4f 100644
--- a/src/postgres/src/backend/parser/gram.y
+++ b/src/postgres/src/backend/parser/gram.y
@@ -9511,7 +9511,6 @@ RenameStmt: ALTER AGGREGATE aggregate_with_argtypes RENAME TO name
 				}
 			| ALTER INDEX qualified_name RENAME TO name
 				{
-					parser_ybc_signal_unsupported(@1, "ALTER INDEX", 1130);
 					RenameStmt *n = makeNode(RenameStmt);
 					n->renameType = OBJECT_INDEX;
 					n->relation = $3;
@@ -9522,7 +9521,6 @@ RenameStmt: ALTER AGGREGATE aggregate_with_argtypes RENAME TO name
 				}
 			| ALTER INDEX IF_P EXISTS qualified_name RENAME TO name
 				{
-					parser_ybc_signal_unsupported(@1, "ALTER INDEX", 1130);
 					RenameStmt *n = makeNode(RenameStmt);
 					n->renameType = OBJECT_INDEX;
 					n->relation = $5;
diff --git a/src/postgres/src/backend/parser/parse_type.c b/src/postgres/src/backend/parser/parse_type.c
index d959b6122a..2c0b93c030 100644
--- a/src/postgres/src/backend/parser/parse_type.c
+++ b/src/postgres/src/backend/parser/parse_type.c
@@ -26,6 +26,7 @@
 #include "utils/lsyscache.h"
 #include "utils/syscache.h"
 
+#include "pg_yb_utils.h"
 
 static int32 typenameTypeMod(ParseState *pstate, const TypeName *typeName,
 				Type typ);
@@ -497,10 +498,18 @@ LookupCollation(ParseState *pstate, List *collnames, int location)
 {
 	Oid			colloid;
 	ParseCallbackState pcbstate;
+	char	   *schemaname;
+	char	   *collation_name;
 
 	if (pstate)
 		setup_parser_errposition_callback(&pcbstate, pstate, location);
 
+	DeconstructQualifiedName(collnames, &schemaname, &collation_name);
+	if (IsYugaByteEnabled() && YBIsDeprecatedLibICUCollation(collation_name))
+		ereport(WARNING,
+				(errcode(ERRCODE_INVALID_OBJECT_DEFINITION),
+				 errmsg("collation is deprecated")));
+
 	colloid = get_collation_oid(collnames, false);
 
 	if (pstate)
diff --git a/src/postgres/src/backend/parser/parse_utilcmd.c b/src/postgres/src/backend/parser/parse_utilcmd.c
index caa9b0326d..fcf50bcf75 100644
--- a/src/postgres/src/backend/parser/parse_utilcmd.c
+++ b/src/postgres/src/backend/parser/parse_utilcmd.c
@@ -3812,6 +3812,8 @@ transformColumnType(CreateStmtContext *cxt, ColumnDef *column)
 	 * including any collation spec that might be present.
 	 */
 	Type		ctype = typenameType(cxt->pstate, column->typeName, NULL);
+	char	   *schemaname;
+	char	   *collation_name;
 
 	if (column->collClause)
 	{
@@ -3828,6 +3830,15 @@ transformColumnType(CreateStmtContext *cxt, ColumnDef *column)
 							format_type_be(HeapTupleGetOid(ctype))),
 					 parser_errposition(cxt->pstate,
 										column->collClause->location)));
+
+		/* deconstruct the name list */
+		DeconstructQualifiedName(column->collClause->collname, &schemaname, &collation_name);
+		if (IsYugaByteEnabled() && YBIsDeprecatedLibICUCollation(collation_name))
+			ereport(ERROR,
+					(errcode(ERRCODE_INDETERMINATE_COLLATION),
+					 errmsg("collation no longer supported and will be dropped in next release"),
+					 parser_errposition(cxt->pstate,
+										column->collClause->location)));
 	}
 
 	ReleaseSysCache(ctype);
diff --git a/src/postgres/src/backend/utils/cache/plancache.c b/src/postgres/src/backend/utils/cache/plancache.c
index 79467e24c3..30e8b0ea31 100644
--- a/src/postgres/src/backend/utils/cache/plancache.c
+++ b/src/postgres/src/backend/utils/cache/plancache.c
@@ -108,6 +108,17 @@ static void PlanCacheRelCallback(Datum arg, Oid relid);
 static void PlanCacheFuncCallback(Datum arg, int cacheid, uint32 hashvalue);
 static void PlanCacheSysCallback(Datum arg, int cacheid, uint32 hashvalue);
 
+/*
+ * For prepared statements, generate custom plans for at least the first 5 runs
+ * (arbitrary)
+ */
+int yb_test_planner_custom_plan_threshold = 5;
+
+/*
+ * Prefer custom plan over generic plan for prepared statement if more
+ * partitions are pruned using a custom plan.
+ */
+bool enable_choose_custom_plan_for_partition_pruning = true;
 
 /*
  * InitPlanCache: initialize module during InitPostgres.
@@ -1044,8 +1055,11 @@ choose_custom_plan(CachedPlanSource *plansource, ParamListInfo boundParams)
 	if (plansource->cursor_options & CURSOR_OPT_CUSTOM_PLAN)
 		return true;
 
-	/* Generate custom plans until we have done at least 5 (arbitrary) */
-	if (plansource->num_custom_plans < 5)
+	/*
+	 * Generate custom plans until we have done at least
+	 * 'yb_test_planner_custom_plan_threshold' runs.
+	 */
+	if (plansource->num_custom_plans < yb_test_planner_custom_plan_threshold)
 		return true;
 
 	/* For single row modify operations, use a custom plan so as to push down
@@ -1064,6 +1078,16 @@ choose_custom_plan(CachedPlanSource *plansource, ParamListInfo boundParams)
 
 	avg_custom_cost = plansource->total_custom_cost / plansource->num_custom_plans;
 
+	/*
+	 * If generic plan is present, then choose custom plan if partition pruning
+	 * or constraint exclusion has pruned more relations for custom plan over
+	 * generic plan.
+	 */
+	if (enable_choose_custom_plan_for_partition_pruning && plansource->gplan &&
+		plansource->yb_custom_max_num_referenced_rels <
+			plansource->yb_generic_num_referenced_rels)
+		return true;
+
 	/*
 	 * Prefer generic plan if it's less expensive than the average custom
 	 * plan.  (Because we include a charge for cost of planning in the
@@ -1080,6 +1104,23 @@ choose_custom_plan(CachedPlanSource *plansource, ParamListInfo boundParams)
 	return true;
 }
 
+/*
+ * num_referenced_relations: Return number of relations referenced by a plan.
+ */
+static int
+num_referenced_relations(CachedPlan *plan)
+{
+	ListCell   *lc1;
+	int nrelations = 0;
+	foreach(lc1, plan->stmt_list)
+	{
+		PlannedStmt *plannedstmt = lfirst_node(PlannedStmt, lc1);
+		nrelations += plannedstmt->yb_num_referenced_relations;
+	}
+
+	return nrelations;
+}
+
 /*
  * cached_plan_cost: calculate estimated cost of a plan
  *
@@ -1205,6 +1246,8 @@ GetCachedPlan(CachedPlanSource *plansource, ParamListInfo boundParams,
 			}
 			/* Update generic_cost whenever we make a new generic plan */
 			plansource->generic_cost = cached_plan_cost(plan, false);
+			plansource->yb_generic_num_referenced_rels =
+				num_referenced_relations(plan);
 
 			/*
 			 * If, based on the now-known value of generic_cost, we'd not have
@@ -1235,6 +1278,20 @@ GetCachedPlan(CachedPlanSource *plansource, ParamListInfo boundParams,
 		{
 			plansource->total_custom_cost += cached_plan_cost(plan, true);
 			plansource->num_custom_plans++;
+
+			/*
+			 * Store the maximum number of relations referenced across all
+			 * the runs using custom plan. In Yugabyte clusters, higher the
+			 * number of relations referenced by a plan, higher the number
+			 * of RPCs required to fetch the data across these relations. This
+			 * mostly comes into play when using partitioned tables, where
+			 * the number of pruned partitions can be a huge performance
+			 * factor.
+			 */
+			int nrelations = num_referenced_relations(plan);
+			if (plansource->num_custom_plans == 1 ||
+				plansource->yb_custom_max_num_referenced_rels < nrelations)
+				plansource->yb_custom_max_num_referenced_rels = nrelations;
 		}
 	}
 
diff --git a/src/postgres/src/backend/utils/init/miscinit.c b/src/postgres/src/backend/utils/init/miscinit.c
index 36e2fe586a..05a02c41bb 100644
--- a/src/postgres/src/backend/utils/init/miscinit.c
+++ b/src/postgres/src/backend/utils/init/miscinit.c
@@ -52,6 +52,7 @@
 #include "utils/syscache.h"
 #include "utils/varlena.h"
 
+#include "pg_yb_utils.h"
 
 #define DIRECTORY_LOCK_FILE		"postmaster.pid"
 
@@ -716,8 +717,14 @@ SetSessionAuthorization(Oid userid, bool is_superuser)
 	/* Must have authenticated already, else can't make permission check */
 	AssertState(OidIsValid(AuthenticatedUserId));
 
-	if (userid != AuthenticatedUserId &&
-		!AuthenticatedUserIsSuperuser)
+	if ((userid != AuthenticatedUserId && !AuthenticatedUserIsSuperuser) &&
+		/*
+		* For YB Managed case, throw an error if:
+		* 1. Caller is not a yb_db_admin member
+		* 2. Caller is trying to set itself as yb_db_admin member or superuser.
+		*/
+		(!IsYbDbAdminUserNosuper(AuthenticatedUserId) ||
+		(IsYbDbAdminUserNosuper(AuthenticatedUserId) && superuser_arg(userid))))
 		ereport(ERROR,
 				(errcode(ERRCODE_INSUFFICIENT_PRIVILEGE),
 				 errmsg("permission denied to set session authorization")));
diff --git a/src/postgres/src/backend/utils/misc/guc.c b/src/postgres/src/backend/utils/misc/guc.c
index 0abd6e14dd..b5a16db67b 100644
--- a/src/postgres/src/backend/utils/misc/guc.c
+++ b/src/postgres/src/backend/utils/misc/guc.c
@@ -2053,7 +2053,19 @@ static struct config_bool ConfigureNamesBool[] =
 		&yb_enable_upsert_mode,
 		false,
 		NULL, NULL, NULL
-    },
+	},
+
+	{
+		{"yb_planner_custom_plan_for_partition_pruning", PGC_USERSET, CLIENT_CONN_STATEMENT,
+			gettext_noop("If enabled, choose custom plan over generic plan "
+						 " for prepared statements based on the number of "
+						 "partition pruned."),
+			NULL
+		},
+		&enable_choose_custom_plan_for_partition_pruning,
+		true,
+		NULL, NULL, NULL
+	},
 
 	/* End-of-list marker */
 	{
@@ -3358,6 +3370,18 @@ static struct config_int ConfigureNamesInt[] =
 		NULL, NULL, NULL
 	},
 
+	{
+		{"yb_test_planner_custom_plan_threshold", PGC_USERSET, QUERY_TUNING,
+			gettext_noop("The number of times to force custom plan generation "
+						 "for prepared statements before considering a "
+						 "generic plan."),
+			NULL
+		},
+		&yb_test_planner_custom_plan_threshold,
+		5, 1, INT_MAX,
+		NULL, NULL, NULL
+	},
+
 	/* End-of-list marker */
 	{
 		{NULL, 0, 0, NULL, NULL}, NULL, 0, 0, 0, NULL, NULL, NULL
@@ -8183,7 +8207,7 @@ ExecSetVariableStmt(VariableSetStmt *stmt, bool isTopLevel)
 			 i < sizeof(YbDbAdminVariables) / sizeof(YbDbAdminVariables[0]);
 			 i++)
 		{
-			if (strcmp(YbDbAdminVariables[i], stmt->name) == 0)
+			if (stmt->name && strcmp(YbDbAdminVariables[i], stmt->name) == 0)
 			{
 				YbDbAdminCanSet = true;
 				break;
diff --git a/src/postgres/src/backend/utils/misc/pg_yb_utils.c b/src/postgres/src/backend/utils/misc/pg_yb_utils.c
index 4b49c6ac98..fe09672d6c 100644
--- a/src/postgres/src/backend/utils/misc/pg_yb_utils.c
+++ b/src/postgres/src/backend/utils/misc/pg_yb_utils.c
@@ -1517,6 +1517,15 @@ yb_servers(PG_FUNCTION_ARGS)
   SRF_RETURN_DONE(funcctx);
 }
 
+bool YBIsDeprecatedLibICUCollation(const char* collation_name)
+{
+	if (strcmp(collation_name, "nds-x-icu") == 0 ||
+		strcmp(collation_name, "nds-DE-x-icu") == 0 ||
+		strcmp(collation_name, "nds-NL-x-icu") == 0)
+		return true;
+	return false;
+}
+
 bool YBIsSupportedLibcLocale(const char *localebuf) {
 	/*
 	 * For libc mode, Yugabyte only supports the basic locales.
@@ -2127,3 +2136,7 @@ Oid YbGetStorageRelid(Relation relation) {
 bool IsYbDbAdminUser(Oid member) {
 	return IsYugaByteEnabled() && has_privs_of_role(member, DEFAULT_ROLE_YB_DB_ADMIN);
 }
+
+bool IsYbDbAdminUserNosuper(Oid member) {
+	return IsYugaByteEnabled() && is_member_of_role_nosuper(member, DEFAULT_ROLE_YB_DB_ADMIN);
+}
diff --git a/src/postgres/src/bin/pg_dump/pg_dump.c b/src/postgres/src/bin/pg_dump/pg_dump.c
index a31e5c5938..66c80e7c13 100644
--- a/src/postgres/src/bin/pg_dump/pg_dump.c
+++ b/src/postgres/src/bin/pg_dump/pg_dump.c
@@ -1183,6 +1183,15 @@ setup_connection(Archive *AH, const char *dumpencoding,
 		ExecuteSqlStatement(AH, "SET yb_format_funcs_include_yb_metadata = true");
 	}
 
+	/*
+	 * Hack to avoid issue #12251 which fails if we perform "BEGIN" followed by
+	 * "SET TRANSACTION ISOLATION LEVEL" when yb_enable_read_committed_isolation
+	 * is true.
+	 *
+	 * TODO(Piyush): Remove this hack once the issue is fixed properly
+	 */
+	ExecuteSqlStatement(AH, "SET DEFAULT_TRANSACTION_ISOLATION TO 'repeatable read'");
+
 	/*
 	 * Start transaction-snapshot mode transaction to dump consistent data.
 	 */
diff --git a/src/postgres/src/include/access/xact.h b/src/postgres/src/include/access/xact.h
index a8b46f96a2..5581783043 100644
--- a/src/postgres/src/include/access/xact.h
+++ b/src/postgres/src/include/access/xact.h
@@ -378,6 +378,7 @@ extern const char* GetCurrentTransactionName(void);
 extern bool TransactionIdIsCurrentTransactionId(TransactionId xid);
 extern void CommandCounterIncrement(void);
 extern void ForceSyncCommit(void);
+extern int YBGetEffectivePggateIsolationLevel();
 extern void YBInitializeTransaction(void);
 extern void YBResetTransactionReadPoint(void);
 extern void YBRestartReadPoint(void);
diff --git a/src/postgres/src/include/commands/trigger.h b/src/postgres/src/include/commands/trigger.h
index 6aae927573..f829b87b27 100644
--- a/src/postgres/src/include/commands/trigger.h
+++ b/src/postgres/src/include/commands/trigger.h
@@ -161,6 +161,11 @@ extern ObjectAddress CreateTrigger(CreateTrigStmt *stmt, const char *queryString
 			  Oid relOid, Oid refRelOid, Oid constraintOid, Oid indexOid,
 			  Oid funcoid, Oid parentTriggerOid, Node *whenClause,
 			  bool isInternal, bool in_partition);
+extern ObjectAddress CreateTriggerFiringOn(CreateTrigStmt *stmt, const char *queryString,
+					  Oid relOid, Oid refRelOid, Oid constraintOid,
+					  Oid indexOid, Oid funcoid, Oid parentTriggerOid,
+					  Node *whenClause, bool isInternal, bool in_partition,
+					  char trigger_fires_when);
 
 extern void RemoveTriggerById(Oid trigOid);
 extern Oid	get_trigger_oid(Oid relid, const char *name, bool missing_ok);
diff --git a/src/postgres/src/include/executor/ybcModifyTable.h b/src/postgres/src/include/executor/ybcModifyTable.h
index d60925b879..2781d3c55a 100644
--- a/src/postgres/src/include/executor/ybcModifyTable.h
+++ b/src/postgres/src/include/executor/ybcModifyTable.h
@@ -55,6 +55,10 @@ typedef void (*yb_bind_for_write_function) (YBCPgStatement stmt,
 /*
  * Insert data into YugaByte table.
  * This function is equivalent to "heap_insert", but it sends data to DocDB (YugaByte storage).
+ *
+ * ybctid argument can be supplied to keep it consistent across shared inserts.
+ * If non-zero, it will be used instead of generation, otherwise it will be set
+ * to the generated value.
  */
 extern Oid YBCHeapInsert(TupleTableSlot *slot,
                          HeapTuple tuple,
@@ -62,11 +66,16 @@ extern Oid YBCHeapInsert(TupleTableSlot *slot,
 extern Oid YBCHeapInsertForDb(Oid dboid,
                               TupleTableSlot *slot,
                               HeapTuple tuple,
-                              EState *estate);
+                              EState *estate,
+                              Datum *ybctid);
 
 /*
  * Insert a tuple into a YugaByte table. Will execute within a distributed
  * transaction if the table is transactional (YSQL default).
+ *
+ * ybctid argument can be supplied to keep it consistent across shared inserts.
+ * If non-zero, it will be used instead of generation, otherwise it will be set
+ * to the generated value.
  */
 extern Oid YBCExecuteInsert(Relation rel,
                             TupleDesc tupleDesc,
@@ -74,11 +83,16 @@ extern Oid YBCExecuteInsert(Relation rel,
 extern Oid YBCExecuteInsertForDb(Oid dboid,
                                  Relation rel,
                                  TupleDesc tupleDesc,
-                                 HeapTuple tuple);
+                                 HeapTuple tuple,
+                                 Datum *ybctid);
 
 /*
  * Execute the insert outside of a transaction.
  * Assumes the caller checked that it is safe to do so.
+ *
+ * ybctid argument can be supplied to keep it consistent across shared inserts.
+ * If non-zero, it will be used instead of generation, otherwise it will be set
+ * to the generated value.
  */
 extern Oid YBCExecuteNonTxnInsert(Relation rel,
                                   TupleDesc tupleDesc,
@@ -86,7 +100,8 @@ extern Oid YBCExecuteNonTxnInsert(Relation rel,
 extern Oid YBCExecuteNonTxnInsertForDb(Oid dboid,
                                        Relation rel,
                                        TupleDesc tupleDesc,
-                                       HeapTuple tuple);
+                                       HeapTuple tuple,
+                                       Datum *ybctid);
 
 /*
  * Insert a tuple into the an index's backing YugaByte index table.
diff --git a/src/postgres/src/include/nodes/plannodes.h b/src/postgres/src/include/nodes/plannodes.h
index 9c251b0aff..fcdcaaf298 100644
--- a/src/postgres/src/include/nodes/plannodes.h
+++ b/src/postgres/src/include/nodes/plannodes.h
@@ -99,6 +99,14 @@ typedef struct PlannedStmt
 	/* statement location in source string (copied from Query) */
 	int			stmt_location;	/* start location, or -1 if unknown */
 	int			stmt_len;		/* length in bytes; 0 means "rest of string" */
+
+	/* YB specific fields */
+
+	/*
+	 * Number of relations that are still referenced by the plan after
+	 * constraint exclusion and partition pruning.
+	 */
+	int		yb_num_referenced_relations;
 } PlannedStmt;
 
 /* macro for fetching the Plan associated with a SubPlan node */
diff --git a/src/postgres/src/include/nodes/relation.h b/src/postgres/src/include/nodes/relation.h
index 59c1d0abce..96fc60b8e2 100644
--- a/src/postgres/src/include/nodes/relation.h
+++ b/src/postgres/src/include/nodes/relation.h
@@ -343,6 +343,12 @@ typedef struct PlannerInfo
 
 	/* Does this query modify any partition key columns? */
 	bool		partColsUpdated;
+
+	/*
+	 * Number of relations that are still referenced by the plan after
+	 * constraint exclusion and partition pruning.
+	 */
+	int     yb_num_referenced_relations;
 } PlannerInfo;
 
 
diff --git a/src/postgres/src/include/pg_yb_utils.h b/src/postgres/src/include/pg_yb_utils.h
index 6e4c2bbd62..d91e39d826 100644
--- a/src/postgres/src/include/pg_yb_utils.h
+++ b/src/postgres/src/include/pg_yb_utils.h
@@ -501,6 +501,8 @@ YbGetTableDescAndProps(Oid table_oid,
  */
 bool YBIsSupportedLibcLocale(const char *localebuf);
 
+bool YBIsDeprecatedLibICUCollation(const char* collation_name);
+
 void YBTestFailDdlIfRequested();
 
 char *YBDetailSorted(char *input);
@@ -577,4 +579,10 @@ Oid YbGetStorageRelid(Relation relation);
  */
 bool IsYbDbAdminUser(Oid member);
 
+/*
+ * Check whether the user ID is of a user who has the yb_db_admin role
+ * (excluding superusers).
+ */
+bool IsYbDbAdminUserNosuper(Oid member);
+
 #endif /* PG_YB_UTILS_H */
diff --git a/src/postgres/src/include/utils/plancache.h b/src/postgres/src/include/utils/plancache.h
index 4178a2f307..8a68e1f649 100644
--- a/src/postgres/src/include/utils/plancache.h
+++ b/src/postgres/src/include/utils/plancache.h
@@ -19,6 +19,18 @@
 #include "nodes/params.h"
 #include "utils/queryenvironment.h"
 
+/*
+ * GUC variable to control how many times a custom plan is chosen over
+ * a generic plan unconditionally. See guc.c for details.
+ */
+extern int yb_test_planner_custom_plan_threshold;
+
+/*
+ * GUC variable to control whether to prefer a custom plan over a generic
+ * plan based on the number of partitions pruned.
+ */
+extern bool enable_choose_custom_plan_for_partition_pruning;
+
 /* Forward declaration, to avoid including parsenodes.h here */
 struct RawStmt;
 
@@ -116,6 +128,12 @@ typedef struct CachedPlanSource
 	double		total_custom_cost;	/* total cost of custom plans so far */
 	int			num_custom_plans;	/* number of plans included in total */
 	bool 		usesPostgresRel; /* Does this plan use pg relations */
+	int			yb_generic_num_referenced_rels; /* Num rels referenced by
+												 * generic plan */
+	int			yb_custom_max_num_referenced_rels; /* Max number of relations
+													* referenced by a custom
+													* plan */
+
 } CachedPlanSource;
 
 /*
diff --git a/src/postgres/src/test/isolation/isolationtester.c b/src/postgres/src/test/isolation/isolationtester.c
index 86697295bc..a072a17bdd 100644
--- a/src/postgres/src/test/isolation/isolationtester.c
+++ b/src/postgres/src/test/isolation/isolationtester.c
@@ -42,6 +42,10 @@ static void run_permutation(TestSpec *testspec, int nsteps, Step **steps);
 
 #define STEP_NONBLOCK	0x1		/* return 0 as soon as cmd waits for a lock */
 #define STEP_RETRY		0x2		/* this is a retry of a previously-waiting cmd */
+
+/* This is YB specific logic. See usage for description */
+#define YB_NUM_SECONDS_TO_WAIT_TO_ASSUME_SESSION_BLOCKED 4
+
 static bool try_complete_step(Step *step, int flags);
 
 static int	step_qsort_cmp(const void *a, const void *b);
@@ -767,15 +771,15 @@ try_complete_step(Step *step, int flags)
 
 			/* Yugabyte specific logic:
 			 *   Since we don't use pg_locks, we can't determine if a session is blocked on another
-			 *   session using the PREP_WAITING function above. So, we instead assume that being blocked
-			 *   for >= 2 second means the session is waiting on another session.
+			 *   session using the PREP_WAITING function above. So, we instead assume that waiting for
+			 *   for >= 2 second means the session is blocked on another session.
 			 *
 			 *   This is not a perfect check but good enough for now.
 			 *
 			 *   TODO(Piyush): Replace this by a deterministic check when pessimistic locking is
 			 *   implemented and wait queue information is exposed via Pg.
 			 */
-			if (td > 2 * USECS_PER_SEC && !canceled) {
+			if (td > YB_NUM_SECONDS_TO_WAIT_TO_ASSUME_SESSION_BLOCKED * USECS_PER_SEC && !canceled) {
 					if (!(flags & STEP_RETRY))
 						printf("step %s: %s <waiting ...>\n", step->name, step->sql);
 					return true;
diff --git a/src/postgres/src/test/isolation/yb_pg_isolation_schedule b/src/postgres/src/test/isolation/yb_pg_isolation_schedule
index 6407469278..d38deb8a41 100644
--- a/src/postgres/src/test/isolation/yb_pg_isolation_schedule
+++ b/src/postgres/src/test/isolation/yb_pg_isolation_schedule
@@ -1,3 +1,4 @@
+# Tests related to READ COMMITTED isolation
 test: yb_pg_eval-plan-qual
 test: yb_read_committed_update_and_explicit_locking
 test: yb_read_committed_insert
@@ -16,3 +17,15 @@ test: yb-skip-locked-after-update
 #    inherit it back from nested sub txns once they are removed.
 # 2. Test to ensure clean-up of partial intents using savepoints.
 # 3. Test all skip locked specs with REPEATABLE READ isolation as well.
+
+test: update-locked-tuple
+test: delete-abort-savept
+test: fk-deadlock
+test: fk-deadlock2
+test: insert-conflict-do-nothing
+test: insert-conflict-do-update-2
+test: insert-conflict-do-update
+test: multixact-no-deadlock
+test: read-only-anomaly
+test: nowait-2
+test: nowait-3
\ No newline at end of file
diff --git a/src/postgres/src/test/regress/expected/triggers.out b/src/postgres/src/test/regress/expected/triggers.out
index 32b51f0532..466363ae32 100644
--- a/src/postgres/src/test/regress/expected/triggers.out
+++ b/src/postgres/src/test/regress/expected/triggers.out
@@ -2312,6 +2312,62 @@ select tgrelid::regclass, count(*) from pg_trigger
 (5 rows)
 
 drop table trg_clone;
+-- Verify that firing state propagates correctly
+CREATE TABLE trgfire (i int) PARTITION BY RANGE (i);
+CREATE TABLE trgfire1 PARTITION OF trgfire FOR VALUES FROM (1) TO (10);
+CREATE OR REPLACE FUNCTION tgf() RETURNS trigger LANGUAGE plpgsql
+  AS $$ begin raise exception 'except'; end $$;
+CREATE TRIGGER tg AFTER INSERT ON trgfire FOR EACH ROW EXECUTE FUNCTION tgf();
+INSERT INTO trgfire VALUES (1);
+ERROR:  except
+CONTEXT:  PL/pgSQL function tgf() line 1 at RAISE
+ALTER TABLE trgfire DISABLE TRIGGER tg;
+INSERT INTO trgfire VALUES (1);
+CREATE TABLE trgfire2 PARTITION OF trgfire FOR VALUES FROM (10) TO (20);
+INSERT INTO trgfire VALUES (11);
+CREATE TABLE trgfire3 (LIKE trgfire);
+ALTER TABLE trgfire ATTACH PARTITION trgfire3 FOR VALUES FROM (20) TO (30);
+INSERT INTO trgfire VALUES (21);
+CREATE TABLE trgfire4 PARTITION OF trgfire FOR VALUES FROM (30) TO (40) PARTITION BY LIST (i);
+CREATE TABLE trgfire4_30 PARTITION OF trgfire4 FOR VALUES IN (30);
+INSERT INTO trgfire VALUES (30);
+CREATE TABLE trgfire5 (LIKE trgfire) PARTITION BY LIST (i);
+CREATE TABLE trgfire5_40 PARTITION OF trgfire5 FOR VALUES IN (40);
+ALTER TABLE trgfire ATTACH PARTITION trgfire5 FOR VALUES FROM (40) TO (50);
+INSERT INTO trgfire VALUES (40);
+SELECT tgrelid::regclass, tgenabled FROM pg_trigger
+  WHERE tgrelid::regclass IN (SELECT oid from pg_class where relname LIKE 'trgfire%')
+  ORDER BY tgrelid::regclass::text;
+   tgrelid   | tgenabled 
+-------------+-----------
+ trgfire     | D
+ trgfire1    | D
+ trgfire2    | D
+ trgfire3    | D
+ trgfire4    | D
+ trgfire4_30 | D
+ trgfire5    | D
+ trgfire5_40 | D
+(8 rows)
+
+ALTER TABLE trgfire ENABLE TRIGGER tg;
+INSERT INTO trgfire VALUES (1);
+ERROR:  except
+CONTEXT:  PL/pgSQL function tgf() line 1 at RAISE
+INSERT INTO trgfire VALUES (11);
+ERROR:  except
+CONTEXT:  PL/pgSQL function tgf() line 1 at RAISE
+INSERT INTO trgfire VALUES (21);
+ERROR:  except
+CONTEXT:  PL/pgSQL function tgf() line 1 at RAISE
+INSERT INTO trgfire VALUES (30);
+ERROR:  except
+CONTEXT:  PL/pgSQL function tgf() line 1 at RAISE
+INSERT INTO trgfire VALUES (40);
+ERROR:  except
+CONTEXT:  PL/pgSQL function tgf() line 1 at RAISE
+DROP TABLE trgfire;
+DROP FUNCTION tgf();
 --
 -- Test the interaction between transition tables and both kinds of
 -- inheritance.  We'll dump the contents of the transition tables in a
diff --git a/src/postgres/src/test/regress/expected/yb_alter_table.out b/src/postgres/src/test/regress/expected/yb_alter_table.out
new file mode 100644
index 0000000000..0ee97bd597
--- /dev/null
+++ b/src/postgres/src/test/regress/expected/yb_alter_table.out
@@ -0,0 +1,24 @@
+---
+--- Verify renaming on temp tables
+---
+CREATE TEMP TABLE temp_table(a int primary key, b int);
+CREATE INDEX temp_table_b_idx ON temp_table(b);
+ALTER INDEX temp_table_pkey RENAME TO temp_table_pkey_new;
+ALTER INDEX temp_table_b_idx RENAME TO temp_table_b_idx_new;
+---
+--- Verify yb_db_admin role can ALTER table
+---
+CREATE TABLE table_other(a int, b int);
+CREATE INDEX index_table_other ON table_other(a);
+SET SESSION AUTHORIZATION yb_db_admin;
+ALTER TABLE table_other RENAME to table_new;
+ALTER TABLE table_new OWNER TO regress_alter_table_user1;
+ALTER TABLE pg_database RENAME TO test; -- should fail
+ERROR:  permission denied: "pg_database" is a system catalog
+ALTER TABLE pg_tablespace OWNER TO regress_alter_table_user1; -- should fail
+ERROR:  permission denied: "pg_tablespace" is a system catalog
+---
+--- Verify yb_db_admin role can ALTER index
+---
+ALTER INDEX index_table_other RENAME TO index_table_other_new;
+RESET SESSION AUTHORIZATION;
diff --git a/src/postgres/src/test/regress/expected/yb_dependency.out b/src/postgres/src/test/regress/expected/yb_dependency.out
new file mode 100644
index 0000000000..9930899e0e
--- /dev/null
+++ b/src/postgres/src/test/regress/expected/yb_dependency.out
@@ -0,0 +1,28 @@
+--
+-- Validate yb_db_admin can reassign object ownership
+--
+CREATE ROLE regress_user1;
+CREATE ROLE regress_user2;
+CREATE ROLE regress_user3;
+CREATE ROLE non_sys_superuser SUPERUSER;
+GRANT regress_user1 TO regress_user3;
+GRANT regress_user2 TO regress_user3;
+SET SESSION AUTHORIZATION yb_db_admin;
+REASSIGN OWNED BY regress_user1 TO regress_user2;
+REASSIGN OWNED BY regress_user2 TO regress_user1;
+-- should fail, cannot reassign objects owned by superuser
+REASSIGN OWNED BY postgres TO yb_db_admin;
+ERROR:  non-superuser cannot reassign objects from superuser
+-- should fail, assigning from superuser to non-superuser
+REASSIGN OWNED BY non_sys_superuser TO yb_db_admin;
+ERROR:  non-superuser cannot reassign objects from superuser
+-- should fail, user needs privileges on both old and new role
+SET SESSION AUTHORIZATION regress_user1;
+REASSIGN OWNED BY regress_user1 TO regress_user2;
+ERROR:  permission denied to reassign objects
+REASSIGN OWNED BY regress_user2 TO regress_user1;
+ERROR:  permission denied to reassign objects
+-- should succeed, user has privileges on both old and new role
+SET SESSION AUTHORIZATION regress_user3;
+REASSIGN OWNED BY regress_user1 TO regress_user2;
+REASSIGN OWNED BY regress_user2 TO regress_user1;
diff --git a/src/postgres/src/test/regress/expected/yb_event_trigger.out b/src/postgres/src/test/regress/expected/yb_event_trigger.out
index cdfb0216cf..d88f15d6a2 100644
--- a/src/postgres/src/test/regress/expected/yb_event_trigger.out
+++ b/src/postgres/src/test/regress/expected/yb_event_trigger.out
@@ -85,7 +85,8 @@ NOTICE:  Calling trigger foo on ddl_command_end
 NOTICE:  Calling trigger xbc on ddl_command_end
 NOTICE:  Calling trigger ybc on ddl_command_end
 -- Verify that yb_db_admin role can use event triggers.
-ALTER EVENT TRIGGER foo OWNER TO yb_db_admin;
+CREATE ROLE superuser_role SUPERUSER;
+CREATE ROLE non_superuser_role;
 SET SESSION AUTHORIZATION yb_db_admin;
 CREATE EVENT TRIGGER admin_foo ON ddl_command_start EXECUTE PROCEDURE test_event_trigger_foo();
 CREATE EVENT TRIGGER admin_bar ON ddl_command_end EXECUTE PROCEDURE test_event_trigger_bar();
@@ -93,5 +94,8 @@ ALTER EVENT TRIGGER admin_foo DISABLE;
 ALTER EVENT TRIGGER admin_foo ENABLE REPLICA;
 ALTER EVENT TRIGGER admin_foo ENABLE ALWAYS;
 ALTER EVENT TRIGGER admin_foo RENAME TO admin_foo_new;
+ALTER EVENT TRIGGER admin_foo_new OWNER TO superuser_role;
+ALTER EVENT TRIGGER admin_foo_new OWNER TO non_superuser_role;
+ERROR:  permission denied to change owner of event trigger "admin_foo_new"
+HINT:  The owner of an event trigger must be a superuser or yb_db_admin role member.
 DROP EVENT TRIGGER admin_foo_new;
-
diff --git a/src/postgres/src/test/regress/expected/yb_feature_temp.out b/src/postgres/src/test/regress/expected/yb_feature_temp.out
index 62e98ae829..1bd2655ba5 100644
--- a/src/postgres/src/test/regress/expected/yb_feature_temp.out
+++ b/src/postgres/src/test/regress/expected/yb_feature_temp.out
@@ -103,29 +103,18 @@ SELECT * FROM temptest;
 DROP TABLE temptest;
 -- test ON COMMIT DROP
 -- TODO(dmitry) ON COMMIT DROP should be fixed in context of #7926
-BEGIN;
-CREATE TEMP TABLE temptest(col int) ON COMMIT DROP;
-INSERT INTO temptest VALUES (1);
-INSERT INTO temptest VALUES (2);
-SELECT * FROM temptest;
- col
------
-   1
-   2
-(2 rows)
-
-COMMIT;
-ERROR:  Illegal state: Transaction for catalog table write operation 'pg_type' not found
-SELECT * FROM temptest;
-ERROR:  could not open file "base/13281/t2_16964": No such file or directory
-BEGIN;
-CREATE TEMP TABLE temptest(col) ON COMMIT DROP AS SELECT 1;
-ERROR:  relation "temptest" already exists
-SELECT * FROM temptest;
-ERROR:  current transaction is aborted, commands ignored until end of transaction block
-COMMIT;
-SELECT * FROM temptest;
-ERROR:  could not open file "base/13281/t2_16964": No such file or directory
+-- BEGIN;
+-- CREATE TEMP TABLE temptest(col int) ON COMMIT DROP;
+-- INSERT INTO temptest VALUES (1);
+-- INSERT INTO temptest VALUES (2);
+-- SELECT * FROM temptest;
+-- COMMIT;
+-- SELECT * FROM temptest;
+-- BEGIN;
+-- CREATE TEMP TABLE temptest(col) ON COMMIT DROP AS SELECT 1;
+-- SELECT * FROM temptest;
+-- COMMIT;
+-- SELECT * FROM temptest;
 -- ON COMMIT is only allowed for TEMP
 CREATE TABLE temptest(col int) ON COMMIT DELETE ROWS;
 ERROR:  ON COMMIT can only be used on temporary tables
@@ -297,7 +286,6 @@ DROP TABLE test;
 DROP TABLE x;
 -- test temp table deletion
 CREATE TEMP TABLE temptest (col int);
-ERROR:  relation "temptest" already exists
 \c
 SELECT * FROM temptest;
 ERROR:  relation "temptest" does not exist
diff --git a/src/postgres/src/test/regress/expected/yb_function.out b/src/postgres/src/test/regress/expected/yb_function.out
new file mode 100644
index 0000000000..5d727e395a
--- /dev/null
+++ b/src/postgres/src/test/regress/expected/yb_function.out
@@ -0,0 +1,64 @@
+CREATE USER regress_alter_generic_user3;
+CREATE USER regress_alter_generic_user2;
+CREATE USER regress_alter_generic_user1 IN ROLE regress_alter_generic_user3;
+CREATE SCHEMA alt_nsp1;
+GRANT ALL ON SCHEMA alt_nsp1 TO PUBLIC;
+SET search_path = alt_nsp1, public;
+---
+--- Verify yb_db_admin can CREATE and DROP functions
+---
+CREATE FUNCTION other_func(int) RETURNS int LANGUAGE sql
+  AS 'SELECT $1 + 1';
+SET SESSION AUTHORIZATION yb_db_admin;
+CREATE FUNCTION admin_func(int) RETURNS int LANGUAGE sql
+  AS 'SELECT $1 + 1';
+CREATE FUNCTION admin_func_leakproof(int) RETURNS int LANGUAGE sql  -- not allowed
+  LEAKPROOF AS 'SELECT $1 + 1';
+ERROR:  only superuser can define a leakproof function
+CREATE OR REPLACE FUNCTION increment(i integer) RETURNS integer AS $$
+  BEGIN
+    RETURN i + 1;
+  END;
+$$ LANGUAGE plpgsql;
+CREATE FUNCTION language_func() RETURNS uuid  -- C functions aren't allowed
+  LANGUAGE c STRICT PARALLEL SAFE
+  AS '$libdir/uuid-ossp', 'uuid_generate_v1';
+ERROR:  permission denied for language c
+DROP FUNCTION admin_func(int);
+DROP FUNCTION other_func(int);
+DROP FUNCTION language_func(); -- does not exist
+ERROR:  function language_func() does not exist
+RESET SESSION AUTHORIZATION;
+---
+--- Validate yb_db_admin role can ALTER function
+---
+CREATE FUNCTION alt_func1(int) RETURNS int LANGUAGE sql
+  AS 'SELECT $1 + 1';
+SET SESSION AUTHORIZATION yb_db_admin;
+ALTER FUNCTION alt_func1(int) OWNER TO regress_alter_generic_user1;
+ALTER FUNCTION alt_func1(int) RENAME TO func_renamed;
+ALTER FUNCTION func_renamed(int) SET SCHEMA alt_nsp1;
+ALTER FUNCTION func_renamed(int) LEAKPROOF;  -- not allowed
+ERROR:  only superuser can define a leakproof function
+ALTER FUNCTION func_renamed(int) NOT LEAKPROOF;
+-- validate regress_alter_generic_user2 can operate on function
+SET SESSION AUTHORIZATION regress_alter_generic_user1;
+ALTER FUNCTION func_renamed(int) OWNER TO regress_alter_generic_user2;  -- failed (no role membership)
+ERROR:  must be member of role "regress_alter_generic_user2"
+ALTER FUNCTION func_renamed(int) OWNER TO regress_alter_generic_user3;  -- OK
+ALTER FUNCTION func_renamed(int) RENAME TO func_renamed2;
+ALTER FUNCTION func_renamed2(int) SET SCHEMA alt_nsp1;
+ALTER FUNCTION func_renamed2(int) LEAKPROOF;  -- not allowed
+ERROR:  only superuser can define a leakproof function
+ALTER FUNCTION func_renamed2(int) NOT LEAKPROOF;
+---
+--- Clean up
+---
+RESET SESSION AUTHORIZATION;
+DROP SCHEMA alt_nsp1 CASCADE;
+NOTICE:  drop cascades to 2 other objects
+DETAIL:  drop cascades to function func_renamed2(integer)
+drop cascades to function increment(integer)
+DROP USER regress_alter_generic_user1;
+DROP USER regress_alter_generic_user2;
+DROP USER regress_alter_generic_user3;
diff --git a/src/postgres/src/test/regress/expected/yb_index_scan.out b/src/postgres/src/test/regress/expected/yb_index_scan.out
index 555886e5b0..d8ba8df307 100644
--- a/src/postgres/src/test/regress/expected/yb_index_scan.out
+++ b/src/postgres/src/test/regress/expected/yb_index_scan.out
@@ -288,6 +288,21 @@ SELECT * FROM sc_multi_desc WHERE k = 1;
  1 | 10 | 10
 (4 rows)
 
+-- Testing for the case in issue #12481
+CREATE INDEX range_ind ON sc_multi_desc(v ASC, r ASC);
+EXPLAIN SELECT v,r FROM sc_multi_desc WHERE v IN (2,4) and r is null;
+                                     QUERY PLAN                                      
+-------------------------------------------------------------------------------------
+ Index Only Scan using range_ind on sc_multi_desc  (cost=0.00..5.12 rows=10 width=8)
+   Index Cond: ((v = ANY ('{2,4}'::integer[])) AND (r IS NULL))
+(2 rows)
+
+SELECT v,r FROM sc_multi_desc WHERE v IN (2,4) and r is null;
+ v | r 
+---+---
+ 2 |  
+(1 row)
+
 -- Test NULLS last ordering.
 CREATE TABLE sc_desc_nl(h int, r int, v int);
 CREATE INDEX on sc_desc_nl(h HASH, r DESC NULLS LAST);
diff --git a/src/postgres/src/test/regress/expected/yb_partition_prune_plancache.out b/src/postgres/src/test/regress/expected/yb_partition_prune_plancache.out
new file mode 100644
index 0000000000..512cc098d3
--- /dev/null
+++ b/src/postgres/src/test/regress/expected/yb_partition_prune_plancache.out
@@ -0,0 +1,272 @@
+-- Test Setup.
+-- Create a range-partitioning hierarchy with multiple keys and default partitions.
+CREATE TABLE rp (a int, b int, c varchar, d text, PRIMARY KEY(a,b,c)) PARTITION BY RANGE(a);
+CREATE TABLE rp_p1 PARTITION OF rp FOR VALUES FROM (0) TO (100);
+CREATE TABLE rp_sub PARTITION OF rp FOR VALUES FROM (100) TO (200) PARTITION BY RANGE (b, c);
+CREATE TABLE rp_p2 PARTITION OF rp_sub FOR VALUES FROM (0, 'a') TO (100, 'j');
+CREATE TABLE rp_p3 PARTITION OF rp_sub DEFAULT;
+-- Create a list-partitioning hierarchy with multiple keys, NULL partitions and default partitions.
+CREATE TABLE lp (a int, b int, c varchar, d text) PARTITION BY LIST(a);
+CREATE INDEX ON lp(a);
+CREATE TABLE lp_p1 PARTITION OF lp FOR VALUES IN (0, 1, 2, 3, 4, 5);
+CREATE TABLE lp_sub PARTITION OF lp FOR VALUES IN (6, 7, 8, 9, 10) PARTITION BY LIST (b);
+CREATE TABLE lp_p2 PARTITION OF lp_sub FOR VALUES IN (null);
+CREATE TABLE lp_p3 PARTITION OF lp_sub FOR VALUES IN (1, 2);
+-- Create a non-partitioned table.
+CREATE TABLE np (a int, b int, c varchar, d text);
+-- Set the number of times custom plans are chosen over generic plans unconditionally.
+-- This means that the first time a prepared statement is executed, it will always be
+-- executed with a custom plan. Cost comparison between custom and generic plans will
+-- take effect at the second invocation.
+SET yb_test_planner_custom_plan_threshold=1;
+-- Note: The presence of actual values provided to the bound parameters in EXPLAIN
+-- output indicates a custom plan. The presence of symbols like '$1' in the EXPLAIN
+-- output indicates that a generic plan was chosen.
+-- SELECT query where one of the partition key values is a bound parameter and the other is
+-- not a bound parameter.
+PREPARE t1(int) AS SELECT * FROM rp WHERE a=$1 AND b=3 AND c='1';
+EXPLAIN EXECUTE t1(1);
+                                  QUERY PLAN
+-------------------------------------------------------------------------------
+ Append  (cost=0.00..4.12 rows=1 width=72)
+   ->  Index Scan using rp_p1_pkey on rp_p1  (cost=0.00..4.12 rows=1 width=72)
+         Index Cond: ((a = 1) AND (b = 3) AND ((c)::text = '1'::text))
+(3 rows)
+
+EXPLAIN EXECUTE t1(1);
+                                  QUERY PLAN
+-------------------------------------------------------------------------------
+ Append  (cost=0.00..4.12 rows=1 width=72)
+   ->  Index Scan using rp_p1_pkey on rp_p1  (cost=0.00..4.12 rows=1 width=72)
+         Index Cond: ((a = 1) AND (b = 3) AND ((c)::text = '1'::text))
+(3 rows)
+
+-- Turn off favoring custom plan over generic plan based on partition pruning.
+SET yb_planner_custom_plan_for_partition_pruning=false;
+EXPLAIN EXECUTE t1(1);
+                                  QUERY PLAN
+-------------------------------------------------------------------------------
+ Append  (cost=0.00..8.24 rows=2 width=72)
+   Subplans Removed: 1
+   ->  Index Scan using rp_p1_pkey on rp_p1  (cost=0.00..4.12 rows=1 width=72)
+         Index Cond: ((a = $1) AND (b = 3) AND ((c)::text = '1'::text))
+(4 rows)
+
+-- Turn it back on.
+SET yb_planner_custom_plan_for_partition_pruning=true;
+-- UPDATE list partitioned table using a JOIN with a non-partitioned table.
+PREPARE t2(int) AS UPDATE np SET d = 1 FROM lp WHERE lp.a = np.a AND lp.a = $1;
+EXPLAIN EXECUTE t2(1);
+                                            QUERY PLAN
+---------------------------------------------------------------------------------------------------
+ Update on np  (cost=0.00..232.80 rows=1000000 width=140)
+   ->  Nested Loop  (cost=0.00..232.80 rows=1000000 width=140)
+         ->  Seq Scan on np  (cost=0.00..102.50 rows=1000 width=72)
+               Filter: (a = 1)
+         ->  Materialize  (cost=0.00..5.32 rows=10 width=40)
+               ->  Append  (cost=0.00..5.27 rows=10 width=40)
+                     ->  Index Scan using lp_p1_a_idx on lp_p1  (cost=0.00..5.22 rows=10 width=40)
+                           Index Cond: (a = 1)
+(8 rows)
+
+EXPLAIN EXECUTE t2(1);
+                                            QUERY PLAN
+---------------------------------------------------------------------------------------------------
+ Update on np  (cost=0.00..232.80 rows=1000000 width=140)
+   ->  Nested Loop  (cost=0.00..232.80 rows=1000000 width=140)
+         ->  Seq Scan on np  (cost=0.00..102.50 rows=1000 width=72)
+               Filter: (a = 1)
+         ->  Materialize  (cost=0.00..5.32 rows=10 width=40)
+               ->  Append  (cost=0.00..5.27 rows=10 width=40)
+                     ->  Index Scan using lp_p1_a_idx on lp_p1  (cost=0.00..5.22 rows=10 width=40)
+                           Index Cond: (a = 1)
+(8 rows)
+
+-- DELETE range partitioned table using a JOIN with a non-partitioned table.
+PREPARE t3(int) AS DELETE FROM rp USING np WHERE rp.a = np.a AND rp.a = $1;
+EXPLAIN EXECUTE t3(1);
+                                          QUERY PLAN
+----------------------------------------------------------------------------------------------
+ Delete on rp  (cost=0.00..1368.00 rows=100000 width=64)
+   Delete on rp_p1
+   ->  Nested Loop  (cost=0.00..1368.00 rows=100000 width=64)
+         ->  Seq Scan on np  (cost=0.00..102.50 rows=1000 width=36)
+               Filter: (a = 1)
+         ->  Materialize  (cost=0.00..15.75 rows=100 width=36)
+               ->  Index Scan using rp_p1_pkey on rp_p1  (cost=0.00..15.25 rows=100 width=36)
+                     Index Cond: (a = 1)
+(8 rows)
+
+EXPLAIN EXECUTE t3(1);
+                                          QUERY PLAN
+----------------------------------------------------------------------------------------------
+ Delete on rp  (cost=0.00..1368.00 rows=100000 width=64)
+   Delete on rp_p1
+   ->  Nested Loop  (cost=0.00..1368.00 rows=100000 width=64)
+         ->  Seq Scan on np  (cost=0.00..102.50 rows=1000 width=36)
+               Filter: (a = 1)
+         ->  Materialize  (cost=0.00..15.75 rows=100 width=36)
+               ->  Index Scan using rp_p1_pkey on rp_p1  (cost=0.00..15.25 rows=100 width=36)
+                     Index Cond: (a = 1)
+(8 rows)
+
+-- Subquery test where the outer query has a partition key as a bound parameter.
+PREPARE t4 AS SELECT * FROM rp WHERE rp.a = $1 AND rp.b IN (SELECT b FROM np WHERE a < 5) ;
+EXPLAIN EXECUTE t4(1);
+                                       QUERY PLAN
+----------------------------------------------------------------------------------------
+ Hash Join  (cost=109.50..126.07 rows=500 width=72)
+   Hash Cond: (rp_p1.b = np.b)
+   ->  Append  (cost=0.00..15.75 rows=100 width=72)
+         ->  Index Scan using rp_p1_pkey on rp_p1  (cost=0.00..15.25 rows=100 width=72)
+               Index Cond: (a = 1)
+   ->  Hash  (cost=107.00..107.00 rows=200 width=4)
+         ->  HashAggregate  (cost=105.00..107.00 rows=200 width=4)
+               Group Key: np.b
+               ->  Seq Scan on np  (cost=0.00..102.50 rows=1000 width=4)
+                     Filter: (a < 5)
+(10 rows)
+
+EXPLAIN EXECUTE t4(1);
+                                       QUERY PLAN
+----------------------------------------------------------------------------------------
+ Hash Join  (cost=109.50..126.07 rows=500 width=72)
+   Hash Cond: (rp_p1.b = np.b)
+   ->  Append  (cost=0.00..15.75 rows=100 width=72)
+         ->  Index Scan using rp_p1_pkey on rp_p1  (cost=0.00..15.25 rows=100 width=72)
+               Index Cond: (a = 1)
+   ->  Hash  (cost=107.00..107.00 rows=200 width=4)
+         ->  HashAggregate  (cost=105.00..107.00 rows=200 width=4)
+               Group Key: np.b
+               ->  Seq Scan on np  (cost=0.00..102.50 rows=1000 width=4)
+                     Filter: (a < 5)
+(10 rows)
+
+-- Subquery test whether the inner query has a partition key as a bound parameter.
+PREPARE t5(int) AS SELECT * FROM np WHERE np.a = 1 AND np.b IN (SELECT b FROM lp WHERE a=$1 AND c='1');
+EXPLAIN EXECUTE t5(1);
+                                         QUERY PLAN
+--------------------------------------------------------------------------------------------
+ Hash Semi Join  (cost=5.45..116.14 rows=500 width=72)
+   Hash Cond: (np.b = lp_p1.b)
+   ->  Seq Scan on np  (cost=0.00..102.50 rows=1000 width=72)
+         Filter: (a = 1)
+   ->  Hash  (cost=5.33..5.33 rows=10 width=4)
+         ->  Append  (cost=0.00..5.33 rows=10 width=4)
+               ->  Index Scan using lp_p1_a_idx on lp_p1  (cost=0.00..5.28 rows=10 width=4)
+                     Index Cond: (a = 1)
+                     Filter: ((c)::text = '1'::text)
+(9 rows)
+
+EXPLAIN EXECUTE t5(1);
+                                         QUERY PLAN
+--------------------------------------------------------------------------------------------
+ Hash Semi Join  (cost=5.45..116.14 rows=500 width=72)
+   Hash Cond: (np.b = lp_p1.b)
+   ->  Seq Scan on np  (cost=0.00..102.50 rows=1000 width=72)
+         Filter: (a = 1)
+   ->  Hash  (cost=5.33..5.33 rows=10 width=4)
+         ->  Append  (cost=0.00..5.33 rows=10 width=4)
+               ->  Index Scan using lp_p1_a_idx on lp_p1  (cost=0.00..5.28 rows=10 width=4)
+                     Index Cond: (a = 1)
+                     Filter: ((c)::text = '1'::text)
+(9 rows)
+
+-- CTE on partitioned tables.
+PREPARE t6(int) AS
+WITH x AS (UPDATE lp SET d=1 WHERE a = $1 returning a, b, c, d)
+SELECT * FROM rp INNER JOIN x ON rp.a = x.a;
+EXPLAIN EXECUTE t6(1);
+                                        QUERY PLAN
+------------------------------------------------------------------------------------------
+ Hash Join  (cost=5.55..333.30 rows=150 width=144)
+   Hash Cond: (rp_p1.a = x.a)
+   CTE x
+     ->  Update on lp  (cost=0.00..5.22 rows=10 width=200)
+           Update on lp_p1
+           ->  Index Scan using lp_p1_a_idx on lp_p1  (cost=0.00..5.22 rows=10 width=200)
+                 Index Cond: (a = 1)
+   ->  Append  (cost=0.00..315.00 rows=3000 width=72)
+         ->  Seq Scan on rp_p1  (cost=0.00..100.00 rows=1000 width=72)
+         ->  Seq Scan on rp_p2  (cost=0.00..100.00 rows=1000 width=72)
+         ->  Seq Scan on rp_p3  (cost=0.00..100.00 rows=1000 width=72)
+   ->  Hash  (cost=0.20..0.20 rows=10 width=72)
+         ->  CTE Scan on x  (cost=0.00..0.20 rows=10 width=72)
+(13 rows)
+
+EXPLAIN EXECUTE t6(1);
+                                        QUERY PLAN
+------------------------------------------------------------------------------------------
+ Hash Join  (cost=5.55..333.30 rows=150 width=144)
+   Hash Cond: (rp_p1.a = x.a)
+   CTE x
+     ->  Update on lp  (cost=0.00..5.22 rows=10 width=200)
+           Update on lp_p1
+           ->  Index Scan using lp_p1_a_idx on lp_p1  (cost=0.00..5.22 rows=10 width=200)
+                 Index Cond: (a = 1)
+   ->  Append  (cost=0.00..315.00 rows=3000 width=72)
+         ->  Seq Scan on rp_p1  (cost=0.00..100.00 rows=1000 width=72)
+         ->  Seq Scan on rp_p2  (cost=0.00..100.00 rows=1000 width=72)
+         ->  Seq Scan on rp_p3  (cost=0.00..100.00 rows=1000 width=72)
+   ->  Hash  (cost=0.20..0.20 rows=10 width=72)
+         ->  CTE Scan on x  (cost=0.00..0.20 rows=10 width=72)
+(13 rows)
+
+-- Generic plan must be chosen for SELECTs/UPDATEs/DELETEs if the number of partitions
+-- pruned by generic plan is equal to that of custom plan.
+PREPARE t7(char) AS SELECT * FROM rp WHERE a=1 AND b=1 AND c=$1;
+EXPLAIN EXECUTE t7('a');
+                                    QUERY PLAN
+----------------------------------------------------------------------------------
+ Append  (cost=0.00..16.50 rows=100 width=72)
+   ->  Index Scan using rp_p1_pkey on rp_p1  (cost=0.00..16.00 rows=100 width=72)
+         Index Cond: ((a = 1) AND (b = 1))
+         Filter: ((c)::bpchar = 'a'::bpchar)
+(4 rows)
+
+EXPLAIN EXECUTE t7('a');
+                                    QUERY PLAN
+----------------------------------------------------------------------------------
+ Append  (cost=0.00..16.50 rows=100 width=72)
+   ->  Index Scan using rp_p1_pkey on rp_p1  (cost=0.00..16.00 rows=100 width=72)
+         Index Cond: ((a = 1) AND (b = 1))
+         Filter: ((c)::bpchar = $1)
+(4 rows)
+
+PREPARE t8(char) AS UPDATE lp SET d=3 WHERE a=1 AND b=1 AND c=$1;
+EXPLAIN EXECUTE t8('a');
+                                    QUERY PLAN
+----------------------------------------------------------------------------------
+ Update on lp  (cost=0.00..5.33 rows=10 width=200)
+   Update on lp_p1
+   ->  Index Scan using lp_p1_a_idx on lp_p1  (cost=0.00..5.33 rows=10 width=200)
+         Index Cond: (a = 1)
+         Filter: ((b = 1) AND ((c)::bpchar = 'a'::bpchar))
+(5 rows)
+
+EXPLAIN EXECUTE t8('a');
+                                    QUERY PLAN
+----------------------------------------------------------------------------------
+ Update on lp  (cost=0.00..5.33 rows=10 width=200)
+   Update on lp_p1
+   ->  Index Scan using lp_p1_a_idx on lp_p1  (cost=0.00..5.33 rows=10 width=200)
+         Index Cond: (a = 1)
+         Filter: ((b = 1) AND ((c)::bpchar = $1))
+(5 rows)
+
+PREPARE t9(int) AS DELETE FROM np WHERE a < $1;
+EXPLAIN EXECUTE t9(1);
+                          QUERY PLAN
+--------------------------------------------------------------
+ Delete on np  (cost=0.00..102.50 rows=1000 width=32)
+   ->  Seq Scan on np  (cost=0.00..102.50 rows=1000 width=32)
+         Filter: (a < 1)
+(3 rows)
+
+EXPLAIN EXECUTE t9(1);
+                          QUERY PLAN
+--------------------------------------------------------------
+ Delete on np  (cost=0.00..102.50 rows=1000 width=32)
+   ->  Seq Scan on np  (cost=0.00..102.50 rows=1000 width=32)
+         Filter: (a < $1)
+(3 rows)
diff --git a/src/postgres/src/test/regress/expected/yb_pg_alter_generic.out b/src/postgres/src/test/regress/expected/yb_pg_alter_generic.out
index 02fe590b41..68d75d56c2 100644
--- a/src/postgres/src/test/regress/expected/yb_pg_alter_generic.out
+++ b/src/postgres/src/test/regress/expected/yb_pg_alter_generic.out
@@ -289,7 +289,7 @@ CREATE ROLE regress_alter_generic_user5 NOSUPERUSER;
 CREATE OPERATOR FAMILY alt_opf5 USING btree;
 SET ROLE regress_alter_generic_user5;
 ALTER OPERATOR FAMILY alt_opf5 USING btree ADD OPERATOR 1 < (int4, int2), FUNCTION 1 btint42cmp(int4, int2);
-ERROR:  must be superuser to alter an operator family
+ERROR:  must be superuser or yb_extension role member to alter an operator family
 RESET ROLE;
 ERROR:  current transaction is aborted, commands ignored until end of transaction block
 DROP OPERATOR FAMILY alt_opf5 USING btree;
@@ -528,26 +528,13 @@ SELECT nspname, prsname
  alt_nsp2 | alt_ts_prs2
 (3 rows)
 
----
---- Validate yb_db_admin role can ALTER function
----
-CREATE FUNCTION alt_func1(int) RETURNS int LANGUAGE sql
-  AS 'SELECT $1 + 1';
-SET SESSION AUTHORIZATION yb_db_admin;
-ALTER FUNCTION alt_func1(int) OWNER TO regress_alter_generic_user2;
-ALTER FUNCTION alt_func1(int) RENAME TO func_renamed;
-ALTER FUNCTION func_renamed(int) SET SCHEMA alt_nsp1;
--- validate regress_alter_generic_user2 can operate on function
-ALTER FUNCTION func_renamed(int) OWNER TO regress_alter_generic_user1;
-ALTER FUNCTION func_renamed(int) RENAME TO func_renamed2;
-ALTER FUNCTION func_renamed2(int) SET SCHEMA alt_nsp1;
 ---
 --- Cleanup resources
 ---
 RESET SESSION AUTHORIZATION;
 \set VERBOSITY terse \\ -- suppress cascade details
 DROP SCHEMA alt_nsp1 CASCADE;
-NOTICE:  drop cascades to 24 other objects
+NOTICE:  drop cascades to 23 other objects
 DROP SCHEMA alt_nsp2 CASCADE;
 NOTICE:  drop cascades to 5 other objects
 DROP USER regress_alter_generic_user1;
diff --git a/src/postgres/src/test/regress/expected/yb_pg_alter_table.out b/src/postgres/src/test/regress/expected/yb_pg_alter_table.out
index 43388bd220..428f3c9258 100644
--- a/src/postgres/src/test/regress/expected/yb_pg_alter_table.out
+++ b/src/postgres/src/test/regress/expected/yb_pg_alter_table.out
@@ -7,6 +7,73 @@ DROP ROLE IF EXISTS regress_alter_table_user1;
 RESET client_min_messages;
 CREATE USER regress_alter_table_user1;
 --
+-- rename - check on both non-temp and temp tables
+--
+CREATE TABLE attmp (regtable int);
+CREATE TEMP TABLE attmp (attmptable int);
+ALTER TABLE attmp RENAME TO attmp_new;
+SELECT * FROM attmp;
+ regtable 
+----------
+(0 rows)
+
+SELECT * FROM attmp_new;
+ attmptable 
+------------
+(0 rows)
+
+ALTER TABLE attmp RENAME TO attmp_new2;
+SELECT * FROM attmp;		-- should fail
+ERROR:  relation "attmp" does not exist
+LINE 1: SELECT * FROM attmp;
+                      ^
+SELECT * FROM attmp_new;
+ attmptable 
+------------
+(0 rows)
+
+SELECT * FROM attmp_new2;
+ regtable 
+----------
+(0 rows)
+
+DROP TABLE attmp_new;
+DROP TABLE attmp_new2;
+-- check rename of partitioned tables and indexes also
+CREATE TABLE part_attmp (a int primary key) partition by range (a);
+CREATE TABLE part_attmp1 PARTITION OF part_attmp FOR VALUES FROM (0) TO (100);
+ALTER INDEX part_attmp_pkey RENAME TO part_attmp_index;
+ALTER INDEX part_attmp1_pkey RENAME TO part_attmp1_index;
+ALTER TABLE part_attmp RENAME TO part_at2tmp;
+ALTER TABLE part_attmp1 RENAME TO part_at2tmp1;
+SET ROLE regress_alter_table_user1;
+ALTER INDEX part_attmp_index RENAME TO fail;
+ERROR:  must be owner of index part_attmp_index
+ALTER INDEX part_attmp1_index RENAME TO fail;
+ERROR:  must be owner of index part_attmp1_index
+ALTER TABLE part_at2tmp RENAME TO fail;
+ERROR:  must be owner of table part_at2tmp
+ALTER TABLE part_at2tmp1 RENAME TO fail;
+ERROR:  must be owner of table part_at2tmp1
+RESET ROLE;
+DROP TABLE part_at2tmp;
+-- ALTER TABLE ... RENAME on non-table relations
+-- renaming indexes (FIXME: this should probably test the index's functionality)
+ALTER INDEX IF EXISTS __onek_unique1 RENAME TO attmp_onek_unique1;
+NOTICE:  relation "__onek_unique1" does not exist, skipping
+ALTER INDEX IF EXISTS __attmp_onek_unique1 RENAME TO onek_unique1;
+NOTICE:  relation "__attmp_onek_unique1" does not exist, skipping
+ALTER INDEX onek_unique1 RENAME TO attmp_onek_unique1;
+ALTER INDEX attmp_onek_unique1 RENAME TO onek_unique1;
+SET ROLE regress_alter_table_user1;
+ALTER INDEX onek_unique1 RENAME TO fail;  -- permission denied
+ERROR:  must be owner of index onek_unique1
+RESET ROLE;
+-- renaming index should rename constraint as well
+ALTER TABLE onek ADD CONSTRAINT onek_unique1_constraint UNIQUE (unique1);
+ALTER INDEX onek_unique1_constraint RENAME TO onek_unique1_constraint_foo;
+ALTER TABLE onek DROP CONSTRAINT onek_unique1_constraint_foo;
+--
 -- lock levels
 --
 drop type lockmodes;
@@ -200,12 +267,4 @@ LINE 1: ALTER TABLE tt7 NOT OF;
                         ^
 HINT:  See https://github.com/YugaByte/yugabyte-db/issues/1124. Click '+' on the description to raise its priority
 \d tt7
----
---- Verfy yb_db_admin role can ALTER table
----
-CREATE TABLE table_other(a int, b int);
-SET SESSION AUTHORIZATION yb_db_admin;
-ALTER TABLE table_other RENAME to table_new;
-ALTER TABLE table_new OWNER TO regress_alter_table_user1;
-DROP TABLE table_new;
 
diff --git a/src/postgres/src/test/regress/expected/yb_pg_event_trigger.out b/src/postgres/src/test/regress/expected/yb_pg_event_trigger.out
index 6da53fc50f..ea5397c39f 100644
--- a/src/postgres/src/test/regress/expected/yb_pg_event_trigger.out
+++ b/src/postgres/src/test/regress/expected/yb_pg_event_trigger.out
@@ -89,7 +89,7 @@ set role regress_evt_user;
 create event trigger regress_event_trigger_noperms on ddl_command_start
    execute procedure test_event_trigger();
 ERROR:  permission denied to create event trigger "regress_event_trigger_noperms"
-HINT:  Must be superuser to create an event trigger.
+HINT:  Must be superuser or yb_db_admin role member to create an event trigger.
 reset role;
 -- test enabling and disabling
 alter event trigger regress_event_trigger disable;
@@ -142,7 +142,7 @@ NOTICE:  test_event_trigger: ddl_command_end ALTER DEFAULT PRIVILEGES
 -- alter owner to non-superuser should fail
 alter event trigger regress_event_trigger owner to regress_evt_user;
 ERROR:  permission denied to change owner of event trigger "regress_event_trigger"
-HINT:  The owner of an event trigger must be a superuser.
+HINT:  The owner of an event trigger must be a superuser or yb_db_admin role member.
 -- alter owner to superuser should work
 alter role regress_evt_user superuser;
 alter event trigger regress_event_trigger owner to regress_evt_user;
diff --git a/src/postgres/src/test/regress/expected/yb_pg_foreign_data.out b/src/postgres/src/test/regress/expected/yb_pg_foreign_data.out
index 27597dc578..7e26b69e73 100644
--- a/src/postgres/src/test/regress/expected/yb_pg_foreign_data.out
+++ b/src/postgres/src/test/regress/expected/yb_pg_foreign_data.out
@@ -77,7 +77,7 @@ DROP FOREIGN DATA WRAPPER foo;
 SET ROLE regress_test_role;
 CREATE FOREIGN DATA WRAPPER foo; -- ERROR
 ERROR:  permission denied to create foreign-data wrapper "foo"
-HINT:  Must be superuser to create a foreign-data wrapper.
+HINT:  Must be superuser or yb_fdw role member to create a foreign-data wrapper.
 RESET ROLE;
 CREATE FOREIGN DATA WRAPPER foo VALIDATOR postgresql_fdw_validator;
 \dew+
@@ -154,7 +154,7 @@ ERROR:  option "b" provided more than once
 SET ROLE regress_test_role;
 ALTER FOREIGN DATA WRAPPER foo OPTIONS (ADD d '5');         -- ERROR
 ERROR:  permission denied to alter foreign-data wrapper "foo"
-HINT:  Must be superuser to alter a foreign-data wrapper.
+HINT:  Must be superuser or yb_fdw role member to alter a foreign-data wrapper.
 SET ROLE regress_test_role_super;
 ALTER FOREIGN DATA WRAPPER foo OPTIONS (ADD d '5');
 \dew+
@@ -168,13 +168,13 @@ ALTER FOREIGN DATA WRAPPER foo OPTIONS (ADD d '5');
 
 ALTER FOREIGN DATA WRAPPER foo OWNER TO regress_test_role;  -- ERROR
 ERROR:  permission denied to change owner of foreign-data wrapper "foo"
-HINT:  The owner of a foreign-data wrapper must be a superuser.
+HINT:  Must be superuser or yb_fdw role member to change owner of a foreign-data wrapper.
 ALTER FOREIGN DATA WRAPPER foo OWNER TO regress_test_role_super;
 ALTER ROLE regress_test_role_super NOSUPERUSER;
 SET ROLE regress_test_role_super;
 ALTER FOREIGN DATA WRAPPER foo OPTIONS (ADD e '6');         -- ERROR
 ERROR:  permission denied to alter foreign-data wrapper "foo"
-HINT:  Must be superuser to alter a foreign-data wrapper.
+HINT:  Must be superuser or yb_fdw role member to alter a foreign-data wrapper.
 RESET ROLE;
 \dew+
                                                         List of foreign-data wrappers
@@ -1168,13 +1168,13 @@ WARNING:  changing the foreign-data wrapper validator can cause the options for
 SET ROLE regress_unprivileged_role;
 CREATE FOREIGN DATA WRAPPER foobar;                             -- ERROR
 ERROR:  permission denied to create foreign-data wrapper "foobar"
-HINT:  Must be superuser to create a foreign-data wrapper.
+HINT:  Must be superuser or yb_fdw role member to create a foreign-data wrapper.
 ALTER FOREIGN DATA WRAPPER foo OPTIONS (gotcha 'true');         -- ERROR
 ERROR:  permission denied to alter foreign-data wrapper "foo"
-HINT:  Must be superuser to alter a foreign-data wrapper.
+HINT:  Must be superuser or yb_fdw role member to alter a foreign-data wrapper.
 ALTER FOREIGN DATA WRAPPER foo OWNER TO regress_unprivileged_role; -- ERROR
 ERROR:  permission denied to change owner of foreign-data wrapper "foo"
-HINT:  Must be superuser to change owner of a foreign-data wrapper.
+HINT:  Must be superuser or yb_fdw role member to change owner of a foreign-data wrapper.
 DROP FOREIGN DATA WRAPPER foo;                                  -- ERROR
 ERROR:  must be owner of foreign-data wrapper foo
 GRANT USAGE ON FOREIGN DATA WRAPPER foo TO regress_test_role;   -- ERROR
@@ -1201,10 +1201,10 @@ GRANT USAGE ON FOREIGN DATA WRAPPER foo TO regress_unprivileged_role WITH GRANT
 SET ROLE regress_unprivileged_role;
 CREATE FOREIGN DATA WRAPPER foobar;                             -- ERROR
 ERROR:  permission denied to create foreign-data wrapper "foobar"
-HINT:  Must be superuser to create a foreign-data wrapper.
+HINT:  Must be superuser or yb_fdw role member to create a foreign-data wrapper.
 ALTER FOREIGN DATA WRAPPER foo OPTIONS (gotcha 'true');         -- ERROR
 ERROR:  permission denied to alter foreign-data wrapper "foo"
-HINT:  Must be superuser to alter a foreign-data wrapper.
+HINT:  Must be superuser or yb_fdw role member to alter a foreign-data wrapper.
 DROP FOREIGN DATA WRAPPER foo;                                  -- ERROR
 ERROR:  must be owner of foreign-data wrapper foo
 GRANT USAGE ON FOREIGN DATA WRAPPER postgresql TO regress_test_role; -- WARNING
diff --git a/src/postgres/src/test/regress/expected/yb_pg_namespace.out b/src/postgres/src/test/regress/expected/yb_pg_namespace.out
index d9c609d741..7da51ee5a4 100644
--- a/src/postgres/src/test/regress/expected/yb_pg_namespace.out
+++ b/src/postgres/src/test/regress/expected/yb_pg_namespace.out
@@ -78,31 +78,3 @@ SELECT COUNT(*) FROM pg_class WHERE relnamespace =
 -------
      0
 (1 row)
-
--- verify yb_db_admin role can manage schemas like a superuser
-CREATE SCHEMA test_ns_schema_other;
-CREATE ROLE test_regress_user1;
-SET SESSION AUTHORIZATION yb_db_admin;
-ALTER SCHEMA test_ns_schema_other RENAME TO test_ns_schema_other_new;
-ALTER SCHEMA test_ns_schema_other_new OWNER TO test_regress_user1;
-DROP SCHEMA test_ns_schema_other_new;
--- verify that the objects were dropped
-SELECT COUNT(*) FROM pg_class WHERE relnamespace =
-    (SELECT oid FROM pg_namespace WHERE nspname = 'test_ns_schema_other_new');
- count
--------
-     0
-(1 row)
-
-CREATE SCHEMA test_ns_schema_yb_db_admin;
-ALTER SCHEMA test_ns_schema_yb_db_admin RENAME TO test_ns_schema_yb_db_admin_new;
-ALTER SCHEMA test_ns_schema_yb_db_admin_new OWNER TO test_regress_user1;
-DROP SCHEMA test_ns_schema_yb_db_admin_new;
--- verify that the objects were dropped
-SELECT COUNT(*) FROM pg_class WHERE relnamespace =
-    (SELECT oid FROM pg_namespace WHERE nspname = 'test_ns_schema_yb_db_admin_new');
- count
--------
-     0
-(1 row)
-
diff --git a/src/postgres/src/test/regress/expected/yb_pg_roleattributes.out b/src/postgres/src/test/regress/expected/yb_pg_roleattributes.out
index 80db346dc5..570aa5f834 100644
--- a/src/postgres/src/test/regress/expected/yb_pg_roleattributes.out
+++ b/src/postgres/src/test/regress/expected/yb_pg_roleattributes.out
@@ -247,14 +247,3 @@ DROP ROLE regress_test_def_replication;
 DROP ROLE regress_test_replication;
 DROP ROLE regress_test_def_bypassrls;
 DROP ROLE regress_test_bypassrls;
--- verify yb_db_admin role can change bypassrls attribute
-CREATE ROLE regress_test_user1;
-SET SESSION ROLE yb_db_admin;
-ALTER ROLE regress_test_user1 WITH BYPASSRLS;
-CREATE ROLE regress_test_user2 WITH BYPASSRLS;
-ALTER ROLE regress_test_user2 WITH NOBYPASSRLS;
--- clean up
-SET SESSION ROLE yugabyte;
-DROP ROLE regress_test_user1;
-DROP ROLE regress_test_user2;
-
diff --git a/src/postgres/src/test/regress/expected/yb_pg_stat.out b/src/postgres/src/test/regress/expected/yb_pg_stat.out
new file mode 100644
index 0000000000..da022e87d7
--- /dev/null
+++ b/src/postgres/src/test/regress/expected/yb_pg_stat.out
@@ -0,0 +1,667 @@
+-- Test for PG_STAT
+-- test FOR LSM idx_scan in pg_stat_user_indexes
+create table maintable(c1 INT, c2 TEXT, PRIMARY KEY(c1));
+insert into maintable (c1, c2) values (4, 'bob');
+create index maintable_idx on maintable (c2) include (c1);
+/*+IndexOnlyScan(maintable_idx)*/
+select * from maintable where c2='bob';
+ c1 | c2
+----+-----
+  4 | bob
+(1 row)
+
+-- need to sleep for over half a second here since updates to pgstat is hardcoded to 500 milliseconds
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+ indexrelname  | idx_scan
+---------------+----------
+ maintable_idx |        1
+(1 row)
+
+/*+IndexScan(maintable)*/
+select * from maintable where c2='bob';
+ c1 | c2
+----+-----
+  4 | bob
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+ indexrelname  | idx_scan
+---------------+----------
+ maintable_idx |        2
+(1 row)
+
+-- negative case where we don't use index scan
+/*+SeqScan(maintable)*/
+select * from maintable;
+ c1 | c2
+----+-----
+  4 | bob
+(1 row)
+
+select * from maintable where c2='bob';
+ c1 | c2
+----+-----
+  4 | bob
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+ indexrelname  | idx_scan
+---------------+----------
+ maintable_idx |        3
+(1 row)
+
+-- test case for primary key scan
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_pkey';
+  indexrelname  | idx_scan
+----------------+----------
+ maintable_pkey |        0
+(1 row)
+
+/*+IndexScan(maintable)*/
+select * from maintable where c1=4;
+ c1 | c2
+----+-----
+  4 | bob
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+ indexrelname  | idx_scan
+---------------+----------
+ maintable_idx |        3
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_pkey';
+  indexrelname  | idx_scan
+----------------+----------
+ maintable_pkey |        1
+(1 row)
+
+-- test case for transaction abort
+begin;
+/*+IndexScan(maintable)*/
+select * from maintable where c2='bob';
+ c1 | c2
+----+-----
+  4 | bob
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+ indexrelname  | idx_scan
+---------------+----------
+ maintable_idx |        3
+(1 row)
+
+abort;
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+ indexrelname  | idx_scan
+---------------+----------
+ maintable_idx |        4
+(1 row)
+
+-- test for table view
+create view maintable_view as select * from maintable;
+select * from maintable_view where c2='bob';
+ c1 | c2
+----+-----
+  4 | bob
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+ indexrelname  | idx_scan
+---------------+----------
+ maintable_idx |        5
+(1 row)
+
+-- test for materialized table view
+create materialized view materialized_maintable_view as select * from maintable where c2='bob';
+select * from materialized_maintable_view where c2='bob';
+ c1 | c2
+----+-----
+  4 | bob
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+ indexrelname  | idx_scan
+---------------+----------
+ maintable_idx |        6
+(1 row)
+
+-- test for refreshing materialized table view
+insert into maintable (c1, c2) values (6, 'sol');
+/*+IndexScan(maintable) IndexScan(materialized_maintable_view)*/
+refresh materialized view materialized_maintable_view;
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+ indexrelname  | idx_scan
+---------------+----------
+ maintable_idx |        7
+(1 row)
+
+-- test for refreshing materialized table view with index
+create index materialized_view_idx on materialized_maintable_view (c2) include (c1);
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='materialized_view_idx';
+     indexrelname      | idx_scan
+-----------------------+----------
+ materialized_view_idx |        0
+(1 row)
+
+select * from materialized_maintable_view where c2='bob';
+ c1 | c2
+----+-----
+  4 | bob
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='materialized_view_idx';
+     indexrelname      | idx_scan
+-----------------------+----------
+ materialized_view_idx |        1
+(1 row)
+
+refresh materialized view materialized_maintable_view;
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+-- currently, after a refresh materialized view is called, idx_scan is reset
+-- this is not consistent with upstream PG and needs to be fixed
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='materialized_view_idx';
+     indexrelname      | idx_scan
+-----------------------+----------
+ materialized_view_idx |        0
+(1 row)
+
+-- test for joined table
+create table table2 (c1 INT PRIMARY KEY, c2 TEXT);
+insert into table2 (c1, c2) values (8, 'bob');
+create index table2_index on table2 (c2) include (c1);
+/*+IndexScan(table2) */
+explain (costs off) select maintable.c1, table2.c1, table2.c2 from maintable, table2 where table2.c2=maintable.c2;
+                  QUERY PLAN
+-----------------------------------------------
+ Nested Loop
+   ->  Seq Scan on maintable
+   ->  Index Scan using table2_index on table2
+         Index Cond: (c2 = maintable.c2)
+(4 rows)
+
+/*+IndexScan(table2) */
+select maintable.c1, table2.c1, table2.c2 from maintable, table2 where table2.c2=maintable.c2;
+ c1 | c1 | c2
+----+----+-----
+  4 |  8 | bob
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname in ('maintable_idx', 'table2_index') order by (indexrelname);
+ indexrelname  | idx_scan
+---------------+----------
+ maintable_idx |        8
+ table2_index  |        1
+(2 rows)
+
+/*+IndexScan(maintable)*/
+explain (costs off) select table2.c1, table2.c2, maintable.c1 from table2, maintable where table2.c2=maintable.c2;
+                    QUERY PLAN
+---------------------------------------------------
+ Nested Loop
+   ->  Seq Scan on table2
+   ->  Index Scan using maintable_idx on maintable
+         Index Cond: (c2 = table2.c2)
+(4 rows)
+
+/*+IndexScan(maintable)*/
+select table2.c1, table2.c2, maintable.c1 from table2, maintable where table2.c2=maintable.c2;
+ c1 | c2  | c1
+----+-----+----
+  8 | bob |  4
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname in ('maintable_idx', 'table2_index') order by (indexrelname);
+ indexrelname  | idx_scan
+---------------+----------
+ maintable_idx |        9
+ table2_index  |        1
+(2 rows)
+
+-- test for multitablet table
+create table multitablet_table (c1 INT PRIMARY KEY, c2 TEXT) split into 3 tablets;
+insert into multitablet_table (c1, c2) values (9, 'caledonia');
+create index multitablet_table_index on multitablet_table (c2) include (c1);
+select * from multitablet_table where c2='caledonia';
+ c1 |    c2
+----+-----------
+  9 | caledonia
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='multitablet_table_index';
+      indexrelname       | idx_scan
+-------------------------+----------
+ multitablet_table_index |        1
+(1 row)
+
+-- test for alter table primary key
+create table basic_table (c1 INT, c2 TEXT);
+insert into basic_table (c1, c2) values (6, '9');
+create index basic_table_idx1 on basic_table (c1) include (c2);
+create index basic_table_idx2 on basic_table (c2) include (c1);
+select * from basic_table where c1=6;
+ c1 | c2
+----+----
+  6 | 9
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'basic_table_idx.' order by (indexrelname);
+   indexrelname   | idx_scan
+------------------+----------
+ basic_table_idx1 |        1
+ basic_table_idx2 |        0
+(2 rows)
+
+select * from basic_table where c2='9';
+ c1 | c2
+----+----
+  6 | 9
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'basic_table_idx.' order by (indexrelname);
+   indexrelname   | idx_scan
+------------------+----------
+ basic_table_idx1 |        1
+ basic_table_idx2 |        1
+(2 rows)
+
+alter table basic_table add primary key (c1);
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+-- currently, alter table primary key resets the idx_scan count to zero
+-- this does not happen in upstream Postgres, will need to fix
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'basic_table_idx.' or indexrelname='basic_table_pkey' order by (indexrelname);
+   indexrelname   | idx_scan
+------------------+----------
+ basic_table_idx1 |        0
+ basic_table_idx2 |        0
+ basic_table_pkey |        0
+(3 rows)
+
+select * from basic_table where c1=6;
+ c1 | c2
+----+----
+  6 | 9
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'basic_table_idx.' or indexrelname='basic_table_pkey' order by (indexrelname);
+   indexrelname   | idx_scan
+------------------+----------
+ basic_table_idx1 |        0
+ basic_table_idx2 |        0
+ basic_table_pkey |        1
+(3 rows)
+
+select * from basic_table where c2='9';
+ c1 | c2
+----+----
+  6 | 9
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'basic_table_idx.' or indexrelname='basic_table_pkey' order by (indexrelname);
+   indexrelname   | idx_scan
+------------------+----------
+ basic_table_idx1 |        0
+ basic_table_idx2 |        1
+ basic_table_pkey |        1
+(3 rows)
+
+-- test for temporary table index
+create temporary table temp_table (c1 INT PRIMARY KEY, c2 TEXT);
+insert into temp_table (c1, c2) values (9, 'penguin');
+create index temp_index on temp_table (c2) include (c1);
+/*+IndexScan(temp_table)*/
+explain (costs off) select * from temp_table where c2='penguin';
+                QUERY PLAN
+-------------------------------------------
+ Index Scan using temp_index on temp_table
+   Index Cond: (c2 = 'penguin'::text)
+(2 rows)
+
+/*+IndexScan(temp_table)*/
+select * from temp_table where c2='penguin';
+ c1 |   c2
+----+---------
+  9 | penguin
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='temp_index';
+ indexrelname | idx_scan
+--------------+----------
+ temp_index   |        1
+(1 row)
+
+-- test for partitioned tables
+create table partitioned_table (k INT, v TEXT) partition by range (k);
+create table p1 partition of partitioned_table
+  for values from (1) to (3);
+create table p2 partition of partitioned_table
+  for values from (3) to (5);
+create table p3 partition of partitioned_table
+  for values from (5) to (7);
+insert into partitioned_table (k, v) values (2, '2');
+insert into partitioned_table (k, v) values (4, '2');
+insert into partitioned_table (k, v) values (6, '2');
+create index partitioned_idx on partitioned_table (v) include (k);
+select * from partitioned_table where v='2' order by (k);
+ k | v
+---+---
+ 2 | 2
+ 4 | 2
+ 6 | 2
+(3 rows)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'p._v_k_idx' order by (indexrelname);
+ indexrelname | idx_scan
+--------------+----------
+ p1_v_k_idx   |        1
+ p2_v_k_idx   |        1
+ p3_v_k_idx   |        1
+(3 rows)
+
+drop index if exists partitioned_idx;
+create index p1idx on p1 (v) include (k);
+create index p2idx on p2 (v) include (k);
+create index p3idx on p3 (v) include (k);
+select * from partitioned_table where v='2' order by (k);
+ k | v
+---+---
+ 2 | 2
+ 4 | 2
+ 6 | 2
+(3 rows)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'p.idx' order by (indexrelname);
+ indexrelname | idx_scan
+--------------+----------
+ p1idx        |        1
+ p2idx        |        1
+ p3idx        |        1
+(3 rows)
+
+
+-- test for GIN idx_scan increment
+create table pendtest (ts tsvector);
+create index pendtest_idx on pendtest using gin(ts);
+insert into pendtest values (to_tsvector('Lore ipsum'));
+/*+IndexScan(pendtest)*/
+select * from pendtest where 'ipsu:*'::tsquery @@ ts;
+         ts
+--------------------
+ 'ipsum':2 'lore':1
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='pendtest_idx';
+ indexrelname | idx_scan
+--------------+----------
+ pendtest_idx |        1
+(1 row)
+
+-- negative case where we don't use index scan
+/*+SeqScan(pendtest)*/
+select * from pendtest;
+         ts
+--------------------
+ 'ipsum':2 'lore':1
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='pendtest_idx';
+ indexrelname | idx_scan
+--------------+----------
+ pendtest_idx |        1
+(1 row)
+
+-- temp index for GIN table
+drop table if exists pendtest cascade;
+create temporary table pendtest (ts tsvector);
+create index pendtest_idx on pendtest using gin(ts);
+insert into pendtest values (to_tsvector('Lore ipsum'));
+-- this should be an index scan but is for some reason still a sequential scan
+-- we will need to fix this during planning phase, ignores pg_hint_plan
+/*+IndexScan(pendtest)*/
+explain (costs off) select * from pendtest where ts @@ to_tsquery('ipsu:*');
+                       QUERY PLAN
+--------------------------------------------------------
+ Bitmap Heap Scan on pendtest
+   Recheck Cond: (ts @@ to_tsquery('ipsu:*'::text))
+   ->  Bitmap Index Scan on pendtest_idx
+         Index Cond: (ts @@ to_tsquery('ipsu:*'::text))
+(4 rows)
+
+/*+IndexScan(pendtest)*/
+select * from pendtest where ts @@ to_tsquery('ipsu:*');
+         ts
+--------------------
+ 'ipsum':2 'lore':1
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='pendtest_idx';
+ indexrelname | idx_scan
+--------------+----------
+ pendtest_idx |        1
+(1 row)
+
+-- negative case where we don't use index scan
+/*+SeqScan(pendtest)*/
+explain (costs off) select * from pendtest;
+      QUERY PLAN
+----------------------
+ Seq Scan on pendtest
+(1 row)
+
+/*+SeqScan(pendtest)*/
+select * from pendtest;
+         ts
+--------------------
+ 'ipsum':2 'lore':1
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='pendtest_idx';
+ indexrelname | idx_scan
+--------------+----------
+ pendtest_idx |        1
+(1 row)
+
+-- test for colocated table
+create database colocated_db with colocated = true;
+\c colocated_db;
+create table mycolocatedtable (c1 INT PRIMARY KEY, c2 TEXT, c3 INT);
+insert into mycolocatedtable (c1, c2, c3) values (6, '9', 8);
+create index mycolocatedtable_index1 on mycolocatedtable (c2);
+create index mycolocatedtable_index2 on mycolocatedtable (c3);
+select * from mycolocatedtable where c2='9';
+ c1 | c2 | c3
+----+----+----
+  6 | 9  |  8
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'mycolocatedtable_index.' order by (indexrelname);
+      indexrelname       | idx_scan
+-------------------------+----------
+ mycolocatedtable_index1 |        1
+ mycolocatedtable_index2 |        0
+(2 rows)
+
+select * from mycolocatedtable where c3=8;
+ c1 | c2 | c3
+----+----+----
+  6 | 9  |  8
+(1 row)
+
+select pg_sleep(1);
+ pg_sleep
+----------
+
+(1 row)
+
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'mycolocatedtable_index.' order by (indexrelname);
+      indexrelname       | idx_scan
+-------------------------+----------
+ mycolocatedtable_index1 |        1
+ mycolocatedtable_index2 |        1
+(2 rows)
diff --git a/src/postgres/src/test/regress/expected/yb_pg_triggers.out b/src/postgres/src/test/regress/expected/yb_pg_triggers.out
index bb388d4b52..d49817e913 100644
--- a/src/postgres/src/test/regress/expected/yb_pg_triggers.out
+++ b/src/postgres/src/test/regress/expected/yb_pg_triggers.out
@@ -2336,6 +2336,74 @@ select tgrelid::regclass, count(*) from pg_trigger
   group by tgrelid::regclass order by tgrelid::regclass;
 */
 drop table trg_clone;
+-- Verify that firing state propagates correctly
+CREATE TABLE trgfire (i int) PARTITION BY RANGE (i);
+CREATE TABLE trgfire1 PARTITION OF trgfire FOR VALUES FROM (1) TO (10);
+CREATE OR REPLACE FUNCTION tgf() RETURNS trigger LANGUAGE plpgsql
+  AS $$ begin raise exception 'except'; end $$;
+CREATE TRIGGER tg AFTER INSERT ON trgfire FOR EACH ROW EXECUTE FUNCTION tgf();
+INSERT INTO trgfire VALUES (1);
+ERROR:  except
+CONTEXT:  PL/pgSQL function tgf() line 1 at RAISE
+ALTER TABLE trgfire DISABLE TRIGGER tg;
+INSERT INTO trgfire VALUES (1);
+CREATE TABLE trgfire2 PARTITION OF trgfire FOR VALUES FROM (10) TO (20);
+INSERT INTO trgfire VALUES (11);
+CREATE TABLE trgfire3 (LIKE trgfire);
+ERROR:  LIKE clause not supported yet
+LINE 1: CREATE TABLE trgfire3 (LIKE trgfire);
+                               ^
+HINT:  See https://github.com/YugaByte/yugabyte-db/issues/1129. Click '+' on the description to raise its priority
+ALTER TABLE trgfire ATTACH PARTITION trgfire3 FOR VALUES FROM (20) TO (30);
+ERROR:  relation "trgfire3" does not exist
+INSERT INTO trgfire VALUES (21);
+ERROR:  no partition of relation "trgfire" found for row
+DETAIL:  Partition key of the failing row contains (i) = (21).
+CREATE TABLE trgfire4 PARTITION OF trgfire FOR VALUES FROM (30) TO (40) PARTITION BY LIST (i);
+CREATE TABLE trgfire4_30 PARTITION OF trgfire4 FOR VALUES IN (30);
+INSERT INTO trgfire VALUES (30);
+CREATE TABLE trgfire5 (LIKE trgfire) PARTITION BY LIST (i);
+ERROR:  LIKE clause not supported yet
+LINE 1: CREATE TABLE trgfire5 (LIKE trgfire) PARTITION BY LIST (i);
+                               ^
+HINT:  See https://github.com/YugaByte/yugabyte-db/issues/1129. Click '+' on the description to raise its priority
+CREATE TABLE trgfire5_40 PARTITION OF trgfire5 FOR VALUES IN (40);
+ERROR:  relation "trgfire5" does not exist
+ALTER TABLE trgfire ATTACH PARTITION trgfire5 FOR VALUES FROM (40) TO (50);
+ERROR:  relation "trgfire5" does not exist
+INSERT INTO trgfire VALUES (40);
+ERROR:  no partition of relation "trgfire" found for row
+DETAIL:  Partition key of the failing row contains (i) = (40).
+SELECT tgrelid::regclass, tgenabled FROM pg_trigger
+  WHERE tgrelid::regclass IN (SELECT oid from pg_class where relname LIKE 'trgfire%')
+  ORDER BY tgrelid::regclass::text;
+   tgrelid   | tgenabled
+-------------+-----------
+ trgfire     | D
+ trgfire1    | D
+ trgfire2    | D
+ trgfire4    | D
+ trgfire4_30 | D
+(5 rows)
+
+ALTER TABLE trgfire ENABLE TRIGGER tg;
+INSERT INTO trgfire VALUES (1);
+ERROR:  except
+CONTEXT:  PL/pgSQL function tgf() line 1 at RAISE
+INSERT INTO trgfire VALUES (11);
+ERROR:  except
+CONTEXT:  PL/pgSQL function tgf() line 1 at RAISE
+INSERT INTO trgfire VALUES (21);
+ERROR:  no partition of relation "trgfire" found for row
+DETAIL:  Partition key of the failing row contains (i) = (21).
+INSERT INTO trgfire VALUES (30);
+ERROR:  except
+CONTEXT:  PL/pgSQL function tgf() line 1 at RAISE
+INSERT INTO trgfire VALUES (40);
+ERROR:  no partition of relation "trgfire" found for row
+DETAIL:  Partition key of the failing row contains (i) = (40).
+DROP TABLE trgfire;
+DROP FUNCTION tgf();
 --
 -- Test the interaction between transition tables and both kinds of
 -- inheritance.  We'll dump the contents of the transition tables in a
diff --git a/src/postgres/src/test/regress/expected/yb_roles.out b/src/postgres/src/test/regress/expected/yb_roles.out
index 4b873bc393..063d0d617c 100644
--- a/src/postgres/src/test/regress/expected/yb_roles.out
+++ b/src/postgres/src/test/regress/expected/yb_roles.out
@@ -1,9 +1,15 @@
--- test yb_extension role
+--
+-- Test yb_extension role
+--
 CREATE USER regress_priv_user;
+CREATE OPERATOR FAMILY alt_opf1 USING hash;
 SET SESSION AUTHORIZATION regress_priv_user;
 CREATE EXTENSION pgcrypto; -- should fail
 ERROR:  permission denied to create extension "pgcrypto"
-HINT:  Must be superuser to create this extension.
+HINT:  Must be superuser or yb_extension role member to create this extension.
+CREATE EXTENSION orafce; -- should fail
+ERROR:  permission denied to create extension "orafce"
+HINT:  Must be superuser or yb_extension role member to create this extension.
 \c -
 GRANT yb_extension TO regress_priv_user;
 SET SESSION AUTHORIZATION regress_priv_user;
@@ -11,15 +17,23 @@ CREATE EXTENSION pgcrypto;
 ALTER EXTENSION pgcrypto UPDATE TO '1.3';
 NOTICE:  version "1.3" of extension "pgcrypto" is already installed
 DROP EXTENSION pgcrypto;
+CREATE EXTENSION pg_trgm WITH VERSION '1.3';
+ALTER EXTENSION pg_trgm UPDATE TO '1.4';
+DROP EXTENSION pg_trgm;
+ALTER OPERATOR FAMILY alt_opf1 USING hash ADD -- should fail
+  OPERATOR 1 < (int1, int2);
+ERROR:  must be superuser or yb_extension role member to alter an operator family
 \c -
 DROP USER regress_priv_user;
--- test yb_fdw role
+--
+-- Test yb_fdw role
+--
 CREATE USER regress_priv_user1;
 CREATE USER regress_priv_user2;
 SET SESSION AUTHORIZATION regress_priv_user1;
 CREATE FOREIGN DATA WRAPPER useless; -- should fail
 ERROR:  permission denied to create foreign-data wrapper "useless"
-HINT:  Must be superuser to create a foreign-data wrapper.
+HINT:  Must be superuser or yb_fdw role member to create a foreign-data wrapper.
 \c -
 GRANT yb_fdw TO regress_priv_user1 WITH ADMIN OPTION;
 GRANT yb_extension TO regress_priv_user1;
@@ -28,7 +42,7 @@ CREATE FOREIGN DATA WRAPPER useless;
 ALTER FOREIGN DATA WRAPPER useless NO VALIDATOR;
 ALTER FOREIGN DATA WRAPPER useless OWNER TO regress_priv_user2; -- should fail
 ERROR:  permission denied to change owner of foreign-data wrapper "useless"
-HINT:  The owner of a foreign-data wrapper must be a superuser.
+HINT:  Must be superuser or yb_fdw role member to change owner of a foreign-data wrapper.
 GRANT yb_fdw TO regress_priv_user2;
 ALTER FOREIGN DATA WRAPPER useless OWNER TO regress_priv_user2;
 CREATE SERVER s1 FOREIGN DATA WRAPPER useless; -- should fail, since the owner changed
@@ -55,3 +69,40 @@ DROP USER regress_priv_user2;
 CREATE ROLE user1 BYPASSRLS;
 ALTER ROLE user1 PASSWORD 'password';
 DROP ROLE user1;
+--
+-- Test yb_db_admin role
+--
+-- verify yb_db_admin role can change bypassrls attribute
+CREATE ROLE regress_test_user1;
+SET SESSION ROLE yb_db_admin;
+ALTER ROLE regress_test_user1 WITH BYPASSRLS;
+CREATE ROLE regress_test_user2 WITH BYPASSRLS;
+ALTER ROLE regress_test_user2 WITH NOBYPASSRLS;
+-- clean up
+SET SESSION ROLE yugabyte;
+DROP ROLE regress_test_user1;
+DROP ROLE regress_test_user2;
+--
+-- Test YB Managed admin role
+--
+RESET SESSION AUTHORIZATION;
+CREATE USER regress_priv_user;
+GRANT yb_extension TO regress_priv_user;
+GRANT yb_fdw TO regress_priv_user;
+GRANT yb_db_admin TO regress_priv_user WITH ADMIN OPTION;
+SET SESSION AUTHORIZATION regress_priv_user;
+CREATE EXTENSION PGAudit;
+ALTER EXTENSION PGAudit UPDATE TO '1.3.2';
+NOTICE:  version "1.3.2" of extension "pgaudit" is already installed
+DROP EXTENSION PGAudit;
+CREATE EXTENSION orafce;
+ALTER EXTENSION orafce UPDATE TO '3.14';
+NOTICE:  version "3.14" of extension "orafce" is already installed
+DROP EXTENSION orafce;
+-- removing yb_db_admin role should result in error
+REVOKE yb_db_admin FROM regress_priv_user;
+CREATE EXTENSION PGAudit;
+ERROR:  permission denied to create event trigger "pgaudit_ddl_command_end"
+HINT:  Must be superuser or yb_db_admin role member to create an event trigger.
+DROP EXTENSION PGAudit;
+ERROR:  extension "pgaudit" does not exist
diff --git a/src/postgres/src/test/regress/expected/yb_schema.out b/src/postgres/src/test/regress/expected/yb_schema.out
index 47e80d71ba..0d57830998 100644
--- a/src/postgres/src/test/regress/expected/yb_schema.out
+++ b/src/postgres/src/test/regress/expected/yb_schema.out
@@ -30,3 +30,29 @@ SELECT * FROM S2.TBL;
 (1 row)
 
 DROP TABLE S2.TBL;
+-- verify yb_db_admin role can manage schemas like a superuser
+CREATE SCHEMA test_ns_schema_other;
+CREATE ROLE test_regress_user1;
+SET SESSION AUTHORIZATION yb_db_admin;
+ALTER SCHEMA test_ns_schema_other RENAME TO test_ns_schema_other_new;
+ALTER SCHEMA test_ns_schema_other_new OWNER TO test_regress_user1;
+DROP SCHEMA test_ns_schema_other_new;
+-- verify that the objects were dropped
+SELECT COUNT(*) FROM pg_class WHERE relnamespace =
+    (SELECT oid FROM pg_namespace WHERE nspname = 'test_ns_schema_other_new');
+ count
+-------
+     0
+(1 row)
+
+CREATE SCHEMA test_ns_schema_yb_db_admin;
+ALTER SCHEMA test_ns_schema_yb_db_admin RENAME TO test_ns_schema_yb_db_admin_new;
+ALTER SCHEMA test_ns_schema_yb_db_admin_new OWNER TO test_regress_user1;
+DROP SCHEMA test_ns_schema_yb_db_admin_new;
+-- verify that the objects were dropped
+SELECT COUNT(*) FROM pg_class WHERE relnamespace =
+    (SELECT oid FROM pg_namespace WHERE nspname = 'test_ns_schema_yb_db_admin_new');
+ count
+-------
+     0
+(1 row)
diff --git a/src/postgres/src/test/regress/expected/yb_tablegroup_dml.out b/src/postgres/src/test/regress/expected/yb_tablegroup_dml.out
index 27fb4b5232..6aff00d10d 100644
--- a/src/postgres/src/test/regress/expected/yb_tablegroup_dml.out
+++ b/src/postgres/src/test/regress/expected/yb_tablegroup_dml.out
@@ -226,6 +226,57 @@ SELECT * FROM tab_range2;
 ---+---
 (0 rows)
 
+-- truncate tablegroup table with no-tablegroup index
+CREATE TABLE tab_truncate (a INT) TABLEGROUP tg_test1;
+CREATE INDEX ON tab_truncate (a) NO TABLEGROUP;
+INSERT INTO tab_truncate VALUES (1);
+TRUNCATE tab_truncate;
+EXPLAIN (costs off)
+SELECT * FROM tab_truncate WHERE a = 1;
+                        QUERY PLAN
+----------------------------------------------------------
+ Index Only Scan using tab_truncate_a_idx on tab_truncate
+   Index Cond: (a = 1)
+(2 rows)
+
+SELECT * FROM tab_truncate WHERE a = 1;
+ a
+---
+(0 rows)
+
+DROP TABLE tab_truncate;
+-- truncate no-tablegroup table with tablegroup index
+CREATE TABLE tab_truncate1 (a INT);
+CREATE INDEX ON tab_truncate1 (a) TABLEGROUP tg_test1;
+INSERT INTO tab_truncate1 VALUES (1);
+CREATE TABLE tab_truncate2 (a INT) TABLEGROUP tg_test1;
+INSERT INTO tab_truncate2 VALUES (1);
+TRUNCATE tab_truncate1;
+SELECT * FROM tab_truncate1;
+ a
+---
+(0 rows)
+
+EXPLAIN (costs off)
+SELECT * FROM tab_truncate1 WHERE a = 1;
+                         QUERY PLAN
+------------------------------------------------------------
+ Index Only Scan using tab_truncate1_a_idx on tab_truncate1
+   Index Cond: (a = 1)
+(2 rows)
+
+SELECT * FROM tab_truncate1 WHERE a = 1;
+ a
+---
+(0 rows)
+
+SELECT * FROM tab_truncate2;
+ a
+---
+ 1
+(1 row)
+
+DROP TABLE tab_truncate1, tab_truncate2;
 -- ALTER TABLE
 CREATE TABLE tab_range_alter (a INT, b INT, PRIMARY KEY (a ASC)) TABLEGROUP tg_test1;
 INSERT INTO tab_range (a) VALUES (0), (1), (2);
diff --git a/src/postgres/src/test/regress/expected/yb_tablespaces.out b/src/postgres/src/test/regress/expected/yb_tablespaces.out
index f1fd112017..ef2d9fdb87 100644
--- a/src/postgres/src/test/regress/expected/yb_tablespaces.out
+++ b/src/postgres/src/test/regress/expected/yb_tablespaces.out
@@ -105,10 +105,10 @@ CREATE SCHEMA testschema;
 CREATE TABLESPACE invalid_tablespace WITH (replica_placement='{"num_replicas": 1, "placement_blocks":[{"cloud":"nonexistent","region":"nowhere","zone":"area51","min_num_replicas":1}]}');
 -- try using unsatisfiable tablespace
 CREATE TABLE testschema.foo (i int) TABLESPACE invalid_tablespace; -- fail
-ERROR:  Invalid argument: Invalid table definition: Error creating table yugabyte.foo on the master: Not enough tablet servers in nonexistent:nowhere:area51. Need at least 1 but only have 0.
+ERROR:  Invalid argument: Invalid table definition: Error creating table yugabyte.foo on the master: Not enough tablet servers in the requested placements. Need at least 1, have 0
 CREATE TABLE testschema.foo (i int) TABLESPACE regress_tblspace;
 ALTER TABLE testschema.foo SET TABLESPACE invalid_tablespace; -- fail
-ERROR:  Invalid argument: Not enough tablet servers in nonexistent:nowhere:area51. Need at least 1 but only have 0.
+ERROR:  Invalid argument: Not enough tablet servers in the requested placements. Need at least 1, have 0
 DROP TABLE testschema.foo;
 DROP TABLESPACE invalid_tablespace;
 -- try a table
diff --git a/src/postgres/src/test/regress/expected/yb_upgrade_db.out b/src/postgres/src/test/regress/expected/yb_upgrade_db.out
new file mode 100644
index 0000000000..c478b186b7
--- /dev/null
+++ b/src/postgres/src/test/regress/expected/yb_upgrade_db.out
@@ -0,0 +1,20 @@
+--
+-- Test case for unsupported collations
+--
+CREATE TABLE collate_test_fail(b text COLLATE "nds-x-icu");
+WARNING:  collation is deprecated
+LINE 1: CREATE TABLE collate_test_fail(b text COLLATE "nds-x-icu");
+                                              ^
+ERROR:  collation no longer supported and will be dropped in next release
+LINE 1: CREATE TABLE collate_test_fail(b text COLLATE "nds-x-icu");
+                                              ^
+CREATE COLLATION nds from "nds-x-icu";
+ERROR:  cannot create collation based on deprecated collation
+CREATE TABLE collate_test_fail (a int, b text);
+SELECT * from collate_test_fail where b like 'a'  COLLATE "nds-x-icu";
+WARNING:  collation is deprecated
+LINE 1: SELECT * from collate_test_fail where b like 'a'  COLLATE "n...
+                                                          ^
+ a | b
+---+---
+(0 rows)
diff --git a/src/postgres/src/test/regress/sql/triggers.sql b/src/postgres/src/test/regress/sql/triggers.sql
index 49e726afcc..c58e9e9aa7 100644
--- a/src/postgres/src/test/regress/sql/triggers.sql
+++ b/src/postgres/src/test/regress/sql/triggers.sql
@@ -1787,6 +1787,38 @@ create trigger intercept_insert_child3
   before insert on child3
   for each row execute procedure intercept_insert();
 
+-- Verify that firing state propagates correctly
+CREATE TABLE trgfire (i int) PARTITION BY RANGE (i);
+CREATE TABLE trgfire1 PARTITION OF trgfire FOR VALUES FROM (1) TO (10);
+CREATE OR REPLACE FUNCTION tgf() RETURNS trigger LANGUAGE plpgsql
+  AS $$ begin raise exception 'except'; end $$;
+CREATE TRIGGER tg AFTER INSERT ON trgfire FOR EACH ROW EXECUTE FUNCTION tgf();
+INSERT INTO trgfire VALUES (1);
+ALTER TABLE trgfire DISABLE TRIGGER tg;
+INSERT INTO trgfire VALUES (1);
+CREATE TABLE trgfire2 PARTITION OF trgfire FOR VALUES FROM (10) TO (20);
+INSERT INTO trgfire VALUES (11);
+CREATE TABLE trgfire3 (LIKE trgfire);
+ALTER TABLE trgfire ATTACH PARTITION trgfire3 FOR VALUES FROM (20) TO (30);
+INSERT INTO trgfire VALUES (21);
+CREATE TABLE trgfire4 PARTITION OF trgfire FOR VALUES FROM (30) TO (40) PARTITION BY LIST (i);
+CREATE TABLE trgfire4_30 PARTITION OF trgfire4 FOR VALUES IN (30);
+INSERT INTO trgfire VALUES (30);
+CREATE TABLE trgfire5 (LIKE trgfire) PARTITION BY LIST (i);
+CREATE TABLE trgfire5_40 PARTITION OF trgfire5 FOR VALUES IN (40);
+ALTER TABLE trgfire ATTACH PARTITION trgfire5 FOR VALUES FROM (40) TO (50);
+INSERT INTO trgfire VALUES (40);
+SELECT tgrelid::regclass, tgenabled FROM pg_trigger
+  WHERE tgrelid::regclass IN (SELECT oid from pg_class where relname LIKE 'trgfire%')
+  ORDER BY tgrelid::regclass::text;
+ALTER TABLE trgfire ENABLE TRIGGER tg;
+INSERT INTO trgfire VALUES (1);
+INSERT INTO trgfire VALUES (11);
+INSERT INTO trgfire VALUES (21);
+INSERT INTO trgfire VALUES (30);
+INSERT INTO trgfire VALUES (40);
+DROP TABLE trgfire;
+DROP FUNCTION tgf();
 
 -- insert, parent trigger sees post-modification parent-format tuple
 insert into parent values ('AAA', 42), ('BBB', 42), ('CCC', 66);
diff --git a/src/postgres/src/test/regress/sql/yb_alter_table.sql b/src/postgres/src/test/regress/sql/yb_alter_table.sql
new file mode 100644
index 0000000000..e3fb12fb1e
--- /dev/null
+++ b/src/postgres/src/test/regress/sql/yb_alter_table.sql
@@ -0,0 +1,23 @@
+---
+--- Verify renaming on temp tables
+---
+CREATE TEMP TABLE temp_table(a int primary key, b int);
+CREATE INDEX temp_table_b_idx ON temp_table(b);
+ALTER INDEX temp_table_pkey RENAME TO temp_table_pkey_new;
+ALTER INDEX temp_table_b_idx RENAME TO temp_table_b_idx_new;
+
+---
+--- Verify yb_db_admin role can ALTER table
+---
+CREATE TABLE table_other(a int, b int);
+CREATE INDEX index_table_other ON table_other(a);
+SET SESSION AUTHORIZATION yb_db_admin;
+ALTER TABLE table_other RENAME to table_new;
+ALTER TABLE table_new OWNER TO regress_alter_table_user1;
+ALTER TABLE pg_database RENAME TO test; -- should fail
+ALTER TABLE pg_tablespace OWNER TO regress_alter_table_user1; -- should fail
+---
+--- Verify yb_db_admin role can ALTER index
+---
+ALTER INDEX index_table_other RENAME TO index_table_other_new;
+RESET SESSION AUTHORIZATION;
diff --git a/src/postgres/src/test/regress/sql/yb_dependency.sql b/src/postgres/src/test/regress/sql/yb_dependency.sql
new file mode 100644
index 0000000000..180046ba64
--- /dev/null
+++ b/src/postgres/src/test/regress/sql/yb_dependency.sql
@@ -0,0 +1,24 @@
+--
+-- Validate yb_db_admin can reassign object ownership
+--
+CREATE ROLE regress_user1;
+CREATE ROLE regress_user2;
+CREATE ROLE regress_user3;
+CREATE ROLE non_sys_superuser SUPERUSER;
+GRANT regress_user1 TO regress_user3;
+GRANT regress_user2 TO regress_user3;
+SET SESSION AUTHORIZATION yb_db_admin;
+REASSIGN OWNED BY regress_user1 TO regress_user2;
+REASSIGN OWNED BY regress_user2 TO regress_user1;
+-- should fail, cannot reassign objects owned by superuser
+REASSIGN OWNED BY postgres TO yb_db_admin;
+-- should fail, assigning from superuser to non-superuser
+REASSIGN OWNED BY non_sys_superuser TO yb_db_admin;
+-- should fail, user needs privileges on both old and new role
+SET SESSION AUTHORIZATION regress_user1;
+REASSIGN OWNED BY regress_user1 TO regress_user2;
+REASSIGN OWNED BY regress_user2 TO regress_user1;
+-- should succeed, user has privileges on both old and new role
+SET SESSION AUTHORIZATION regress_user3;
+REASSIGN OWNED BY regress_user1 TO regress_user2;
+REASSIGN OWNED BY regress_user2 TO regress_user1;
diff --git a/src/postgres/src/test/regress/sql/yb_event_trigger.sql b/src/postgres/src/test/regress/sql/yb_event_trigger.sql
index 1d1afb37c8..91c4f89588 100644
--- a/src/postgres/src/test/regress/sql/yb_event_trigger.sql
+++ b/src/postgres/src/test/regress/sql/yb_event_trigger.sql
@@ -70,7 +70,8 @@ SELECT * FROM test;
 DROP TABLE test;
 
 -- Verify that yb_db_admin role can use event triggers.
-ALTER EVENT TRIGGER foo OWNER TO yb_db_admin;
+CREATE ROLE superuser_role SUPERUSER;
+CREATE ROLE non_superuser_role;
 SET SESSION AUTHORIZATION yb_db_admin;
 CREATE EVENT TRIGGER admin_foo ON ddl_command_start EXECUTE PROCEDURE test_event_trigger_foo();
 CREATE EVENT TRIGGER admin_bar ON ddl_command_end EXECUTE PROCEDURE test_event_trigger_bar();
@@ -78,5 +79,6 @@ ALTER EVENT TRIGGER admin_foo DISABLE;
 ALTER EVENT TRIGGER admin_foo ENABLE REPLICA;
 ALTER EVENT TRIGGER admin_foo ENABLE ALWAYS;
 ALTER EVENT TRIGGER admin_foo RENAME TO admin_foo_new;
+ALTER EVENT TRIGGER admin_foo_new OWNER TO superuser_role;
+ALTER EVENT TRIGGER admin_foo_new OWNER TO non_superuser_role;
 DROP EVENT TRIGGER admin_foo_new;
-
diff --git a/src/postgres/src/test/regress/sql/yb_feature_temp.sql b/src/postgres/src/test/regress/sql/yb_feature_temp.sql
index 35203f2090..90fba60c91 100644
--- a/src/postgres/src/test/regress/sql/yb_feature_temp.sql
+++ b/src/postgres/src/test/regress/sql/yb_feature_temp.sql
@@ -101,25 +101,25 @@ DROP TABLE temptest;
 -- test ON COMMIT DROP
 -- TODO(dmitry) ON COMMIT DROP should be fixed in context of #7926
 
-BEGIN;
+-- BEGIN;
 
-CREATE TEMP TABLE temptest(col int) ON COMMIT DROP;
+-- CREATE TEMP TABLE temptest(col int) ON COMMIT DROP;
 
-INSERT INTO temptest VALUES (1);
-INSERT INTO temptest VALUES (2);
+-- INSERT INTO temptest VALUES (1);
+-- INSERT INTO temptest VALUES (2);
 
-SELECT * FROM temptest;
-COMMIT;
+-- SELECT * FROM temptest;
+-- COMMIT;
 
-SELECT * FROM temptest;
+-- SELECT * FROM temptest;
 
-BEGIN;
-CREATE TEMP TABLE temptest(col) ON COMMIT DROP AS SELECT 1;
+-- BEGIN;
+-- CREATE TEMP TABLE temptest(col) ON COMMIT DROP AS SELECT 1;
 
-SELECT * FROM temptest;
-COMMIT;
+-- SELECT * FROM temptest;
+-- COMMIT;
 
-SELECT * FROM temptest;
+-- SELECT * FROM temptest;
 
 -- ON COMMIT is only allowed for TEMP
 
diff --git a/src/postgres/src/test/regress/sql/yb_function.sql b/src/postgres/src/test/regress/sql/yb_function.sql
new file mode 100644
index 0000000000..37e0e34da9
--- /dev/null
+++ b/src/postgres/src/test/regress/sql/yb_function.sql
@@ -0,0 +1,58 @@
+CREATE USER regress_alter_generic_user3;
+CREATE USER regress_alter_generic_user2;
+CREATE USER regress_alter_generic_user1 IN ROLE regress_alter_generic_user3;
+CREATE SCHEMA alt_nsp1;
+GRANT ALL ON SCHEMA alt_nsp1 TO PUBLIC;
+SET search_path = alt_nsp1, public;
+
+---
+--- Verify yb_db_admin can CREATE and DROP functions
+---
+CREATE FUNCTION other_func(int) RETURNS int LANGUAGE sql
+  AS 'SELECT $1 + 1';
+SET SESSION AUTHORIZATION yb_db_admin;
+CREATE FUNCTION admin_func(int) RETURNS int LANGUAGE sql
+  AS 'SELECT $1 + 1';
+CREATE FUNCTION admin_func_leakproof(int) RETURNS int LANGUAGE sql  -- not allowed
+  LEAKPROOF AS 'SELECT $1 + 1';
+CREATE OR REPLACE FUNCTION increment(i integer) RETURNS integer AS $$
+  BEGIN
+    RETURN i + 1;
+  END;
+$$ LANGUAGE plpgsql;
+CREATE FUNCTION language_func() RETURNS uuid  -- C functions aren't allowed
+  LANGUAGE c STRICT PARALLEL SAFE
+  AS '$libdir/uuid-ossp', 'uuid_generate_v1';
+DROP FUNCTION admin_func(int);
+DROP FUNCTION other_func(int);
+DROP FUNCTION language_func(); -- does not exist
+RESET SESSION AUTHORIZATION;
+
+---
+--- Validate yb_db_admin role can ALTER function
+---
+CREATE FUNCTION alt_func1(int) RETURNS int LANGUAGE sql
+  AS 'SELECT $1 + 1';
+SET SESSION AUTHORIZATION yb_db_admin;
+ALTER FUNCTION alt_func1(int) OWNER TO regress_alter_generic_user1;
+ALTER FUNCTION alt_func1(int) RENAME TO func_renamed;
+ALTER FUNCTION func_renamed(int) SET SCHEMA alt_nsp1;
+ALTER FUNCTION func_renamed(int) LEAKPROOF;  -- not allowed
+ALTER FUNCTION func_renamed(int) NOT LEAKPROOF;
+-- validate regress_alter_generic_user2 can operate on function
+SET SESSION AUTHORIZATION regress_alter_generic_user1;
+ALTER FUNCTION func_renamed(int) OWNER TO regress_alter_generic_user2;  -- failed (no role membership)
+ALTER FUNCTION func_renamed(int) OWNER TO regress_alter_generic_user3;  -- OK
+ALTER FUNCTION func_renamed(int) RENAME TO func_renamed2;
+ALTER FUNCTION func_renamed2(int) SET SCHEMA alt_nsp1;
+ALTER FUNCTION func_renamed2(int) LEAKPROOF;  -- not allowed
+ALTER FUNCTION func_renamed2(int) NOT LEAKPROOF;
+
+---
+--- Clean up
+---
+RESET SESSION AUTHORIZATION;
+DROP SCHEMA alt_nsp1 CASCADE;
+DROP USER regress_alter_generic_user1;
+DROP USER regress_alter_generic_user2;
+DROP USER regress_alter_generic_user3;
diff --git a/src/postgres/src/test/regress/sql/yb_index_scan.sql b/src/postgres/src/test/regress/sql/yb_index_scan.sql
index 3218d2b62f..b4531cfb9b 100644
--- a/src/postgres/src/test/regress/sql/yb_index_scan.sql
+++ b/src/postgres/src/test/regress/sql/yb_index_scan.sql
@@ -61,6 +61,11 @@ INSERT INTO sc_multi_desc(k, r, v) VALUES (1, 10, 10),(1, 10, 10),(1, NULL, 2),(
 EXPLAIN (COSTS OFF) SELECT * FROM sc_multi_desc WHERE k = 1;
 SELECT * FROM sc_multi_desc WHERE k = 1;
 
+-- Testing for the case in issue #12481
+CREATE INDEX range_ind ON sc_multi_desc(v ASC, r ASC);
+EXPLAIN SELECT v,r FROM sc_multi_desc WHERE v IN (2,4) and r is null;
+SELECT v,r FROM sc_multi_desc WHERE v IN (2,4) and r is null;
+
 -- Test NULLS last ordering.
 CREATE TABLE sc_desc_nl(h int, r int, v int);
 CREATE INDEX on sc_desc_nl(h HASH, r DESC NULLS LAST);
diff --git a/src/postgres/src/test/regress/sql/yb_partition_prune_plancache.sql b/src/postgres/src/test/regress/sql/yb_partition_prune_plancache.sql
new file mode 100644
index 0000000000..eb0d12f6ab
--- /dev/null
+++ b/src/postgres/src/test/regress/sql/yb_partition_prune_plancache.sql
@@ -0,0 +1,81 @@
+-- Test Setup.
+-- Create a range-partitioning hierarchy with multiple keys and default partitions.
+CREATE TABLE rp (a int, b int, c varchar, d text, PRIMARY KEY(a,b,c)) PARTITION BY RANGE(a);
+CREATE TABLE rp_p1 PARTITION OF rp FOR VALUES FROM (0) TO (100);
+CREATE TABLE rp_sub PARTITION OF rp FOR VALUES FROM (100) TO (200) PARTITION BY RANGE (b, c);
+CREATE TABLE rp_p2 PARTITION OF rp_sub FOR VALUES FROM (0, 'a') TO (100, 'j');
+CREATE TABLE rp_p3 PARTITION OF rp_sub DEFAULT;
+
+-- Create a list-partitioning hierarchy with multiple keys, NULL partitions and default partitions.
+CREATE TABLE lp (a int, b int, c varchar, d text) PARTITION BY LIST(a);
+CREATE INDEX ON lp(a);
+CREATE TABLE lp_p1 PARTITION OF lp FOR VALUES IN (0, 1, 2, 3, 4, 5);
+CREATE TABLE lp_sub PARTITION OF lp FOR VALUES IN (6, 7, 8, 9, 10) PARTITION BY LIST (b);
+CREATE TABLE lp_p2 PARTITION OF lp_sub FOR VALUES IN (null);
+CREATE TABLE lp_p3 PARTITION OF lp_sub FOR VALUES IN (1, 2);
+
+-- Create a non-partitioned table.
+CREATE TABLE np (a int, b int, c varchar, d text);
+
+-- Set the number of times custom plans are chosen over generic plans unconditionally.
+-- This means that the first time a prepared statement is executed, it will always be
+-- executed with a custom plan. Cost comparison between custom and generic plans will
+-- take effect at the second invocation.
+SET yb_test_planner_custom_plan_threshold=1;
+
+-- Note: The presence of actual values provided to the bound parameters in EXPLAIN
+-- output indicates a custom plan. The presence of symbols like '$1' in the EXPLAIN
+-- output indicates that a generic plan was chosen.
+-- SELECT query where one of the partition key values is a bound parameter and the other is
+-- not a bound parameter.
+PREPARE t1(int) AS SELECT * FROM rp WHERE a=$1 AND b=3 AND c='1';
+EXPLAIN EXECUTE t1(1);
+EXPLAIN EXECUTE t1(1);
+
+-- Turn off favoring custom plan over generic plan based on partition pruning.
+SET yb_planner_custom_plan_for_partition_pruning=false;
+EXPLAIN EXECUTE t1(1);
+
+-- Turn it back on.
+SET yb_planner_custom_plan_for_partition_pruning=true;
+
+-- UPDATE list partitioned table using a JOIN with a non-partitioned table.
+PREPARE t2(int) AS UPDATE np SET d = 1 FROM lp WHERE lp.a = np.a AND lp.a = $1;
+EXPLAIN EXECUTE t2(1);
+EXPLAIN EXECUTE t2(1);
+
+-- DELETE range partitioned table using a JOIN with a non-partitioned table.
+PREPARE t3(int) AS DELETE FROM rp USING np WHERE rp.a = np.a AND rp.a = $1;
+EXPLAIN EXECUTE t3(1);
+EXPLAIN EXECUTE t3(1);
+
+-- Subquery test where the outer query has a partition key as a bound parameter.
+PREPARE t4 AS SELECT * FROM rp WHERE rp.a = $1 AND rp.b IN (SELECT b FROM np WHERE a < 5) ;
+EXPLAIN EXECUTE t4(1);
+EXPLAIN EXECUTE t4(1);
+
+-- Subquery test whether the inner query has a partition key as a bound parameter.
+PREPARE t5(int) AS SELECT * FROM np WHERE np.a = 1 AND np.b IN (SELECT b FROM lp WHERE a=$1 AND c='1');
+EXPLAIN EXECUTE t5(1);
+EXPLAIN EXECUTE t5(1);
+
+-- CTE on partitioned tables.
+PREPARE t6(int) AS
+WITH x AS (UPDATE lp SET d=1 WHERE a = $1 returning a, b, c, d)
+SELECT * FROM rp INNER JOIN x ON rp.a = x.a;
+EXPLAIN EXECUTE t6(1);
+EXPLAIN EXECUTE t6(1);
+
+-- Generic plan must be chosen for SELECTs/UPDATEs/DELETEs if the number of partitions
+-- pruned by generic plan is equal to that of custom plan.
+PREPARE t7(char) AS SELECT * FROM rp WHERE a=1 AND b=1 AND c=$1;
+EXPLAIN EXECUTE t7('a');
+EXPLAIN EXECUTE t7('a');
+
+PREPARE t8(char) AS UPDATE lp SET d=3 WHERE a=1 AND b=1 AND c=$1;
+EXPLAIN EXECUTE t8('a');
+EXPLAIN EXECUTE t8('a');
+
+PREPARE t9(int) AS DELETE FROM np WHERE a < $1;
+EXPLAIN EXECUTE t9(1);
+EXPLAIN EXECUTE t9(1);
diff --git a/src/postgres/src/test/regress/sql/yb_pg_alter_generic.sql b/src/postgres/src/test/regress/sql/yb_pg_alter_generic.sql
index dfa6024fd1..399b36c616 100644
--- a/src/postgres/src/test/regress/sql/yb_pg_alter_generic.sql
+++ b/src/postgres/src/test/regress/sql/yb_pg_alter_generic.sql
@@ -461,20 +461,6 @@ SELECT nspname, prsname
   FROM pg_ts_parser t, pg_namespace n
   WHERE t.prsnamespace = n.oid AND nspname like 'alt_nsp%'
   ORDER BY nspname, prsname;
-
----
---- Validate yb_db_admin role can ALTER function
----
-CREATE FUNCTION alt_func1(int) RETURNS int LANGUAGE sql
-  AS 'SELECT $1 + 1';
-SET SESSION AUTHORIZATION yb_db_admin;
-ALTER FUNCTION alt_func1(int) OWNER TO regress_alter_generic_user2;
-ALTER FUNCTION alt_func1(int) RENAME TO func_renamed;
-ALTER FUNCTION func_renamed(int) SET SCHEMA alt_nsp1;
--- validate regress_alter_generic_user2 can operate on function
-ALTER FUNCTION func_renamed(int) OWNER TO regress_alter_generic_user1;
-ALTER FUNCTION func_renamed(int) RENAME TO func_renamed2;
-ALTER FUNCTION func_renamed2(int) SET SCHEMA alt_nsp1;
 ---
 --- Cleanup resources
 ---
diff --git a/src/postgres/src/test/regress/sql/yb_pg_alter_table.sql b/src/postgres/src/test/regress/sql/yb_pg_alter_table.sql
index 3db5a54aa3..d358fecd8e 100644
--- a/src/postgres/src/test/regress/sql/yb_pg_alter_table.sql
+++ b/src/postgres/src/test/regress/sql/yb_pg_alter_table.sql
@@ -9,6 +9,58 @@ RESET client_min_messages;
 
 CREATE USER regress_alter_table_user1;
 
+--
+-- rename - check on both non-temp and temp tables
+--
+CREATE TABLE attmp (regtable int);
+CREATE TEMP TABLE attmp (attmptable int);
+
+ALTER TABLE attmp RENAME TO attmp_new;
+
+SELECT * FROM attmp;
+SELECT * FROM attmp_new;
+
+ALTER TABLE attmp RENAME TO attmp_new2;
+
+SELECT * FROM attmp;		-- should fail
+SELECT * FROM attmp_new;
+SELECT * FROM attmp_new2;
+
+DROP TABLE attmp_new;
+DROP TABLE attmp_new2;
+
+-- check rename of partitioned tables and indexes also
+CREATE TABLE part_attmp (a int primary key) partition by range (a);
+CREATE TABLE part_attmp1 PARTITION OF part_attmp FOR VALUES FROM (0) TO (100);
+ALTER INDEX part_attmp_pkey RENAME TO part_attmp_index;
+ALTER INDEX part_attmp1_pkey RENAME TO part_attmp1_index;
+ALTER TABLE part_attmp RENAME TO part_at2tmp;
+ALTER TABLE part_attmp1 RENAME TO part_at2tmp1;
+SET ROLE regress_alter_table_user1;
+ALTER INDEX part_attmp_index RENAME TO fail;
+ALTER INDEX part_attmp1_index RENAME TO fail;
+ALTER TABLE part_at2tmp RENAME TO fail;
+ALTER TABLE part_at2tmp1 RENAME TO fail;
+RESET ROLE;
+DROP TABLE part_at2tmp;
+
+-- ALTER TABLE ... RENAME on non-table relations
+-- renaming indexes (FIXME: this should probably test the index's functionality)
+ALTER INDEX IF EXISTS __onek_unique1 RENAME TO attmp_onek_unique1;
+ALTER INDEX IF EXISTS __attmp_onek_unique1 RENAME TO onek_unique1;
+
+ALTER INDEX onek_unique1 RENAME TO attmp_onek_unique1;
+ALTER INDEX attmp_onek_unique1 RENAME TO onek_unique1;
+
+SET ROLE regress_alter_table_user1;
+ALTER INDEX onek_unique1 RENAME TO fail;  -- permission denied
+RESET ROLE;
+
+-- renaming index should rename constraint as well
+ALTER TABLE onek ADD CONSTRAINT onek_unique1_constraint UNIQUE (unique1);
+ALTER INDEX onek_unique1_constraint RENAME TO onek_unique1_constraint_foo;
+ALTER TABLE onek DROP CONSTRAINT onek_unique1_constraint_foo;
+
 --
 -- lock levels
 --
@@ -119,12 +171,3 @@ ALTER TABLE tt7 OF tt_t1;			-- reassign an already-typed table
 ALTER TABLE tt7 NOT OF;
 \d tt7
 
----
---- Verfy yb_db_admin role can ALTER table
----
-CREATE TABLE table_other(a int, b int);
-SET SESSION AUTHORIZATION yb_db_admin;
-ALTER TABLE table_other RENAME to table_new;
-ALTER TABLE table_new OWNER TO regress_alter_table_user1;
-DROP TABLE table_new;
-
diff --git a/src/postgres/src/test/regress/sql/yb_pg_namespace.sql b/src/postgres/src/test/regress/sql/yb_pg_namespace.sql
index 637869334d..2638d5e75b 100644
--- a/src/postgres/src/test/regress/sql/yb_pg_namespace.sql
+++ b/src/postgres/src/test/regress/sql/yb_pg_namespace.sql
@@ -49,22 +49,3 @@ DROP SCHEMA test_ns_schema_renamed CASCADE;
 -- verify that the objects were dropped
 SELECT COUNT(*) FROM pg_class WHERE relnamespace =
     (SELECT oid FROM pg_namespace WHERE nspname = 'test_ns_schema_renamed');
-
--- verify yb_db_admin role can manage schemas like a superuser
-CREATE SCHEMA test_ns_schema_other;
-CREATE ROLE test_regress_user1;
-SET SESSION AUTHORIZATION yb_db_admin;
-ALTER SCHEMA test_ns_schema_other RENAME TO test_ns_schema_other_new;
-ALTER SCHEMA test_ns_schema_other_new OWNER TO test_regress_user1;
-DROP SCHEMA test_ns_schema_other_new;
--- verify that the objects were dropped
-SELECT COUNT(*) FROM pg_class WHERE relnamespace =
-    (SELECT oid FROM pg_namespace WHERE nspname = 'test_ns_schema_other_new');
-CREATE SCHEMA test_ns_schema_yb_db_admin;
-ALTER SCHEMA test_ns_schema_yb_db_admin RENAME TO test_ns_schema_yb_db_admin_new;
-ALTER SCHEMA test_ns_schema_yb_db_admin_new OWNER TO test_regress_user1;
-DROP SCHEMA test_ns_schema_yb_db_admin_new;
--- verify that the objects were dropped
-SELECT COUNT(*) FROM pg_class WHERE relnamespace =
-    (SELECT oid FROM pg_namespace WHERE nspname = 'test_ns_schema_yb_db_admin_new');
-
diff --git a/src/postgres/src/test/regress/sql/yb_pg_roleattributes.sql b/src/postgres/src/test/regress/sql/yb_pg_roleattributes.sql
index ed92ca9046..1b034d752f 100644
--- a/src/postgres/src/test/regress/sql/yb_pg_roleattributes.sql
+++ b/src/postgres/src/test/regress/sql/yb_pg_roleattributes.sql
@@ -95,15 +95,3 @@ DROP ROLE regress_test_def_replication;
 DROP ROLE regress_test_replication;
 DROP ROLE regress_test_def_bypassrls;
 DROP ROLE regress_test_bypassrls;
-
--- verify yb_db_admin role can change bypassrls attribute
-CREATE ROLE regress_test_user1;
-SET SESSION ROLE yb_db_admin;
-ALTER ROLE regress_test_user1 WITH BYPASSRLS;
-CREATE ROLE regress_test_user2 WITH BYPASSRLS;
-ALTER ROLE regress_test_user2 WITH NOBYPASSRLS;
--- clean up
-SET SESSION ROLE yugabyte;
-DROP ROLE regress_test_user1;
-DROP ROLE regress_test_user2;
-
diff --git a/src/postgres/src/test/regress/sql/yb_pg_stat.sql b/src/postgres/src/test/regress/sql/yb_pg_stat.sql
new file mode 100644
index 0000000000..fdbc239e04
--- /dev/null
+++ b/src/postgres/src/test/regress/sql/yb_pg_stat.sql
@@ -0,0 +1,213 @@
+-- Test for PG_STAT
+-- test FOR LSM idx_scan in pg_stat_user_indexes
+create table maintable(c1 INT, c2 TEXT, PRIMARY KEY(c1));
+insert into maintable (c1, c2) values (4, 'bob');
+create index maintable_idx on maintable (c2) include (c1);
+/*+IndexOnlyScan(maintable_idx)*/
+select * from maintable where c2='bob';
+-- need to sleep for over half a second here since updates to pgstat is hardcoded to 500 milliseconds
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+
+/*+IndexScan(maintable)*/
+select * from maintable where c2='bob';
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+
+-- negative case where we don't use index scan
+/*+SeqScan(maintable)*/
+select * from maintable;
+select * from maintable where c2='bob';
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+
+-- test case for primary key scan
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_pkey';
+/*+IndexScan(maintable)*/
+select * from maintable where c1=4;
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_pkey';
+
+-- test case for transaction abort
+begin;
+/*+IndexScan(maintable)*/
+select * from maintable where c2='bob';
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+abort;
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+
+-- test for table view
+create view maintable_view as select * from maintable;
+select * from maintable_view where c2='bob';
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+
+-- test for materialized table view
+create materialized view materialized_maintable_view as select * from maintable where c2='bob';
+select * from materialized_maintable_view where c2='bob';
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+
+-- test for refreshing materialized table view
+insert into maintable (c1, c2) values (6, 'sol'); 
+/*+IndexScan(maintable) IndexScan(materialized_maintable_view)*/
+refresh materialized view materialized_maintable_view;
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='maintable_idx';
+
+-- test for refreshing materialized table view with index
+create index materialized_view_idx on materialized_maintable_view (c2) include (c1);
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='materialized_view_idx';
+select * from materialized_maintable_view where c2='bob';
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='materialized_view_idx';
+refresh materialized view materialized_maintable_view;
+select pg_sleep(1);
+-- currently, after a refresh materialized view is called, idx_scan is reset 
+-- this is not consistent with upstream PG and needs to be fixed
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='materialized_view_idx';
+
+-- test for joined table
+create table table2 (c1 INT PRIMARY KEY, c2 TEXT);
+insert into table2 (c1, c2) values (8, 'bob');
+create index table2_index on table2 (c2) include (c1);
+/*+IndexScan(table2) */
+explain (costs off) select maintable.c1, table2.c1, table2.c2 from maintable, table2 where table2.c2=maintable.c2;
+/*+IndexScan(table2) */
+select maintable.c1, table2.c1, table2.c2 from maintable, table2 where table2.c2=maintable.c2;
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname in ('maintable_idx', 'table2_index') order by (indexrelname);
+
+/*+IndexScan(maintable)*/
+explain (costs off) select table2.c1, table2.c2, maintable.c1 from table2, maintable where table2.c2=maintable.c2;
+/*+IndexScan(maintable)*/
+select table2.c1, table2.c2, maintable.c1 from table2, maintable where table2.c2=maintable.c2;
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname in ('maintable_idx', 'table2_index') order by (indexrelname);
+
+-- test for multitablet table
+create table multitablet_table (c1 INT PRIMARY KEY, c2 TEXT) split into 3 tablets;
+insert into multitablet_table (c1, c2) values (9, 'caledonia');
+create index multitablet_table_index on multitablet_table (c2) include (c1);
+select * from multitablet_table where c2='caledonia';
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='multitablet_table_index';
+
+-- test for alter table primary key
+create table basic_table (c1 INT, c2 TEXT);
+insert into basic_table (c1, c2) values (6, '9');
+create index basic_table_idx1 on basic_table (c1) include (c2);
+create index basic_table_idx2 on basic_table (c2) include (c1);
+select * from basic_table where c1=6;
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'basic_table_idx.' order by (indexrelname);
+select * from basic_table where c2='9';
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'basic_table_idx.' order by (indexrelname);
+
+alter table basic_table add primary key (c1);
+
+select pg_sleep(1);
+-- currently, alter table primary key resets the idx_scan count to zero
+-- this does not happen in upstream Postgres, will need to fix
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'basic_table_idx.' or indexrelname='basic_table_pkey' order by (indexrelname);
+select * from basic_table where c1=6;
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'basic_table_idx.' or indexrelname='basic_table_pkey' order by (indexrelname);
+select * from basic_table where c2='9';
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'basic_table_idx.' or indexrelname='basic_table_pkey' order by (indexrelname);
+
+-- test for temporary table index
+create temporary table temp_table (c1 INT PRIMARY KEY, c2 TEXT);
+insert into temp_table (c1, c2) values (9, 'penguin');
+create index temp_index on temp_table (c2) include (c1);
+/*+IndexScan(temp_table)*/
+explain (costs off) select * from temp_table where c2='penguin';
+/*+IndexScan(temp_table)*/
+select * from temp_table where c2='penguin';
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='temp_index';
+
+-- test for partitioned tables
+create table partitioned_table (k INT, v TEXT) partition by range (k);
+create table p1 partition of partitioned_table
+  for values from (1) to (3);
+create table p2 partition of partitioned_table
+  for values from (3) to (5);
+create table p3 partition of partitioned_table
+  for values from (5) to (7);
+insert into partitioned_table (k, v) values (2, '2');
+insert into partitioned_table (k, v) values (4, '2');
+insert into partitioned_table (k, v) values (6, '2');
+create index partitioned_idx on partitioned_table (v) include (k);
+select * from partitioned_table where v='2' order by (k);
+
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'p._v_k_idx' order by (indexrelname);
+
+drop index if exists partitioned_idx;
+create index p1idx on p1 (v) include (k);
+create index p2idx on p2 (v) include (k);
+create index p3idx on p3 (v) include (k);
+select * from partitioned_table where v='2' order by (k);
+
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'p.idx' order by (indexrelname);
+  
+-- test for GIN idx_scan increment
+create table pendtest (ts tsvector);
+create index pendtest_idx on pendtest using gin(ts);
+insert into pendtest values (to_tsvector('Lore ipsum'));
+/*+IndexScan(pendtest)*/
+select * from pendtest where 'ipsu:*'::tsquery @@ ts;
+
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='pendtest_idx';
+
+-- negative case where we don't use index scan
+/*+SeqScan(pendtest)*/
+select * from pendtest;
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='pendtest_idx';
+
+-- temp index for GIN table
+drop table if exists pendtest cascade;
+create temporary table pendtest (ts tsvector);
+create index pendtest_idx on pendtest using gin(ts);
+insert into pendtest values (to_tsvector('Lore ipsum'));
+-- this should be an index scan but is for some reason still a sequential scan
+-- we will need to fix this during planning phase, ignores pg_hint_plan
+/*+IndexScan(pendtest)*/
+explain (costs off) select * from pendtest where ts @@ to_tsquery('ipsu:*');
+/*+IndexScan(pendtest)*/
+select * from pendtest where ts @@ to_tsquery('ipsu:*');
+
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='pendtest_idx';
+
+-- negative case where we don't use index scan
+/*+SeqScan(pendtest)*/
+explain (costs off) select * from pendtest;
+/*+SeqScan(pendtest)*/
+select * from pendtest;
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname='pendtest_idx';
+
+-- test for colocated table
+create database colocated_db with colocated = true;
+\c colocated_db;
+create table mycolocatedtable (c1 INT PRIMARY KEY, c2 TEXT, c3 INT);
+insert into mycolocatedtable (c1, c2, c3) values (6, '9', 8);
+create index mycolocatedtable_index1 on mycolocatedtable (c2);
+create index mycolocatedtable_index2 on mycolocatedtable (c3);
+select * from mycolocatedtable where c2='9';
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'mycolocatedtable_index.' order by (indexrelname);
+select * from mycolocatedtable where c3=8;
+select pg_sleep(1);
+select indexrelname,idx_scan from pg_stat_user_indexes where indexrelname ~ 'mycolocatedtable_index.' order by (indexrelname);
diff --git a/src/postgres/src/test/regress/sql/yb_pg_triggers.sql b/src/postgres/src/test/regress/sql/yb_pg_triggers.sql
index 1a9e3c33cf..1e195c1b87 100644
--- a/src/postgres/src/test/regress/sql/yb_pg_triggers.sql
+++ b/src/postgres/src/test/regress/sql/yb_pg_triggers.sql
@@ -1635,6 +1635,38 @@ select tgrelid::regclass, count(*) from pg_trigger
 */
 drop table trg_clone;
 
+-- Verify that firing state propagates correctly
+CREATE TABLE trgfire (i int) PARTITION BY RANGE (i);
+CREATE TABLE trgfire1 PARTITION OF trgfire FOR VALUES FROM (1) TO (10);
+CREATE OR REPLACE FUNCTION tgf() RETURNS trigger LANGUAGE plpgsql
+  AS $$ begin raise exception 'except'; end $$;
+CREATE TRIGGER tg AFTER INSERT ON trgfire FOR EACH ROW EXECUTE FUNCTION tgf();
+INSERT INTO trgfire VALUES (1);
+ALTER TABLE trgfire DISABLE TRIGGER tg;
+INSERT INTO trgfire VALUES (1);
+CREATE TABLE trgfire2 PARTITION OF trgfire FOR VALUES FROM (10) TO (20);
+INSERT INTO trgfire VALUES (11);
+CREATE TABLE trgfire3 (LIKE trgfire);
+ALTER TABLE trgfire ATTACH PARTITION trgfire3 FOR VALUES FROM (20) TO (30);
+INSERT INTO trgfire VALUES (21);
+CREATE TABLE trgfire4 PARTITION OF trgfire FOR VALUES FROM (30) TO (40) PARTITION BY LIST (i);
+CREATE TABLE trgfire4_30 PARTITION OF trgfire4 FOR VALUES IN (30);
+INSERT INTO trgfire VALUES (30);
+CREATE TABLE trgfire5 (LIKE trgfire) PARTITION BY LIST (i);
+CREATE TABLE trgfire5_40 PARTITION OF trgfire5 FOR VALUES IN (40);
+ALTER TABLE trgfire ATTACH PARTITION trgfire5 FOR VALUES FROM (40) TO (50);
+INSERT INTO trgfire VALUES (40);
+SELECT tgrelid::regclass, tgenabled FROM pg_trigger
+  WHERE tgrelid::regclass IN (SELECT oid from pg_class where relname LIKE 'trgfire%')
+  ORDER BY tgrelid::regclass::text;
+ALTER TABLE trgfire ENABLE TRIGGER tg;
+INSERT INTO trgfire VALUES (1);
+INSERT INTO trgfire VALUES (11);
+INSERT INTO trgfire VALUES (21);
+INSERT INTO trgfire VALUES (30);
+INSERT INTO trgfire VALUES (40);
+DROP TABLE trgfire;
+DROP FUNCTION tgf();
 --
 -- Test the interaction between transition tables and both kinds of
 -- inheritance.  We'll dump the contents of the transition tables in a
diff --git a/src/postgres/src/test/regress/sql/yb_roles.sql b/src/postgres/src/test/regress/sql/yb_roles.sql
index 51a727c2fa..174d885428 100644
--- a/src/postgres/src/test/regress/sql/yb_roles.sql
+++ b/src/postgres/src/test/regress/sql/yb_roles.sql
@@ -1,17 +1,28 @@
--- test yb_extension role
+--
+-- Test yb_extension role
+--
 CREATE USER regress_priv_user;
+CREATE OPERATOR FAMILY alt_opf1 USING hash;
 SET SESSION AUTHORIZATION regress_priv_user;
 CREATE EXTENSION pgcrypto; -- should fail
+CREATE EXTENSION orafce; -- should fail
 \c -
 GRANT yb_extension TO regress_priv_user;
 SET SESSION AUTHORIZATION regress_priv_user;
 CREATE EXTENSION pgcrypto;
 ALTER EXTENSION pgcrypto UPDATE TO '1.3';
 DROP EXTENSION pgcrypto;
+CREATE EXTENSION pg_trgm WITH VERSION '1.3';
+ALTER EXTENSION pg_trgm UPDATE TO '1.4';
+DROP EXTENSION pg_trgm;
+ALTER OPERATOR FAMILY alt_opf1 USING hash ADD -- should fail
+  OPERATOR 1 < (int1, int2);
 \c -
 DROP USER regress_priv_user;
 
--- test yb_fdw role
+--
+-- Test yb_fdw role
+--
 CREATE USER regress_priv_user1;
 CREATE USER regress_priv_user2;
 SET SESSION AUTHORIZATION regress_priv_user1;
@@ -50,3 +61,38 @@ DROP USER regress_priv_user2;
 CREATE ROLE user1 BYPASSRLS;
 ALTER ROLE user1 PASSWORD 'password';
 DROP ROLE user1;
+
+--
+-- Test yb_db_admin role
+--
+-- verify yb_db_admin role can change bypassrls attribute
+CREATE ROLE regress_test_user1;
+SET SESSION ROLE yb_db_admin;
+ALTER ROLE regress_test_user1 WITH BYPASSRLS;
+CREATE ROLE regress_test_user2 WITH BYPASSRLS;
+ALTER ROLE regress_test_user2 WITH NOBYPASSRLS;
+
+-- clean up
+SET SESSION ROLE yugabyte;
+DROP ROLE regress_test_user1;
+DROP ROLE regress_test_user2;
+
+--
+-- Test YB Managed admin role
+--
+RESET SESSION AUTHORIZATION;
+CREATE USER regress_priv_user;
+GRANT yb_extension TO regress_priv_user;
+GRANT yb_fdw TO regress_priv_user;
+GRANT yb_db_admin TO regress_priv_user WITH ADMIN OPTION;
+SET SESSION AUTHORIZATION regress_priv_user;
+CREATE EXTENSION PGAudit;
+ALTER EXTENSION PGAudit UPDATE TO '1.3.2';
+DROP EXTENSION PGAudit;
+CREATE EXTENSION orafce;
+ALTER EXTENSION orafce UPDATE TO '3.14';
+DROP EXTENSION orafce;
+-- removing yb_db_admin role should result in error
+REVOKE yb_db_admin FROM regress_priv_user;
+CREATE EXTENSION PGAudit;
+DROP EXTENSION PGAudit;
diff --git a/src/postgres/src/test/regress/sql/yb_schema.sql b/src/postgres/src/test/regress/sql/yb_schema.sql
index a0c95b3390..fdcc51507f 100644
--- a/src/postgres/src/test/regress/sql/yb_schema.sql
+++ b/src/postgres/src/test/regress/sql/yb_schema.sql
@@ -21,3 +21,21 @@ DROP TABLE S1.TBL;
 SELECT * FROM S2.TBL;
 
 DROP TABLE S2.TBL;
+
+-- verify yb_db_admin role can manage schemas like a superuser
+CREATE SCHEMA test_ns_schema_other;
+CREATE ROLE test_regress_user1;
+SET SESSION AUTHORIZATION yb_db_admin;
+ALTER SCHEMA test_ns_schema_other RENAME TO test_ns_schema_other_new;
+ALTER SCHEMA test_ns_schema_other_new OWNER TO test_regress_user1;
+DROP SCHEMA test_ns_schema_other_new;
+-- verify that the objects were dropped
+SELECT COUNT(*) FROM pg_class WHERE relnamespace =
+    (SELECT oid FROM pg_namespace WHERE nspname = 'test_ns_schema_other_new');
+CREATE SCHEMA test_ns_schema_yb_db_admin;
+ALTER SCHEMA test_ns_schema_yb_db_admin RENAME TO test_ns_schema_yb_db_admin_new;
+ALTER SCHEMA test_ns_schema_yb_db_admin_new OWNER TO test_regress_user1;
+DROP SCHEMA test_ns_schema_yb_db_admin_new;
+-- verify that the objects were dropped
+SELECT COUNT(*) FROM pg_class WHERE relnamespace =
+    (SELECT oid FROM pg_namespace WHERE nspname = 'test_ns_schema_yb_db_admin_new');
diff --git a/src/postgres/src/test/regress/sql/yb_tablegroup_dml.sql b/src/postgres/src/test/regress/sql/yb_tablegroup_dml.sql
index a02b8b4aee..755002928f 100644
--- a/src/postgres/src/test/regress/sql/yb_tablegroup_dml.sql
+++ b/src/postgres/src/test/regress/sql/yb_tablegroup_dml.sql
@@ -92,6 +92,29 @@ TRUNCATE TABLE tab_range;
 TRUNCATE TABLE tab_range2;
 SELECT * FROM tab_range2;
 
+-- truncate tablegroup table with no-tablegroup index
+CREATE TABLE tab_truncate (a INT) TABLEGROUP tg_test1;
+CREATE INDEX ON tab_truncate (a) NO TABLEGROUP;
+INSERT INTO tab_truncate VALUES (1);
+TRUNCATE tab_truncate;
+EXPLAIN (costs off)
+SELECT * FROM tab_truncate WHERE a = 1;
+SELECT * FROM tab_truncate WHERE a = 1;
+DROP TABLE tab_truncate;
+-- truncate no-tablegroup table with tablegroup index
+CREATE TABLE tab_truncate1 (a INT);
+CREATE INDEX ON tab_truncate1 (a) TABLEGROUP tg_test1;
+INSERT INTO tab_truncate1 VALUES (1);
+CREATE TABLE tab_truncate2 (a INT) TABLEGROUP tg_test1;
+INSERT INTO tab_truncate2 VALUES (1);
+TRUNCATE tab_truncate1;
+SELECT * FROM tab_truncate1;
+EXPLAIN (costs off)
+SELECT * FROM tab_truncate1 WHERE a = 1;
+SELECT * FROM tab_truncate1 WHERE a = 1;
+SELECT * FROM tab_truncate2;
+DROP TABLE tab_truncate1, tab_truncate2;
+
 -- ALTER TABLE
 CREATE TABLE tab_range_alter (a INT, b INT, PRIMARY KEY (a ASC)) TABLEGROUP tg_test1;
 INSERT INTO tab_range (a) VALUES (0), (1), (2);
diff --git a/src/postgres/src/test/regress/sql/yb_upgrade_db.sql b/src/postgres/src/test/regress/sql/yb_upgrade_db.sql
new file mode 100644
index 0000000000..b2dd16bf58
--- /dev/null
+++ b/src/postgres/src/test/regress/sql/yb_upgrade_db.sql
@@ -0,0 +1,9 @@
+--
+-- Test case for unsupported collations
+--
+CREATE TABLE collate_test_fail(b text COLLATE "nds-x-icu");
+
+CREATE COLLATION nds from "nds-x-icu";
+
+CREATE TABLE collate_test_fail (a int, b text);
+SELECT * from collate_test_fail where b like 'a'  COLLATE "nds-x-icu";
diff --git a/src/postgres/src/test/regress/yb_feature_serial_schedule b/src/postgres/src/test/regress/yb_feature_serial_schedule
index c3203462e5..c67b2d13e9 100644
--- a/src/postgres/src/test/regress/yb_feature_serial_schedule
+++ b/src/postgres/src/test/regress/yb_feature_serial_schedule
@@ -18,3 +18,4 @@ test: yb_feature_alter_rename
 test: yb_feature_temp
 test: yb_feature_db
 test: yb_feature_matview
+test: yb_upgrade_db
diff --git a/src/postgres/src/test/regress/yb_misc_serial_schedule b/src/postgres/src/test/regress/yb_misc_serial_schedule
index 1c209ac13b..ee1f7c9783 100644
--- a/src/postgres/src/test/regress/yb_misc_serial_schedule
+++ b/src/postgres/src/test/regress/yb_misc_serial_schedule
@@ -12,3 +12,4 @@ test: yb_guc
 test: yb_depend
 test: yb_query_consistent_snapshot
 test: yb_obj_properties_functions
+test: yb_dependency
diff --git a/src/postgres/src/test/regress/yb_pg_partition_prune_schedule b/src/postgres/src/test/regress/yb_pg_partition_prune_schedule
index 67da452e04..acb13bc4a2 100644
--- a/src/postgres/src/test/regress/yb_pg_partition_prune_schedule
+++ b/src/postgres/src/test/regress/yb_pg_partition_prune_schedule
@@ -1,7 +1,5 @@
 # src/test/regress/yb_pg_partition_prune_schedule
 #
 ##########################################################################
-# This testsuite contains testcases ported from postgres partition pruning
-# regression tests.
-##########################################################################
 test: yb_pg_partition_prune
+test: yb_partition_prune_plancache
diff --git a/src/postgres/src/test/regress/yb_pg_stat_schedule b/src/postgres/src/test/regress/yb_pg_stat_schedule
new file mode 100644
index 0000000000..66bc44a50f
--- /dev/null
+++ b/src/postgres/src/test/regress/yb_pg_stat_schedule
@@ -0,0 +1,6 @@
+# src/test/regress/yb_pg_stat_schedule
+#
+####################################################################################################
+# Testsuite for tests covering pg_stat collection behavior.
+####################################################################################################
+test: yb_pg_stat
diff --git a/src/postgres/src/test/regress/yb_proc_schedule b/src/postgres/src/test/regress/yb_proc_schedule
index 7bf9d718ac..fe5d5031e8 100644
--- a/src/postgres/src/test/regress/yb_proc_schedule
+++ b/src/postgres/src/test/regress/yb_proc_schedule
@@ -1,6 +1,7 @@
 # src/test/regress/yb_proc_schedule
 #
 ####################################################################################################
-# This suite includes tests on the partition_hash function
+# This suite includes postgres proc tests.
 ####################################################################################################
 test: yb_hash_code
+test: yb_function
diff --git a/src/postgres/src/test/regress/yb_table_serial_schedule b/src/postgres/src/test/regress/yb_table_serial_schedule
index 796892e8c6..d103e27f1f 100644
--- a/src/postgres/src/test/regress/yb_table_serial_schedule
+++ b/src/postgres/src/test/regress/yb_table_serial_schedule
@@ -7,9 +7,14 @@
 # - DROP
 ################################################################################
 
-# Upstream postgres ported tests.  Skip yb_pg_create_table since it is already
-# run as a dependency in many other schedules.
-test: yb_pg_alter_table
+# Chain of dependencies for yb_pg_alter_table
+test: yb_pg_create_function_1
+test: yb_pg_create_type
+test: yb_pg_create_table
+test: yb_pg_copy
+test: yb_pg_create_index
 
+test: yb_pg_alter_table
+test: yb_alter_table
 test: yb_create_table
 test: yb_drop_table
diff --git a/src/yb/client/client_builder-internal.cc b/src/yb/client/client_builder-internal.cc
index 35cdf89470..b9a9edbc73 100644
--- a/src/yb/client/client_builder-internal.cc
+++ b/src/yb/client/client_builder-internal.cc
@@ -42,13 +42,11 @@
 #include "yb/util/status_fwd.h"
 #include "yb/util/metric_entity.h"
 
-DEFINE_int32(
-    yb_client_num_reactors, 16,
-    "Number of reactor threads for the yb client to communicate with different tservers.");
+DEFINE_int32(yb_client_num_reactors, 16,
+             "Number of reactor threads for the yb client to communicate with different tservers.");
 
-DEFINE_int32(
-    yb_client_admin_operation_timeout_sec, 120,
-    "The number of seconds after which an admin operation should timeout.");
+DEFINE_int32(yb_client_admin_operation_timeout_sec, 120,
+             "The number of seconds after which an admin operation should timeout.");
 
 namespace yb {
 
diff --git a/src/yb/client/table_creator.cc b/src/yb/client/table_creator.cc
index 30c6715710..709d8b6d32 100644
--- a/src/yb/client/table_creator.cc
+++ b/src/yb/client/table_creator.cc
@@ -110,6 +110,11 @@ YBTableCreator& YBTableCreator::tablespace_id(const std::string& tablespace_id)
   return *this;
 }
 
+YBTableCreator& YBTableCreator::is_matview(bool is_matview) {
+  is_matview_ = is_matview;
+  return *this;
+}
+
 YBTableCreator& YBTableCreator::matview_pg_table_id(const std::string& matview_pg_table_id) {
   matview_pg_table_id_ = matview_pg_table_id;
   return *this;
@@ -268,6 +273,10 @@ Status YBTableCreator::Create() {
     req.set_tablespace_id(tablespace_id_);
   }
 
+  if (is_matview_) {
+    req.set_is_matview(*is_matview_);
+  }
+
   if (!matview_pg_table_id_.empty()) {
     req.set_matview_pg_table_id(matview_pg_table_id_);
   }
diff --git a/src/yb/client/table_creator.h b/src/yb/client/table_creator.h
index b1ccdb7696..52172ba7c3 100644
--- a/src/yb/client/table_creator.h
+++ b/src/yb/client/table_creator.h
@@ -70,6 +70,8 @@ class YBTableCreator {
 
   YBTableCreator& tablespace_id(const std::string& tablespace_id);
 
+  YBTableCreator& is_matview(bool is_matview);
+
   YBTableCreator& matview_pg_table_id(const std::string& matview_pg_table_id);
 
   // Sets the schema with which to create the table. Must remain valid for
@@ -218,6 +220,8 @@ class YBTableCreator {
   // The id of the tablespace to which this table is to be associated with.
   std::string tablespace_id_;
 
+  boost::optional<bool> is_matview_;
+
   std::string matview_pg_table_id_;
 
   const TransactionMetadata* txn_ = nullptr;
diff --git a/src/yb/common/common.proto b/src/yb/common/common.proto
index 2184441655..697382c42c 100644
--- a/src/yb/common/common.proto
+++ b/src/yb/common/common.proto
@@ -148,6 +148,10 @@ message TablePropertiesPB {
   optional bool is_ysql_catalog_table = 8 [ default = false ];
   optional bool retain_delete_markers = 9 [ default = false ];
   optional uint64 backfilling_timestamp = 10;
+
+  // Used to distinguish which algorithm should be used for partition key generation,
+  // value == 0 stands for the default buggy algorithm for range partitioning case, see #12189.
+  optional uint32 partition_key_version = 11;
 }
 
 message SchemaPB {
diff --git a/src/yb/common/jsonb.cc b/src/yb/common/jsonb.cc
index 0204723f88..53a478ca5b 100644
--- a/src/yb/common/jsonb.cc
+++ b/src/yb/common/jsonb.cc
@@ -46,9 +46,6 @@ Jsonb::Jsonb(std::string&& jsonb)
     : serialized_jsonb_(std::move(jsonb)) {
 }
 
-void Jsonb::Assign(const std::string& jsonb) {
-  serialized_jsonb_ = jsonb;
-}
 
 void Jsonb::Assign(std::string&& jsonb) {
   serialized_jsonb_ = std::move(jsonb);
@@ -715,11 +712,13 @@ Status Jsonb::ApplyJsonbOperatorToObject(const Slice& jsonb, const QLJsonOperati
   return STATUS_SUBSTITUTE(NotFound, "Couldn't find key $0 in json document", search_key);
 }
 
-Status Jsonb::ApplyJsonbOperators(const QLJsonColumnOperationsPB& json_ops, QLValue* result) const {
+Status Jsonb::ApplyJsonbOperators(const std::string &serialized_json,
+                                  const QLJsonColumnOperationsPB& json_ops,
+                                  QLValue* result) {
   const int num_ops = json_ops.json_operations().size();
 
   Slice jsonop_result;
-  Slice operand(serialized_jsonb_);
+  Slice operand(serialized_json);
   JEntry element_metadata;
   for (int i = 0; i < num_ops; i++) {
     const QLJsonOperationPB &op = json_ops.json_operations().Get(i);
diff --git a/src/yb/common/jsonb.h b/src/yb/common/jsonb.h
index fe169893f5..8a115d7977 100644
--- a/src/yb/common/jsonb.h
+++ b/src/yb/common/jsonb.h
@@ -67,7 +67,6 @@ class Jsonb {
 
   explicit Jsonb(std::string&& jsonb);
 
-  void Assign(const std::string& jsonb);
   void Assign(std::string&& jsonb);
 
   // Creates a serialized jsonb string from plaintext json.
@@ -86,8 +85,9 @@ class Jsonb {
   // Returns a json string for serialized jsonb
   CHECKED_STATUS ToJsonString(std::string* json) const;
 
-  CHECKED_STATUS ApplyJsonbOperators(const QLJsonColumnOperationsPB& json_ops,
-                                     QLValue* result) const;
+  static CHECKED_STATUS ApplyJsonbOperators(const std::string &serialized_json,
+                                            const QLJsonColumnOperationsPB& json_ops,
+                                            QLValue* result);
 
   const std::string& SerializedJsonb() const;
 
diff --git a/src/yb/common/partition.cc b/src/yb/common/partition.cc
index 792c66a85a..1a3b41bd98 100644
--- a/src/yb/common/partition.cc
+++ b/src/yb/common/partition.cc
@@ -158,6 +158,10 @@ void SetColumnIdentifiers(const vector<ColumnId>& column_ids,
 
 } // namespace
 
+bool PartitionSchema::IsHashPartitioning(const PartitionSchemaPB& pb) {
+  return pb.has_hash_schema();
+}
+
 Status PartitionSchema::FromPB(const PartitionSchemaPB& pb,
                                const Schema& schema,
                                PartitionSchema* partition_schema) {
diff --git a/src/yb/common/partition.h b/src/yb/common/partition.h
index 77ad66020b..6aac89d94c 100644
--- a/src/yb/common/partition.h
+++ b/src/yb/common/partition.h
@@ -189,34 +189,32 @@ class PartitionSchema {
   static constexpr int32_t kMaxPartitionKey = std::numeric_limits<uint16_t>::max();
 
   // Deserializes a protobuf message into a partition schema.
-  static CHECKED_STATUS FromPB(const PartitionSchemaPB& pb,
-                               const Schema& schema,
-                               PartitionSchema* partition_schema) WARN_UNUSED_RESULT;
+  static Status FromPB(const PartitionSchemaPB& pb,
+                       const Schema& schema,
+                       PartitionSchema* partition_schema);
+
+  static bool IsHashPartitioning(const PartitionSchemaPB& pb);
 
   // Serializes a partition schema into a protobuf message.
   void ToPB(PartitionSchemaPB* pb) const;
 
-  CHECKED_STATUS EncodeRedisKey(const YBPartialRow& row, std::string* buf) const WARN_UNUSED_RESULT;
-
-  CHECKED_STATUS EncodeRedisKey(const ConstContiguousRow& row,
-                                std::string* buf) const WARN_UNUSED_RESULT;
+  Status EncodeRedisKey(const YBPartialRow& row, std::string* buf) const;
+  Status EncodeRedisKey(const ConstContiguousRow& row, std::string* buf) const;
+  Status EncodeRedisKey(const Slice& slice, std::string* buf) const;
 
-  CHECKED_STATUS EncodeRedisKey(const Slice& slice, std::string* buf) const WARN_UNUSED_RESULT;
-
-  CHECKED_STATUS EncodeKey(const google::protobuf::RepeatedPtrField<QLExpressionPB>& hash_values,
-                           std::string* buf) const WARN_UNUSED_RESULT;
+  Status EncodeKey(const google::protobuf::RepeatedPtrField<QLExpressionPB>& hash_values,
+                   std::string* buf) const;
 
   CHECKED_STATUS EncodeKey(const google::protobuf::RepeatedPtrField<PgsqlExpressionPB>& hash_values,
                            std::string* buf) const WARN_UNUSED_RESULT;
 
   // Appends the row's encoded partition key into the provided buffer.
   // On failure, the buffer may have data partially appended.
-  CHECKED_STATUS EncodeKey(const YBPartialRow& row, std::string* buf) const WARN_UNUSED_RESULT;
+  Status EncodeKey(const YBPartialRow& row, std::string* buf) const;
 
   // Appends the row's encoded partition key into the provided buffer.
   // On failure, the buffer may have data partially appended.
-  CHECKED_STATUS EncodeKey(const ConstContiguousRow& row, std::string* buf) const
-    WARN_UNUSED_RESULT;
+  Status EncodeKey(const ConstContiguousRow& row, std::string* buf) const;
 
   bool IsHashPartitioning() const;
 
@@ -233,8 +231,8 @@ class PartitionSchema {
   static uint16_t DecodeMultiColumnHashValue(const std::string& partition_key);
 
   // Does [partition_key_start, partition_key_end] form a valid range.
-  static CHECKED_STATUS IsValidHashPartitionRange(const std::string& partition_key_start,
-                                                  const std::string& partition_key_end);
+  static Status IsValidHashPartitionRange(const std::string& partition_key_start,
+                                          const std::string& partition_key_end);
 
   static bool IsValidHashPartitionKeyBound(const std::string& partition_key);
 
@@ -255,7 +253,7 @@ class PartitionSchema {
   // TODO(neil) Investigate partitions to support both hash and range schema.
   // - First, use range schema to split the table.
   // - Second, use hash schema to partition each split.
-  CHECKED_STATUS CreatePartitions(int32_t num_tablets, std::vector<Partition>* partitions) const;
+  Status CreatePartitions(int32_t num_tablets, std::vector<Partition>* partitions) const;
 
   // Kudu partition creation
   // NOTE: The following function from Kudu is to support a C++ API instead of SQL or CQL. They
@@ -268,19 +266,19 @@ class PartitionSchema {
   // The number of resulting partitions is the product of the number of hash
   // buckets for each hash bucket component, multiplied by
   // (split_rows.size() + 1).
-  CHECKED_STATUS CreatePartitions(const std::vector<YBPartialRow>& split_rows,
-                                  const Schema& schema,
-                                  std::vector<Partition>* partitions) const WARN_UNUSED_RESULT;
+  Status CreatePartitions(const std::vector<YBPartialRow>& split_rows,
+                          const Schema& schema,
+                          std::vector<Partition>* partitions) const;
 
   // Tests if the partition contains the row.
-  CHECKED_STATUS PartitionContainsRow(const Partition& partition,
-                                      const YBPartialRow& row,
-                                      bool* contains) const WARN_UNUSED_RESULT;
+  Status PartitionContainsRow(const Partition& partition,
+                              const YBPartialRow& row,
+                              bool* contains) const;
 
   // Tests if the partition contains the row.
-  CHECKED_STATUS PartitionContainsRow(const Partition& partition,
-                                      const ConstContiguousRow& row,
-                                      bool* contains) const WARN_UNUSED_RESULT;
+  Status PartitionContainsRow(const Partition& partition,
+                              const ConstContiguousRow& row,
+                              bool* contains) const;
 
   // Returns a text description of the partition suitable for debug printing.
   std::string PartitionDebugString(const Partition& partition, const Schema& schema) const;
@@ -327,53 +325,53 @@ class PartitionSchema {
   };
 
   // Convertion between PB and partition schema.
-  static CHECKED_STATUS KuduFromPB(const PartitionSchemaPB& pb,
-                                 const Schema& schema,
-                                 PartitionSchema* partition_schema);
+  static Status KuduFromPB(const PartitionSchemaPB& pb,
+                           const Schema& schema,
+                           PartitionSchema* partition_schema);
   void KuduToPB(PartitionSchemaPB* pb) const;
 
   // Creates the set of table partitions using multi column hash schema. In this schema, we divide
   // the [ hash(0), hash(max_partition_key) ] range equally into the requested number of intervals.
-  CHECKED_STATUS CreateHashPartitions(int32_t num_tablets,
-                                      std::vector<Partition>* partitions,
-                                      int32_t max_partition_key = kMaxPartitionKey) const;
+  Status CreateHashPartitions(int32_t num_tablets,
+                              std::vector<Partition>* partitions,
+                              int32_t max_partition_key = kMaxPartitionKey) const;
 
   // Creates the set of table partitions using primary-key range schema. In this schema, we divide
   // the table by given ranges in the partitions vector.
-  CHECKED_STATUS CreateRangePartitions(std::vector<Partition>* partitions) const;
+  Status CreateRangePartitions(std::vector<Partition>* partitions) const;
 
   // Encodes the specified columns of a row into lexicographic sort-order preserving format.
-  static CHECKED_STATUS EncodeColumns(const YBPartialRow& row,
-                                      const std::vector<ColumnId>& column_ids,
-                                      std::string* buf);
+  static Status EncodeColumns(const YBPartialRow& row,
+                              const std::vector<ColumnId>& column_ids,
+                              std::string* buf);
 
   // Encodes the specified columns of a row into lexicographic sort-order preserving format.
-  static CHECKED_STATUS EncodeColumns(const ConstContiguousRow& row,
-                                      const std::vector<ColumnId>& column_ids,
-                                      std::string* buf);
+  static Status EncodeColumns(const ConstContiguousRow& row,
+                              const std::vector<ColumnId>& column_ids,
+                              std::string* buf);
 
   // Hashes a compound string of all columns into a 16-bit integer.
   static uint16_t HashColumnCompoundValue(const std::string& compound);
 
   // Encodes the specified columns of a row into 2-byte partition key using the multi column
   // hashing scheme.
-  static CHECKED_STATUS EncodeColumns(const YBPartialRow& row, std::string* buf);
+  static Status EncodeColumns(const YBPartialRow& row, std::string* buf);
 
   // Encodes the specified columns of a row into 2-byte partition key using the multi column
   // hashing scheme.
-  static CHECKED_STATUS EncodeColumns(const ConstContiguousRow& row, std::string* buf);
+  static Status EncodeColumns(const ConstContiguousRow& row, std::string* buf);
 
   // Assigns the row to a hash bucket according to the hash schema.
   template<typename Row>
-  static CHECKED_STATUS BucketForRow(const Row& row,
-                                     const HashBucketSchema& hash_bucket_schema,
-                                     int32_t* bucket);
+  static Status BucketForRow(const Row& row,
+                             const HashBucketSchema& hash_bucket_schema,
+                             int32_t* bucket);
 
   // Private templated helper for PartitionContainsRow.
   template<typename Row>
-  CHECKED_STATUS PartitionContainsRowImpl(const Partition& partition,
-                                          const Row& row,
-                                          bool* contains) const;
+  Status PartitionContainsRowImpl(const Partition& partition,
+                                  const Row& row,
+                                  bool* contains) const;
 
   // Appends the stringified range partition components of a partial row to a
   // vector.
@@ -394,22 +392,22 @@ class PartitionSchema {
 
   // Decodes a range partition key into a partial row, with variable-length
   // fields stored in the arena.
-  CHECKED_STATUS DecodeRangeKey(Slice* encode_key,
-                                YBPartialRow* partial_row,
-                                Arena* arena) const;
+  Status DecodeRangeKey(Slice* encode_key,
+                        YBPartialRow* partial_row,
+                        Arena* arena) const;
 
   // Decodes the hash bucket component of a partition key into its buckets.
   //
   // This should only be called with partition keys created from a row, not with
   // partition keys from a partition.
-  CHECKED_STATUS DecodeHashBuckets(Slice* partition_key, std::vector<int32_t>* buckets) const;
+  Status DecodeHashBuckets(Slice* partition_key, std::vector<int32_t>* buckets) const;
 
   // Clears the state of this partition schema.
   void Clear();
 
   // Validates that this partition schema is valid. Returns OK, or an
   // appropriate error code for an invalid partition schema.
-  CHECKED_STATUS Validate(const Schema& schema) const;
+  Status Validate(const Schema& schema) const;
 
   std::vector<HashBucketSchema> hash_bucket_schemas_;
   RangeSchema range_schema_;
diff --git a/src/yb/common/ql_expr.cc b/src/yb/common/ql_expr.cc
index 432a3e17d9..fca601f015 100644
--- a/src/yb/common/ql_expr.cc
+++ b/src/yb/common/ql_expr.cc
@@ -50,9 +50,8 @@ CHECKED_STATUS QLExprExecutor::EvalExpr(const QLExpressionPB& ql_expr,
       if (temp.IsNull()) {
         result_writer.SetNull();
       } else {
-        common::Jsonb jsonb;
-        temp.MoveToJsonb(&jsonb);
-        RETURN_NOT_OK(jsonb.ApplyJsonbOperators(json_ops, &result_writer.NewValue()));
+        RETURN_NOT_OK(common::Jsonb::ApplyJsonbOperators(
+            temp.Value().jsonb_value(), json_ops, &result_writer.NewValue()));
       }
       break;
     }
@@ -874,14 +873,6 @@ std::string QLTableRow::ToString(const Schema& schema) const {
   return ret;
 }
 
-void QLExprResult::MoveToJsonb(common::Jsonb* out) {
-  if (existing_value_) {
-    out->Assign(existing_value_->jsonb_value());
-    existing_value_ = nullptr;
-  } else {
-    out->Assign(std::move(*value_.mutable_value()->mutable_jsonb_value()));
-  }
-}
 
 const QLValuePB& QLExprResult::Value() const {
   if (existing_value_) {
diff --git a/src/yb/common/ql_expr.h b/src/yb/common/ql_expr.h
index af48e04c67..38d0e82888 100644
--- a/src/yb/common/ql_expr.h
+++ b/src/yb/common/ql_expr.h
@@ -49,7 +49,6 @@ class QLExprResult {
  public:
   const QLValuePB& Value() const;
 
-  void MoveToJsonb(common::Jsonb* out);
 
   void MoveTo(QLValuePB* out);
 
diff --git a/src/yb/common/schema-test.cc b/src/yb/common/schema-test.cc
index a6c8778dc9..f735e4acd1 100644
--- a/src/yb/common/schema-test.cc
+++ b/src/yb/common/schema-test.cc
@@ -88,15 +88,17 @@ TEST(TestSchema, TestSchema) {
   ASSERT_GT(schema.memory_footprint_excluding_this(),
             empty_schema.memory_footprint_excluding_this());
 
-  EXPECT_EQ("Schema [\n"
-            "\tkey[string NOT NULL NOT A PARTITION KEY],\n"
-            "\tuint32val[uint32 NULLABLE NOT A PARTITION KEY],\n"
-            "\tint32val[int32 NOT NULL NOT A PARTITION KEY]\n"
-            "]\nproperties: contain_counters: false is_transactional: false "
-            "consistency_level: STRONG "
-            "use_mangled_column_name: false "
-            "is_ysql_catalog_table: false "
-            "retain_delete_markers: false",
+  EXPECT_EQ(Format("Schema [\n"
+                   "\tkey[string NOT NULL NOT A PARTITION KEY],\n"
+                   "\tuint32val[uint32 NULLABLE NOT A PARTITION KEY],\n"
+                   "\tint32val[int32 NOT NULL NOT A PARTITION KEY]\n"
+                   "]\nproperties: contain_counters: false is_transactional: false "
+                   "consistency_level: STRONG "
+                   "use_mangled_column_name: false "
+                   "is_ysql_catalog_table: false "
+                   "retain_delete_markers: false "
+                   "partition_key_version: $0",
+                   kCurrentPartitionKeyVersion),
             schema.ToString());
   EXPECT_EQ("key[string NOT NULL NOT A PARTITION KEY]", schema.column(0).ToString());
   EXPECT_EQ("uint32 NULLABLE NOT A PARTITION KEY", schema.column(1).TypeToString());
@@ -325,15 +327,17 @@ TEST(TestSchema, TestCreateProjection) {
 
   // By names, without IDs
   ASSERT_OK(schema.CreateProjectionByNames({ "col1", "col2", "col4" }, &partial_schema));
-  EXPECT_EQ("Schema [\n"
-            "\tcol1[string NOT NULL NOT A PARTITION KEY],\n"
-            "\tcol2[string NOT NULL NOT A PARTITION KEY],\n"
-            "\tcol4[string NOT NULL NOT A PARTITION KEY]\n"
-            "]\nproperties: contain_counters: false is_transactional: false "
-            "consistency_level: STRONG "
-            "use_mangled_column_name: false "
-            "is_ysql_catalog_table: false "
-            "retain_delete_markers: false",
+  EXPECT_EQ(Format("Schema [\n"
+                   "\tcol1[string NOT NULL NOT A PARTITION KEY],\n"
+                   "\tcol2[string NOT NULL NOT A PARTITION KEY],\n"
+                   "\tcol4[string NOT NULL NOT A PARTITION KEY]\n"
+                   "]\nproperties: contain_counters: false is_transactional: false "
+                   "consistency_level: STRONG "
+                   "use_mangled_column_name: false "
+                   "is_ysql_catalog_table: false "
+                   "retain_delete_markers: false "
+                   "partition_key_version: $0",
+                   kCurrentPartitionKeyVersion),
             partial_schema.ToString());
 
   // By names, with IDS
@@ -346,10 +350,12 @@ TEST(TestSchema, TestCreateProjection) {
                    "consistency_level: STRONG "
                    "use_mangled_column_name: false "
                    "is_ysql_catalog_table: false "
-                   "retain_delete_markers: false",
+                   "retain_delete_markers: false "
+                   "partition_key_version: $3",
                    schema_with_ids.column_id(0),
                    schema_with_ids.column_id(1),
-                   schema_with_ids.column_id(3)),
+                   schema_with_ids.column_id(3),
+                   kCurrentPartitionKeyVersion),
             partial_schema.ToString());
 
   // By names, with missing names.
@@ -370,10 +376,12 @@ TEST(TestSchema, TestCreateProjection) {
                    "consistency_level: STRONG "
                    "use_mangled_column_name: false "
                    "is_ysql_catalog_table: false "
-                   "retain_delete_markers: false",
+                   "retain_delete_markers: false "
+                   "partition_key_version: $3",
                    schema_with_ids.column_id(0),
                    schema_with_ids.column_id(1),
-                   schema_with_ids.column_id(3)),
+                   schema_with_ids.column_id(3),
+                   kCurrentPartitionKeyVersion),
             partial_schema.ToString());
 }
 
diff --git a/src/yb/common/schema.cc b/src/yb/common/schema.cc
index d4545b3a89..668f9b23b3 100644
--- a/src/yb/common/schema.cc
+++ b/src/yb/common/schema.cc
@@ -133,6 +133,8 @@ size_t ColumnSchema::memory_footprint_including_this() const {
 // TableProperties
 // ------------------------------------------------------------------------------------------------
 
+const TableId kNoCopartitionTableId = "";
+
 void TableProperties::ToTablePropertiesPB(TablePropertiesPB *pb) const {
   if (HasDefaultTimeToLive()) {
     pb->set_default_time_to_live(default_time_to_live_);
@@ -149,6 +151,7 @@ void TableProperties::ToTablePropertiesPB(TablePropertiesPB *pb) const {
   }
   pb->set_is_ysql_catalog_table(is_ysql_catalog_table_);
   pb->set_retain_delete_markers(retain_delete_markers_);
+  pb->set_partition_key_version(partition_key_version_);
 }
 
 TableProperties TableProperties::FromTablePropertiesPB(const TablePropertiesPB& pb) {
@@ -180,6 +183,9 @@ TableProperties TableProperties::FromTablePropertiesPB(const TablePropertiesPB&
   if (pb.has_retain_delete_markers()) {
     table_properties.SetRetainDeleteMarkers(pb.retain_delete_markers());
   }
+  if (pb.has_partition_key_version()) {
+    table_properties.set_partition_key_version(pb.partition_key_version());
+  }
   return table_properties;
 }
 
@@ -208,6 +214,9 @@ void TableProperties::AlterFromTablePropertiesPB(const TablePropertiesPB& pb) {
   if (pb.has_retain_delete_markers()) {
     SetRetainDeleteMarkers(pb.retain_delete_markers());
   }
+  if (pb.has_partition_key_version()) {
+    set_partition_key_version(pb.partition_key_version());
+  }
 }
 
 void TableProperties::Reset() {
@@ -220,6 +229,7 @@ void TableProperties::Reset() {
   num_tablets_ = 0;
   is_ysql_catalog_table_ = false;
   retain_delete_markers_ = false;
+  partition_key_version_ = kCurrentPartitionKeyVersion;
 }
 
 string TableProperties::ToString() const {
@@ -233,9 +243,10 @@ string TableProperties::ToString() const {
     result += Format("copartition_table_id: $0 ", copartition_table_id_);
   }
   return result + Format(
-      "consistency_level: $0 is_ysql_catalog_table: $1 }",
+      "consistency_level: $0 is_ysql_catalog_table: $1 partition_key_version: $2 }",
       consistency_level_,
-      is_ysql_catalog_table_);
+      is_ysql_catalog_table_,
+      partition_key_version_);
 }
 
 // ------------------------------------------------------------------------------------------------
diff --git a/src/yb/common/schema.h b/src/yb/common/schema.h
index b4d93158a3..c2edf77f7f 100644
--- a/src/yb/common/schema.h
+++ b/src/yb/common/schema.h
@@ -298,7 +298,11 @@ class ColumnSchema {
 };
 
 class ContiguousRow;
-const TableId kNoCopartitionTableId = "";
+extern const TableId kNoCopartitionTableId;
+
+// TODO(tsplit): default value must be revisit after #12190 and #12191 are fixed
+constexpr uint32_t kCurrentPartitionKeyVersion = 0;
+
 
 class TableProperties {
  public:
@@ -316,6 +320,7 @@ class TableProperties {
     // Ignoring num_tablets_.
     // Ignoring retain_delete_markers_.
     // Ignoring wal_retention_secs_.
+    // Ignoring partition_key_version_.
   }
 
   bool operator!=(const TableProperties& other) const {
@@ -347,6 +352,7 @@ class TableProperties {
     // Ignoring contain_counters_.
     // Ignoring retain_delete_markers_.
     // Ignoring wal_retention_secs_.
+    // Ignoring partition_key_version_.
     return true;
   }
 
@@ -434,6 +440,14 @@ class TableProperties {
     retain_delete_markers_ = retain_delete_markers;
   }
 
+  uint32_t partition_key_version() const {
+    return partition_key_version_;
+  }
+
+  void set_partition_key_version(uint32_t value) {
+    partition_key_version_ = value;
+  }
+
   void ToTablePropertiesPB(TablePropertiesPB *pb) const;
 
   static TableProperties FromTablePropertiesPB(const TablePropertiesPB& pb);
@@ -460,6 +474,7 @@ class TableProperties {
   bool use_mangled_column_name_ = false;
   int num_tablets_ = 0;
   bool is_ysql_catalog_table_ = false;
+  uint32_t partition_key_version_ = kCurrentPartitionKeyVersion;
 };
 
 typedef std::string PgSchemaName;
diff --git a/src/yb/docdb/doc_pgsql_scanspec.cc b/src/yb/docdb/doc_pgsql_scanspec.cc
index 3d8e10bc4d..abe2435800 100644
--- a/src/yb/docdb/doc_pgsql_scanspec.cc
+++ b/src/yb/docdb/doc_pgsql_scanspec.cc
@@ -238,7 +238,7 @@ void DocPgsqlScanSpec::InitRangeOptions(const PgsqlConditionPB& condition) {
       range_options_indexes_.emplace_back(condition.operands(0).column_id());
 
       if (condition.op() == QL_OP_EQUAL) {
-        auto pv = PrimitiveValue::FromQLValuePB(condition.operands(1).value(), sortingType);
+        auto pv = PrimitiveValue::FromQLValuePBForKey(condition.operands(1).value(), sortingType);
         (*range_options_)[col_idx - num_hash_cols].push_back(std::move(pv));
       } else { // QL_OP_IN
         DCHECK_EQ(condition.op(), QL_OP_IN);
@@ -253,7 +253,7 @@ void DocPgsqlScanSpec::InitRangeOptions(const PgsqlConditionPB& condition) {
         for (int i = 0; i < opt_size; i++) {
           int elem_idx = is_reverse_order ? opt_size - i - 1 : i;
           const auto &elem = options.elems(elem_idx);
-          auto pv = PrimitiveValue::FromQLValuePB(elem, sortingType);
+          auto pv = PrimitiveValue::FromQLValuePBForKey(elem, sortingType);
           (*range_options_)[col_idx - num_hash_cols].push_back(std::move(pv));
         }
       }
diff --git a/src/yb/docdb/doc_ql_scanspec.cc b/src/yb/docdb/doc_ql_scanspec.cc
index 399f118e5c..5e90406b98 100644
--- a/src/yb/docdb/doc_ql_scanspec.cc
+++ b/src/yb/docdb/doc_ql_scanspec.cc
@@ -132,7 +132,7 @@ void DocQLScanSpec::InitRangeOptions(const QLConditionPB& condition) {
       range_options_indexes_.emplace_back(condition.operands(0).column_id());
 
       if (condition.op() == QL_OP_EQUAL) {
-        auto pv = PrimitiveValue::FromQLValuePB(condition.operands(1).value(), sortingType);
+        auto pv = PrimitiveValue::FromQLValuePBForKey(condition.operands(1).value(), sortingType);
         (*range_options_)[col_idx - num_hash_cols].push_back(std::move(pv));
       } else { // QL_OP_IN
         DCHECK_EQ(condition.op(), QL_OP_IN);
@@ -146,7 +146,7 @@ void DocQLScanSpec::InitRangeOptions(const QLConditionPB& condition) {
         for (int i = 0; i < opt_size; i++) {
           int elem_idx = is_reverse_order ? opt_size - i - 1 : i;
           const auto &elem = options.elems(elem_idx);
-          auto pv = PrimitiveValue::FromQLValuePB(elem, sortingType);
+          auto pv = PrimitiveValue::FromQLValuePBForKey(elem, sortingType);
           (*range_options_)[col_idx - num_hash_cols].push_back(std::move(pv));
         }
       }
diff --git a/src/yb/docdb/doc_rowwise_iterator.cc b/src/yb/docdb/doc_rowwise_iterator.cc
index 6005ea2d4f..b9eec78288 100644
--- a/src/yb/docdb/doc_rowwise_iterator.cc
+++ b/src/yb/docdb/doc_rowwise_iterator.cc
@@ -399,6 +399,9 @@ class HybridScanChoices : public ScanChoices {
             // SELECT * FROM ... WHERE c1 IN ();
             // then nothing should pass the filter.
             // To enforce this, we create a range bound (kHighest, kLowest)
+            //
+            // As of D15647 we do not send empty options.
+            // This is kept for backward compatibility during rolling upgrades.
             range_cols_scan_options_lower_[idx
               - num_hash_cols].push_back(PrimitiveValue(ValueType::kHighest));
             range_cols_scan_options_upper_[idx
diff --git a/src/yb/docdb/primitive_value.cc b/src/yb/docdb/primitive_value.cc
index ba1430ef88..b94a0cc6ef 100644
--- a/src/yb/docdb/primitive_value.cc
+++ b/src/yb/docdb/primitive_value.cc
@@ -487,7 +487,6 @@ string PrimitiveValue::ToValue() const {
 
     case ValueType::kCollStringDescending: FALLTHROUGH_INTENDED;
     case ValueType::kCollString:
-      LOG(DFATAL) << "collation encoded string found for docdb value";
       FALLTHROUGH_INTENDED;
     case ValueType::kStringDescending: FALLTHROUGH_INTENDED;
     case ValueType::kString:
@@ -1955,16 +1954,16 @@ PrimitiveValue PrimitiveValue::FromQLValuePB(const QLValuePB& value,
       return PrimitiveValue::VarInt(value.varint_value(), sort_order);
     case QLValuePB::kStringValue: {
       const string& val = value.string_value();
-      // In both Postgres and YCQL, character value cannot have embedded \0 byte.
-      // Redis allows embedded \0 byte but it does not use QLValuePB so will not
-      // come here to pick up 'is_collate'. Therefore, if the value is not empty
-      // and the first byte is \0, it indicates this is a collation encoded string.
-      if (!val.empty() && val[0] == '\0') {
-        // An empty collation encoded string is at least 3 bytes.
-        CHECK_GE(val.size(), 3);
-        return PrimitiveValue(val, sort_order, true /* is_collate */);
-      }
-      return PrimitiveValue(val, sort_order);
+      // In Postgres, character value cannot have embedded \0 byte. Redis allows embedded
+      // \0 byte but it does not use QLValuePB so will not come here. YCQL also allows
+      // embedded \0 byte but in YCQL there is no collation concept so kCollString becomes
+      // a synonym for kString. If the value is not empty and the first byte is \0, in
+      // Postgres it indicates this is a collation encoded string. We use kCollString for
+      // both Postgres and YCQL:
+      // (1) in Postgres kCollString means a collation encoded string;
+      // (2) in YCQL kCollString is a synonym for kString so it is also correct;
+      const bool is_collate = !val.empty() && val[0] == '\0';
+      return PrimitiveValue(val, sort_order, is_collate);
     }
     case QLValuePB::kBinaryValue:
       // TODO consider using dedicated encoding for binary (not string) to avoid overhead of
@@ -2029,6 +2028,14 @@ PrimitiveValue PrimitiveValue::FromQLValuePB(const QLValuePB& value,
   LOG(FATAL) << "Unsupported datatype in PrimitiveValue: " << value.value_case();
 }
 
+PrimitiveValue PrimitiveValue::FromQLValuePBForKey(const QLValuePB& value,
+                                                   SortingType sorting_type) {
+  if (IsNull(value)) {
+    return PrimitiveValue::NullValue(sorting_type);
+  }
+  return FromQLValuePB(value, sorting_type);
+}
+
 void PrimitiveValue::ToQLValuePB(const PrimitiveValue& primitive_value,
                                  const std::shared_ptr<QLType>& ql_type,
                                  QLValuePB* ql_value) {
diff --git a/src/yb/docdb/primitive_value.h b/src/yb/docdb/primitive_value.h
index f72d1dbbb6..cfc110c01e 100644
--- a/src/yb/docdb/primitive_value.h
+++ b/src/yb/docdb/primitive_value.h
@@ -120,6 +120,9 @@ class PrimitiveValue {
   static PrimitiveValue FromQLValuePB(const QLValuePB& value,
                                       SortingType sorting_type);
 
+  static PrimitiveValue FromQLValuePBForKey(const QLValuePB& value,
+                                            SortingType sorting_type);
+
   // Set a primitive value in a QLValuePB.
   static void ToQLValuePB(const PrimitiveValue& pv,
                           const std::shared_ptr<QLType>& ql_type,
diff --git a/src/yb/gutil/casts.h b/src/yb/gutil/casts.h
index 1964195ff3..a481d1c5a5 100644
--- a/src/yb/gutil/casts.h
+++ b/src/yb/gutil/casts.h
@@ -28,6 +28,8 @@
 #include <string.h>         // for memcpy
 #include <limits.h>         // for enumeration casts and tests
 
+#include <limits>
+
 #include "yb/gutil/macros.h"
 #include "yb/gutil/template_util.h"
 #include "yb/gutil/type_traits.h"
@@ -379,10 +381,22 @@ inline bool tight_enum_test_cast(int e_val, Enum* e_var) {
   }
 }
 
+template <class Out, class In>
+Out trim_cast(const In& in) {
+  if (in > std::numeric_limits<Out>::max()) {
+    return std::numeric_limits<Out>::max();
+  }
+  if (in < std::numeric_limits<Out>::min()) {
+    return std::numeric_limits<Out>::min();
+  }
+  return static_cast<Out>(in);
+}
+
 } // namespace yb
 
 using yb::bit_cast;
 using yb::down_cast;
 using yb::implicit_cast;
+using yb::trim_cast;
 
 #endif // YB_GUTIL_CASTS_H
diff --git a/src/yb/integration-tests/create-table-itest.cc b/src/yb/integration-tests/create-table-itest.cc
index aba371f45d..2b6ff4c751 100644
--- a/src/yb/integration-tests/create-table-itest.cc
+++ b/src/yb/integration-tests/create-table-itest.cc
@@ -55,6 +55,8 @@
 #include "yb/master/master_defaults.h"
 #include "yb/master/master_util.h"
 
+#include "yb/tserver/tserver_service.pb.h"
+
 #include "yb/util/metrics.h"
 #include "yb/util/path_util.h"
 #include "yb/util/tsan_util.h"
@@ -102,6 +104,52 @@ class CreateTableITest : public ExternalMiniClusterITestBase {
         .wait(true)
         .Create();
   }
+
+  Result<bool> VerifyTServerTablets(int idx, int num_tablets, int num_leaders,
+                                    const std::string& table_name, bool verify_leaders) {
+    auto tablets = VERIFY_RESULT(cluster_->GetTablets(cluster_->tablet_server(idx)));
+
+    int leader_count = 0, tablet_count = 0;
+    for (const auto& tablet : tablets) {
+      if (tablet.table_name() != table_name) {
+        continue;
+      }
+      if (tablet.state() != tablet::RaftGroupStatePB::RUNNING) {
+        return false;
+      }
+      tablet_count++;
+      if (tablet.is_leader()) {
+        leader_count++;
+      }
+    }
+    LOG(INFO) << "For table " << table_name << ", on tserver " << idx << " number of leaders "
+              << leader_count << " number of tablets " << tablet_count;
+    if ((verify_leaders && leader_count != num_leaders) || tablet_count != num_tablets) {
+      return false;
+    }
+    return true;
+  }
+
+  void PreparePlacementInfo(const std::unordered_map<string, int>& zone_to_replica_count,
+                            int num_replicas, master::PlacementInfoPB* placement_info) {
+    placement_info->set_num_replicas(num_replicas);
+    for (const auto& zone_and_count : zone_to_replica_count) {
+      auto* pb = placement_info->add_placement_blocks();
+      pb->mutable_cloud_info()->set_placement_cloud("c");
+      pb->mutable_cloud_info()->set_placement_region("r");
+      pb->mutable_cloud_info()->set_placement_zone(zone_and_count.first);
+      pb->set_min_num_replicas(zone_and_count.second);
+    }
+  }
+
+  void AddTServerInZone(const string& zone) {
+    vector<std::string> flags = {
+      "--placement_cloud=c",
+      "--placement_region=r",
+      "--placement_zone=" + zone
+    };
+    ASSERT_OK(cluster_->AddTabletServer(true, flags));
+  }
 };
 
 // TODO(bogdan): disabled until ENG-2687
@@ -256,9 +304,8 @@ TEST_F(CreateTableITest, TestCreateWhenMajorityOfReplicasFailCreation) {
   ASSERT_EQ(tablets.size(), kNumTablets) << "Tablets on TS0: " << tablets;
 }
 
-// Regression test for KUDU-1317. Ensure that, when a table is created,
-// the tablets are well spread out across the machines in the cluster and
-// that recovery from failures will be well parallelized.
+// Ensure that, when a table is created,
+// the tablets are well spread out across the machines in the cluster.
 TEST_F(CreateTableITest, TestSpreadReplicasEvenly) {
   const int kNumServers = 10;
   const int kNumTablets = 20;
@@ -277,65 +324,12 @@ TEST_F(CreateTableITest, TestSpreadReplicasEvenly) {
             .num_tablets(kNumTablets)
             .Create());
 
-  // Computing the standard deviation of the number of replicas per server.
-  const double kMeanPerServer = kNumTablets * 3.0 / kNumServers;
-  double sum_squared_deviation = 0;
-  vector<int> tablet_counts;
+  // Load should be equal on all the 10 servers without any deviation.
   for (int ts_idx = 0; ts_idx < kNumServers; ts_idx++) {
     int num_replicas = inspect_->ListTabletsOnTS(ts_idx).size();
     LOG(INFO) << "TS " << ts_idx << " has " << num_replicas << " tablets";
-    double deviation = static_cast<double>(num_replicas) - kMeanPerServer;
-    sum_squared_deviation += deviation * deviation;
-  }
-  double stddev = 0.0;
-  // The denominator in the following formula is kNumServers - 1 instead of kNumServers because
-  // of Bessel's correction for unbiased estimation of variance.
-  if (kNumServers > 1) {
-    stddev = sqrt(sum_squared_deviation / (kNumServers - 1));
-  }
-  LOG(INFO) << "stddev = " << stddev;
-  LOG(INFO) << "mean = " << kMeanPerServer;
-  // We want to ensure that stddev is small compared to mean.
-  const double threshold_ratio = 0.2;
-  // We are verifying that stddev is less than 20% of the mean + 1.0.
-  // "+ 1.0" is needed because stddev is inflated by discreet counting.
-
-  // In 100 runs, the maximum threshold needed was 10%. 20% is a safe value to prevent
-  // failures from random chance.
-  ASSERT_LE(stddev, kMeanPerServer * threshold_ratio + 1.0);
-
-  // Construct a map from tablet ID to the set of servers that each tablet is hosted on.
-  multimap<string, int> tablet_to_servers;
-  for (int ts_idx = 0; ts_idx < kNumServers; ts_idx++) {
-    vector<string> tablets = inspect_->ListTabletsOnTS(ts_idx);
-    for (const string& tablet_id : tablets) {
-      tablet_to_servers.insert(std::make_pair(tablet_id, ts_idx));
-    }
-  }
-
-  // For each server, count how many other servers it shares tablets with.
-  // This is highly correlated to how well parallelized recovery will be
-  // in the case the server crashes.
-  int sum_num_peers = 0;
-  for (int ts_idx = 0; ts_idx < kNumServers; ts_idx++) {
-    vector<string> tablets = inspect_->ListTabletsOnTS(ts_idx);
-    set<int> peer_servers;
-    for (const string& tablet_id : tablets) {
-      auto peer_indexes = tablet_to_servers.equal_range(tablet_id);
-      for (auto it = peer_indexes.first; it != peer_indexes.second; ++it) {
-        peer_servers.insert(it->second);
-      }
-    }
-
-    peer_servers.erase(ts_idx);
-    LOG(INFO) << "Server " << ts_idx << " has " << peer_servers.size() << " peers";
-    sum_num_peers += peer_servers.size();
+    ASSERT_EQ(num_replicas, 6);
   }
-
-  // On average, servers should have at least half the other servers as peers.
-  double avg_num_peers = static_cast<double>(sum_num_peers) / kNumServers;
-  LOG(INFO) << "avg_num_peers = " << avg_num_peers;
-  ASSERT_GE(avg_num_peers, kNumServers / 2);
 }
 
 TEST_F(CreateTableITest, TestNoAllocBlacklist) {
@@ -686,4 +680,315 @@ TEST_F(CreateTableITest, TestNumTabletsFlags) {
   ASSERT_EQ(tablets.size(), 9);
 }
 
+TEST_F(CreateTableITest, OnlyMajorityReplicasWithoutPlacement) {
+  const int kNumTablets = 6;
+  const string kNamespaceName = "my_keyspace";
+  const YQLDatabase kNamespaceType = YQL_DATABASE_CQL;
+  const string kTableName = "test-table";
+  const string kTableName2 = "test-table2";
+  std::unordered_set<int> stopped_tservers;
+  int num_tservers = 3;
+  int num_alive_tservers = 0;
+
+  // Start an RF3.
+  vector<std::string> master_flags = {
+    "--tserver_unresponsive_timeout_ms=5000"
+  };
+  ASSERT_NO_FATALS(StartCluster({}, master_flags, num_tservers));
+  num_alive_tservers = 3;
+  LOG(INFO) << "Started an RF3 cluster with 3 tservers and 1 master";
+
+  // Stop a node.
+  ASSERT_OK(cluster_->tablet_server(2)->Pause());
+  LOG(INFO) << "Paused tserver index 2";
+
+  // Wait for the master leader to mark it dead.
+  ASSERT_OK(cluster_->WaitForMasterToMarkTSDead(2));
+  stopped_tservers.emplace(2);
+  --num_alive_tservers;
+  LOG(INFO) << "TServer index 2 is now marked DEAD by the leader master";
+
+  // Now issue a create table.
+  // Create a namespace.
+  ASSERT_OK(client_->CreateNamespaceIfNotExists(kNamespaceName, kNamespaceType));
+  LOG(INFO) << "Created YQL Namespace " << kNamespaceName;
+
+  client::YBSchema client_schema(client::YBSchemaFromSchema(GetSimpleTestSchema()));
+
+  YBTableName table_name(kNamespaceType, kNamespaceName, kTableName);
+  std::unique_ptr<client::YBTableCreator> table_creator1(client_->NewTableCreator());
+  ASSERT_OK(table_creator1->table_name(table_name)
+                            .schema(&client_schema)
+                            .num_tablets(kNumTablets)
+                            .wait(true)
+                            .Create());
+  LOG(INFO) << "Created table " << kNamespaceName << "." << kTableName;
+
+  // Verify that each tserver contains kNumTablets with kNumTablets/2 leaders.
+  ASSERT_OK(WaitFor([&]() -> Result<bool> {
+    for (int i = 0; i < num_tservers; i++) {
+      if (stopped_tservers.count(i)) {
+        continue;
+      }
+      if (!VERIFY_RESULT(VerifyTServerTablets(
+          i, kNumTablets, kNumTablets / num_alive_tservers, kTableName, true))) {
+        return false;
+      }
+    }
+    return true;
+  }, 120s * kTimeMultiplier, "Are tablets running", 1s));
+
+  // Stop another node. Create table should now fail.
+  ASSERT_OK(cluster_->tablet_server(1)->Pause());
+  LOG(INFO) << "Paused tserver index 1";
+
+  // Wait for the master leader to mark it dead.
+  ASSERT_OK(cluster_->WaitForMasterToMarkTSDead(1));
+  stopped_tservers.emplace(1);
+  --num_alive_tservers;
+  LOG(INFO) << "TServer index 1 is now marked DEAD by the leader master";
+
+  YBTableName table_name2(kNamespaceType, kNamespaceName, kTableName2);
+  std::unique_ptr<client::YBTableCreator> table_creator2(client_->NewTableCreator());
+  ASSERT_NOK(table_creator2->table_name(table_name2)
+                            .schema(&client_schema)
+                            .num_tablets(kNumTablets)
+                            .wait(true)
+                            .timeout(10s * kTimeMultiplier)
+                            .Create());
+
+  // Now resume the paused tservers.
+  ASSERT_OK(cluster_->tablet_server(2)->Resume());
+  ASSERT_OK(cluster_->tablet_server(1)->Resume());
+  stopped_tservers.erase(2);
+  stopped_tservers.erase(1);
+  ++num_alive_tservers;
+  ++num_alive_tservers;
+  LOG(INFO) << "Tablet Server 2 and 1 resumed";
+
+  // Verify each tserver getting kNumTablets with leadership of kNumTablets/3.
+  ASSERT_OK(WaitFor([&]() -> Result<bool> {
+    for (int i = 0; i < num_tservers; i++) {
+      if (!VERIFY_RESULT(VerifyTServerTablets(
+          i, kNumTablets, kNumTablets / num_alive_tservers, kTableName, true))) {
+        return false;
+      }
+    }
+    return true;
+  }, 120s * kTimeMultiplier, "Are tablets running", 1s));
+}
+
+TEST_F(CreateTableITest, OnlyMajorityReplicasWithPlacement) {
+  const int kNumTablets = 6;
+  const string kNamespaceName = "my_keyspace";
+  const YQLDatabase kNamespaceType = YQL_DATABASE_CQL;
+  const string kTableName1 = "test-table1";
+  const string kTableName2 = "test-table2";
+  const string kTableName3 = "test-table3";
+  const string kTableName4 = "test-table4";
+  const string kTableName5 = "test-table5";
+  std::unordered_set<int> stopped_tservers;
+  int num_tservers = 3;
+  int num_alive_tservers = 0;
+
+  vector<std::string> master_flags = {
+    "--tserver_unresponsive_timeout_ms=5000"
+  };
+  vector<std::string> tserver_flags = {
+    "--placement_cloud=c",
+    "--placement_region=r",
+    "--placement_zone=z${index}"
+  };
+
+  // Test - 1.
+  // Placement Policy: c.r.z1:1, c.r.z2:1, c.r.z3:1 with num_replicas as 3.
+  // Available tservers: 1 in c.r.z1 and 1 in c.r.z2.
+  // Result: Create Table should succeed.
+
+  // Start an RF3 with tservers placed in "c.r.z0,c.r.z1,c.r.z2".
+  ASSERT_NO_FATALS(StartCluster(tserver_flags, master_flags, 3));
+  num_alive_tservers = 3;
+  LOG(INFO) << "Started an RF3 cluster with 3 tservers in c.r.z0,c.r.z1,c.r.z2 and 1 master";
+
+  // Modify placement info to contain at least one replica in each of the three zones.
+  master::ReplicationInfoPB replication_info;
+  auto* placement_info = replication_info.mutable_live_replicas();
+  PreparePlacementInfo({ {"z0", 1}, {"z1", 1}, {"z2", 1} }, 3, placement_info);
+
+  ASSERT_OK(client_->SetReplicationInfo(replication_info));
+  LOG(INFO) << "Set replication info to c.r.z0,c.r.z1,c.r.z2 with num_replicas as 3";
+
+  // Create a namespace.
+  ASSERT_OK(client_->CreateNamespaceIfNotExists(kNamespaceName, kNamespaceType));
+  LOG(INFO) << "Created YQL Namespace " << kNamespaceName;
+
+  // Create a schema.
+  client::YBSchema client_schema(client::YBSchemaFromSchema(GetSimpleTestSchema()));
+  LOG(INFO) << "Created schema for tables";
+
+  // Bring down one tserver in z2.
+  ASSERT_OK(cluster_->tablet_server(2)->Pause());
+  LOG(INFO) << "Paused tserver with index 2";
+
+  // Wait for the master leader to mark them dead.
+  ASSERT_OK(cluster_->WaitForMasterToMarkTSDead(2));
+  stopped_tservers.emplace(2);
+  --num_alive_tservers;
+  LOG(INFO) << "Tserver index 2 is now marked DEAD by the leader master";
+
+  // Issue a create table request, it should succeed.
+  YBTableName table_name1(kNamespaceType, kNamespaceName, kTableName1);
+  std::unique_ptr<client::YBTableCreator> table_creator1(client_->NewTableCreator());
+  ASSERT_OK(table_creator1->table_name(table_name1)
+                            .schema(&client_schema)
+                            .num_tablets(kNumTablets)
+                            .wait(true)
+                            .Create());
+  LOG(INFO) << "Created table " << kNamespaceName << "." << kTableName1;
+
+  // Verify that each tserver contains kNumTablets with kNumTablets/2 leaders.
+  ASSERT_OK(WaitFor([&]() -> Result<bool> {
+    for (int i = 0; i < num_tservers; i++) {
+      if (stopped_tservers.count(i)) {
+        continue;
+      }
+      if (!VERIFY_RESULT(VerifyTServerTablets(
+          i, kNumTablets, kNumTablets / num_alive_tservers, kTableName1, true))) {
+        return false;
+      }
+    }
+    return true;
+  }, 120s * kTimeMultiplier, "Are tablets running", 1s));
+
+  // Test - 2.
+  // Placement Policy: c.r.z1:1, c.r.z2:1, c.r.z3:1 with num_replicas as 3.
+  // Available tservers: 1 in c.r.z1.
+  // Result: CreateTable will fail because we don't have a raft quorum underneath.
+
+  // Bring down another tserver, create table should now fail.
+  ASSERT_OK(cluster_->tablet_server(1)->Pause());
+  LOG(INFO) << "Paused tserver with index 1";
+
+  // Wait for the master leader to mark them dead.
+  ASSERT_OK(cluster_->WaitForMasterToMarkTSDead(1));
+  stopped_tservers.emplace(1);
+  --num_alive_tservers;
+  LOG(INFO) << "Tserver index 1 is now marked DEAD by the leader master";
+
+  YBTableName table_name2(kNamespaceType, kNamespaceName, kTableName2);
+  std::unique_ptr<client::YBTableCreator> table_creator2(client_->NewTableCreator());
+  ASSERT_NOK(table_creator2->table_name(table_name2)
+                            .schema(&client_schema)
+                            .num_tablets(kNumTablets)
+                            .wait(true)
+                            .timeout(10s * kTimeMultiplier)
+                            .Create());
+
+  // Test - 3.
+  // Placement Policy: c.r.z1:1, c.r.z2:1, c.r.z3:1 with num_replicas as 3.
+  // Available tservers: 2 in c.r.z1.
+  // Result: Create Table will not succeed.
+
+  // Add another tserver in c.r.z0. Create table should still fail after adding.
+  AddTServerInZone("z0");
+  ASSERT_OK(cluster_->WaitForTabletServerCount(++num_tservers, MonoDelta::FromSeconds(20)));
+  ASSERT_OK(cluster_->WaitForMasterToMarkTSAlive(3));
+  ++num_alive_tservers;
+
+  YBTableName table_name3(kNamespaceType, kNamespaceName, kTableName3);
+  std::unique_ptr<client::YBTableCreator> table_creator3(client_->NewTableCreator());
+  ASSERT_NOK(table_creator3->table_name(table_name3)
+                            .schema(&client_schema)
+                            .num_tablets(kNumTablets)
+                            .wait(true)
+                            .timeout(10s * kTimeMultiplier)
+                            .Create());
+
+  // Test - 4.
+  // Placement Policy: c.r.z1:1, c.r.z2:1, c.r.z3:1 with num_replicas as 5.
+  // Available tservers: 2 in c.r.z1 and 1 in c.r.z2.
+  // Result: Create Table should succeed.
+
+  // Increase the number of replicas to 5 with the same placement config.
+  master::ReplicationInfoPB replication_info2;
+  auto* placement_info2 = replication_info2.mutable_live_replicas();
+  PreparePlacementInfo({ {"z0", 1}, {"z1", 1}, {"z2", 1} }, 5, placement_info2);
+
+  ASSERT_OK(client_->SetReplicationInfo(replication_info2));
+  LOG(INFO) << "Set replication info to c.r.z0,c.r.z1,c.r.z2 with num_replicas as 5";
+
+  // Now resume tserver 2 and wait for master to mark it alive.
+  ASSERT_OK(cluster_->tablet_server(2)->Resume());
+  ASSERT_OK(cluster_->WaitForMasterToMarkTSAlive(2));
+  ++num_alive_tservers;
+  LOG(INFO) << "Tablet Server index 2 resumed";
+
+  // Create table should now succeed.
+  YBTableName table_name4(kNamespaceType, kNamespaceName, kTableName4);
+  std::unique_ptr<client::YBTableCreator> table_creator4(client_->NewTableCreator());
+  ASSERT_OK(table_creator4->table_name(table_name4)
+                            .schema(&client_schema)
+                            .num_tablets(kNumTablets)
+                            .wait(true)
+                            .Create());
+  // Validate data.
+  ASSERT_OK(WaitFor([&]() -> Result<bool> {
+    for (int i = 0; i < num_tservers; i++) {
+      if (stopped_tservers.count(i)) {
+        continue;
+      }
+      if (!VERIFY_RESULT(VerifyTServerTablets(
+          i, kNumTablets, kNumTablets / num_alive_tservers, kTableName4, true))) {
+        return false;
+      }
+    }
+    return true;
+  }, 120s * kTimeMultiplier, "Are tablets running", 1s));
+
+  // Test - 5.
+  // Placement Policy: c.r.z1:1, c.r.z2:1, c.r.z3:1 with num_replicas as 5 as live replicas
+  // and c.r.z4:1 with num_replicas as 1 as read_replica.
+  // Available tservers: 2 in c.r.z1 and 1 in c.r.z2.
+  // Result: Create Table should succeed despite having 0 read replica nodes.
+
+  // Modify Placement info to contain a read replica also.
+  master::ReplicationInfoPB replication_info3;
+  auto* placement_info3 = replication_info3.mutable_live_replicas();
+  PreparePlacementInfo({ {"z0", 1}, {"z1", 1}, {"z2", 1} }, 5, placement_info3);
+  auto* read_placement_info = replication_info3.add_read_replicas();
+  read_placement_info->set_placement_uuid("read-replica");
+  PreparePlacementInfo({ {"z4", 1} }, 1, read_placement_info);
+  ASSERT_OK(client_->SetReplicationInfo(replication_info3));
+  LOG(INFO) << "Set replication info to " << replication_info3.ShortDebugString();
+
+  // Try creating a table. It should succeed.
+  YBTableName table_name5(kNamespaceType, kNamespaceName, kTableName5);
+  std::unique_ptr<client::YBTableCreator> table_creator5(client_->NewTableCreator());
+  ASSERT_OK(table_creator1->table_name(table_name5)
+                            .schema(&client_schema)
+                            .num_tablets(kNumTablets)
+                            .wait(true)
+                            .Create());
+
+  // Resume tserver 1.
+  ASSERT_OK(cluster_->tablet_server(1)->Resume());
+  stopped_tservers.erase(1);
+  ++num_alive_tservers;
+  LOG(INFO) << "Tablet server index 1 resumed";
+
+  // LB should move data to this fourth server also.
+  ASSERT_OK(WaitFor([&]() -> Result<bool> {
+    for (int i = 0; i < num_tservers; i++) {
+      if (stopped_tservers.count(i)) {
+        continue;
+      }
+      if (!VERIFY_RESULT(VerifyTServerTablets(
+          i, kNumTablets, kNumTablets / num_alive_tservers, kTableName4, false))) {
+        return false;
+      }
+    }
+    return true;
+  }, 120s * kTimeMultiplier, "Are tablets running", 1s));
+}
+
 }  // namespace yb
diff --git a/src/yb/integration-tests/external_mini_cluster.cc b/src/yb/integration-tests/external_mini_cluster.cc
index a4d69fcaab..4c811532a6 100644
--- a/src/yb/integration-tests/external_mini_cluster.cc
+++ b/src/yb/integration-tests/external_mini_cluster.cc
@@ -1255,12 +1255,13 @@ Status ExternalMiniCluster::WaitForInitDb() {
   }
 }
 
-Result<bool> ExternalMiniCluster::is_ts_stale(int ts_idx) {
+Result<bool> ExternalMiniCluster::is_ts_stale(int ts_idx, MonoDelta deadline) {
   auto proxy = GetMasterProxy<master::MasterClusterProxy>();
   std::shared_ptr<rpc::RpcController> controller = std::make_shared<rpc::RpcController>();
   master::ListTabletServersRequestPB req;
   master::ListTabletServersResponsePB resp;
   controller->Reset();
+  controller->set_timeout(deadline);
 
   RETURN_NOT_OK(proxy.ListTabletServers(req, &resp, controller.get()));
 
@@ -1295,6 +1296,22 @@ Result<bool> ExternalMiniCluster::is_ts_stale(int ts_idx) {
   return is_stale;
 }
 
+CHECKED_STATUS ExternalMiniCluster::WaitForMasterToMarkTSAlive(int ts_idx, MonoDelta deadline) {
+  RETURN_NOT_OK(WaitFor([&]() -> Result<bool> {
+    return !VERIFY_RESULT(is_ts_stale(ts_idx));
+  }, deadline * kTimeMultiplier, "Is TS Alive", 1s));
+
+  return Status::OK();
+}
+
+CHECKED_STATUS ExternalMiniCluster::WaitForMasterToMarkTSDead(int ts_idx, MonoDelta deadline) {
+  RETURN_NOT_OK(WaitFor([&]() -> Result<bool> {
+    return is_ts_stale(ts_idx);
+  }, deadline * kTimeMultiplier, "Is TS dead", 1s));
+
+  return Status::OK();
+}
+
 string ExternalMiniCluster::GetBindIpForTabletServer(int index) const {
   if (opts_.use_even_ips) {
     return Substitute("127.0.0.$0", (index + 1) * 2);
diff --git a/src/yb/integration-tests/external_mini_cluster.h b/src/yb/integration-tests/external_mini_cluster.h
index f708fa96e7..1406a193e6 100644
--- a/src/yb/integration-tests/external_mini_cluster.h
+++ b/src/yb/integration-tests/external_mini_cluster.h
@@ -64,6 +64,7 @@
 #include "yb/util/monotime.h"
 #include "yb/util/net/net_util.h"
 #include "yb/util/status.h"
+#include "yb/util/tsan_util.h"
 
 namespace yb {
 
@@ -449,7 +450,14 @@ class ExternalMiniCluster : public MiniClusterBase {
   string data_root() const { return data_root_; }
 
   // Return true if the tserver has been marked as DEAD by master leader.
-  Result<bool> is_ts_stale(int ts_idx);
+  Result<bool> is_ts_stale(
+      int ts_idx, MonoDelta deadline = MonoDelta::FromSeconds(120) * kTimeMultiplier);
+
+  CHECKED_STATUS WaitForMasterToMarkTSAlive(
+      int ts_idx, MonoDelta deadline = MonoDelta::FromSeconds(120) * kTimeMultiplier);
+
+  CHECKED_STATUS WaitForMasterToMarkTSDead(
+      int ts_idx, MonoDelta deadline = MonoDelta::FromSeconds(120) * kTimeMultiplier);
 
  protected:
   FRIEND_TEST(MasterFailoverTest, TestKillAnyMaster);
diff --git a/src/yb/integration-tests/tablet-split-itest-base.cc b/src/yb/integration-tests/tablet-split-itest-base.cc
index b923371af8..1986acc175 100644
--- a/src/yb/integration-tests/tablet-split-itest-base.cc
+++ b/src/yb/integration-tests/tablet-split-itest-base.cc
@@ -133,7 +133,7 @@ Status SplitTablet(master::CatalogManagerIf* catalog_mgr, const tablet::Tablet&
   tablet.TEST_db()->GetProperty(rocksdb::DB::Properties::kAggregatedTableProperties, &properties);
   LOG(INFO) << "DB properties: " << properties;
 
-  return catalog_mgr->SplitTablet(tablet_id, true /* is_manual_split */);
+  return catalog_mgr->SplitTablet(tablet_id, master::ManualSplit::kTrue);
 }
 
 Status DoSplitTablet(master::CatalogManagerIf* catalog_mgr, const tablet::Tablet& tablet) {
diff --git a/src/yb/integration-tests/tablet-split-itest.cc b/src/yb/integration-tests/tablet-split-itest.cc
index ecc2d16a7e..c0da1a7667 100644
--- a/src/yb/integration-tests/tablet-split-itest.cc
+++ b/src/yb/integration-tests/tablet-split-itest.cc
@@ -117,6 +117,7 @@ DECLARE_int64(tablet_force_split_threshold_bytes);
 DECLARE_int32(tserver_heartbeat_metrics_interval_ms);
 DECLARE_bool(TEST_validate_all_tablet_candidates);
 DECLARE_uint64(outstanding_tablet_split_limit);
+DECLARE_uint64(outstanding_tablet_split_limit_per_tserver);
 DECLARE_double(TEST_fail_tablet_split_probability);
 DECLARE_bool(TEST_skip_post_split_compaction);
 DECLARE_int32(TEST_nodes_per_cloud);
@@ -907,6 +908,7 @@ class AutomaticTabletSplitITest : public TabletSplitITest {
     ANNOTATE_UNPROTECTED_WRITE(FLAGS_enable_automatic_tablet_splitting) = true;
     ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_validate_all_tablet_candidates) = false;
     ANNOTATE_UNPROTECTED_WRITE(FLAGS_outstanding_tablet_split_limit) = 5;
+    ANNOTATE_UNPROTECTED_WRITE(FLAGS_outstanding_tablet_split_limit_per_tserver) = 5;
   }
 
  protected:
@@ -1503,7 +1505,7 @@ TEST_F(AutomaticTabletSplitITest, LimitNumberOfOutstandingTabletSplits) {
     peers = ListTableActiveTabletLeadersPeers(cluster_.get(), table_->id());
     num_tablets = peers.size();
 
-    // There should be kTabletSplitLimit intial tablets + kTabletSplitLimit new tablets per loop.
+    // There should be kTabletSplitLimit initial tablets + kTabletSplitLimit new tablets per loop.
     EXPECT_EQ(num_tablets, (split_round + 2) * kTabletSplitLimit);
   }
 
@@ -1515,6 +1517,94 @@ TEST_F(AutomaticTabletSplitITest, LimitNumberOfOutstandingTabletSplits) {
   cluster_->Shutdown();
 }
 
+TEST_F(AutomaticTabletSplitITest, LimitNumberOfOutstandingTabletSplitsPerTserver) {
+  constexpr int kNumRowsPerBatch = 2000;
+
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_outstanding_tablet_split_limit) = 5;
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_outstanding_tablet_split_limit_per_tserver) = 1;
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_tablet_split_low_phase_shard_count_per_node) = 10000;
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_tablet_split_low_phase_size_threshold_bytes) = 0;
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_rocksdb_disable_compactions) = true;
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_pause_tserver_get_split_key) = true;
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_fail_tablet_split_probability) = 1.0;
+  // Need to disable load balancing until the first tablet is split, otherwise it might end up
+  // being overreplicated on 4 tservers when we split, resulting in the split children being on 4
+  // tservers.
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_enable_load_balancing) = false;
+
+  ASSERT_OK(cluster_->AddTabletServer());
+  ASSERT_OK(cluster_->AddTabletServer());
+  ASSERT_OK(cluster_->WaitForTabletServerCount(5));
+
+  SetNumTablets(2);
+  CreateTable();
+
+  auto catalog_mgr = ASSERT_RESULT(catalog_manager());
+  auto table_info = catalog_mgr->GetTableInfo(table_->id());
+
+  auto peers = ListTableActiveTabletLeadersPeers(cluster_.get(), table_->id());
+  ASSERT_EQ(peers.size(), 2);
+  ASSERT_OK(WriteRows(kNumRowsPerBatch, 1));
+  for (const auto& peer : peers) {
+    ASSERT_OK(WaitFor([&]() {
+      return peer->shared_tablet()->transaction_participant()->TEST_CountIntents().first == 0;
+    }, 30s, "Did not apply transaction from intents db in time."));
+  }
+  // Flush to ensure an SST file is generated so splitting can occur.
+  // One of the tablets (call it A) should be automatically split after the flush. Since RF=3 and we
+  // have 5 tservers, the other tablet (B) must share at least one tserver with A. Since we limit
+  // the number of outstanding splits on a tserver to 1, B should not be split (since that would
+  // result in two outstanding splits on the tserver that hosted a replica of A and B).
+  ASSERT_OK(FlushAllTabletReplicas(peers[0]->tablet_id(), table_->id()));
+  ASSERT_OK(FlushAllTabletReplicas(peers[1]->tablet_id(), table_->id()));
+
+  // Check that no more than 1 split task is created (the split task should be counted as an
+  // ongoing split).
+  SleepForBgTaskIters(4);
+  int num_split_tasks = 0;
+  for (const auto& task : table_info->GetTasks()) {
+    // These tasks will retry automatically until they succeed or fail.
+    if (task->type() == yb::server::MonitoredTask::ASYNC_GET_TABLET_SPLIT_KEY ||
+        task->type() == yb::server::MonitoredTask::ASYNC_SPLIT_TABLET) {
+      ++num_split_tasks;
+    }
+  }
+  ASSERT_EQ(num_split_tasks, 1);
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_pause_tserver_get_split_key) = false;
+
+  // Check that non-running child tablets count against the per-tserver split limit, and so only
+  // one split is triggered.
+  SleepForBgTaskIters(4);
+  ASSERT_EQ(table_info->GetTablets().size(), 3);
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_fail_tablet_split_probability) = 0.0;
+
+  ASSERT_OK(WaitForTabletSplitCompletion(3));
+  ASSERT_NOK(WaitForTabletSplitCompletion(
+      4,                                    // expected_non_split_tablets
+      0,                                    // expected_split_tablets (default)
+      0,                                    // num_replicas_online (default)
+      client::kTableName,                   // table (default)
+      false));                              // core_dump_on_failure
+
+  // Add a 6th tserver. Tablet B should be load balanced onto the three tservers that do not have
+  // replicas of tablet A, and should subsequently split. Note that the children of A should remain
+  // on the same 3 tservers as A, since we don't move compacting tablets (this is important to
+  // ensure that there are no ongoing splits on the 3 tservers that B is on).
+  ASSERT_OK(cluster_->AddTabletServer());
+  ASSERT_OK(cluster_->WaitForTabletServerCount(6));
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_enable_load_balancing) = true;
+  ASSERT_OK(WaitFor([&]() {
+    std::unordered_set<std::string> seen_tservers;
+    for (const auto& peer : ListTableActiveTabletPeers(cluster_.get(), table_->id())) {
+      seen_tservers.insert(peer->permanent_uuid());
+    }
+    LOG(INFO) << "seen_tservers.size(): " <<  seen_tservers.size();
+    return seen_tservers.size() == 6;
+  }, 30s * kTimeMultiplier, "Did not load balance in time."));
+
+  ASSERT_OK(WaitForTabletSplitCompletion(4));
+}
+
 TEST_F(AutomaticTabletSplitITest, DroppedTablesExcludedFromOutstandingSplitLimit) {
   constexpr int kNumRowsPerBatch = 1000;
   constexpr int kTabletSplitLimit = 1;
@@ -1760,7 +1850,7 @@ TEST_F(TabletSplitSingleServerITest, MaxFileSizeTTLTabletOnlyValidForManualSplit
   // Tablet should still be a valid candidate if ignore_ttl_validation is set to true
   // (e.g. for manual tablet splitting).
   ASSERT_OK(split_manager->ValidateSplitCandidateTablet(*source_tablet_info,
-      true /* ignore_ttl_validation */));
+      master::IgnoreTtlValidation::kTrue, master::IgnoreDisabledList::kTrue));
 }
 
 TEST_F(TabletSplitSingleServerITest, AutoSplitNotValidOnceCheckedForTtl) {
@@ -1796,7 +1886,7 @@ TEST_F(TabletSplitSingleServerITest, AutoSplitNotValidOnceCheckedForTtl) {
   // is true (e.g. in the case of manual tablet splitting).
   split_manager->MarkTtlTableForSplitIgnore(table_->id());
   ASSERT_OK(split_manager->ValidateSplitCandidateTable(table_info,
-      true /* ignore_disabled_list */));
+      master::IgnoreDisabledList::kTrue));
 }
 
 TEST_F(TabletSplitSingleServerITest, TabletServerOrphanedPostSplitData) {
diff --git a/src/yb/master/async_rpc_tasks.cc b/src/yb/master/async_rpc_tasks.cc
index 1363471239..892ef7e262 100644
--- a/src/yb/master/async_rpc_tasks.cc
+++ b/src/yb/master/async_rpc_tasks.cc
@@ -1324,7 +1324,7 @@ bool AsyncRemoveTableFromTablet::SendRequest(int attempt) {
 namespace {
 
 bool IsDefinitelyPermanentError(const Status& s) {
-  return s.IsInvalidArgument() || s.IsNotFound();
+  return s.IsInvalidArgument() || s.IsNotFound() || s.IsNotSupported();
 }
 
 } // namespace
@@ -1334,7 +1334,7 @@ bool IsDefinitelyPermanentError(const Status& s) {
 // ============================================================================
 AsyncGetTabletSplitKey::AsyncGetTabletSplitKey(
     Master* master, ThreadPool* callback_pool, const scoped_refptr<TabletInfo>& tablet,
-    bool is_manual_split, DataCallbackType result_cb)
+    const ManualSplit is_manual_split, DataCallbackType result_cb)
     : AsyncTabletLeaderTask(master, callback_pool, tablet), result_cb_(result_cb) {
   req_.set_tablet_id(tablet_id());
   req_.set_is_manual_split(is_manual_split);
diff --git a/src/yb/master/async_rpc_tasks.h b/src/yb/master/async_rpc_tasks.h
index 154e6c2eee..e5c65baa8e 100644
--- a/src/yb/master/async_rpc_tasks.h
+++ b/src/yb/master/async_rpc_tasks.h
@@ -695,7 +695,7 @@ class AsyncGetTabletSplitKey : public AsyncTabletLeaderTask {
 
   AsyncGetTabletSplitKey(
       Master* master, ThreadPool* callback_pool, const scoped_refptr<TabletInfo>& tablet,
-      bool is_manual_split, DataCallbackType result_cb);
+      ManualSplit is_manual_split, DataCallbackType result_cb);
 
   Type type() const override { return ASYNC_GET_TABLET_SPLIT_KEY; }
 
diff --git a/src/yb/master/catalog_entity_info.cc b/src/yb/master/catalog_entity_info.cc
index 7a6abdc803..1b4e03150b 100644
--- a/src/yb/master/catalog_entity_info.cc
+++ b/src/yb/master/catalog_entity_info.cc
@@ -334,6 +334,14 @@ bool TableInfo::colocated() const {
   return LockForRead()->pb.colocated();
 }
 
+std::string TableInfo::matview_pg_table_id() const {
+  return LockForRead()->pb.matview_pg_table_id();
+}
+
+bool TableInfo::is_matview() const {
+  return LockForRead()->pb.is_matview();
+}
+
 std::string TableInfo::indexed_table_id() const {
   return LockForRead()->indexed_table_id();
 }
diff --git a/src/yb/master/catalog_entity_info.h b/src/yb/master/catalog_entity_info.h
index f3d07805cf..db95612274 100644
--- a/src/yb/master/catalog_entity_info.h
+++ b/src/yb/master/catalog_entity_info.h
@@ -405,6 +405,10 @@ class TableInfo : public RefCountedThreadSafe<TableInfo>,
 
   bool colocated() const;
 
+  std::string matview_pg_table_id() const;
+  // True if the table is a materialized view.
+  bool is_matview() const;
+
   // Return the table's ID. Does not require synchronization.
   virtual const std::string& id() const override { return table_id_; }
 
@@ -551,6 +555,8 @@ class TableInfo : public RefCountedThreadSafe<TableInfo>,
   // where the tablets of the newly created table should reside.
   void SetTablespaceIdForTableCreation(const TablespaceId& tablespace_id);
 
+  void SetMatview();
+
  private:
   friend class RefCountedThreadSafe<TableInfo>;
   ~TableInfo();
diff --git a/src/yb/master/catalog_entity_info.proto b/src/yb/master/catalog_entity_info.proto
index 73cd4f10d2..287eda62e2 100644
--- a/src/yb/master/catalog_entity_info.proto
+++ b/src/yb/master/catalog_entity_info.proto
@@ -155,6 +155,10 @@ message SysTablesEntryPB {
   // True if the table is colocated (including tablegroups, excluding YSQL system tables).
   optional bool colocated = 25 [ default = false ];
 
+  // For materialized views:
+  optional bytes matview_pg_table_id = 36; // The PG OID of a materialized view
+  optional bool is_matview = 37 [ default = false ]; // True if the table is a materialized view.
+
   optional uint32 fully_applied_schema_version = 23;
   repeated IndexInfoPB fully_applied_indexes = 26;
   optional IndexInfoPB fully_applied_index_info = 27;
diff --git a/src/yb/master/catalog_manager.cc b/src/yb/master/catalog_manager.cc
index 6ee78ca8e7..10bc8474fe 100644
--- a/src/yb/master/catalog_manager.cc
+++ b/src/yb/master/catalog_manager.cc
@@ -734,7 +734,6 @@ void CatalogManager::NamespaceNameMapper::clear() {
 
 CatalogManager::CatalogManager(Master* master)
     : master_(master),
-      rng_(GetRandomSeed32()),
       tablet_exists_(false),
       state_(kConstructed),
       leader_ready_term_(-1),
@@ -2332,14 +2331,13 @@ CHECKED_STATUS CatalogManager::TEST_SplitTablet(
     const TabletId& tablet_id, const std::string& split_encoded_key,
     const std::string& split_partition_key) {
   auto source_tablet_info = VERIFY_RESULT(GetTabletInfo(tablet_id));
-  return DoSplitTablet(source_tablet_info, split_encoded_key, split_partition_key,
-      true /* is_manual_split */);
+  return DoSplitTablet(
+      source_tablet_info, split_encoded_key, split_partition_key, ManualSplit::kTrue);
 }
 
 Status CatalogManager::TEST_SplitTablet(
     const scoped_refptr<TabletInfo>& source_tablet_info, docdb::DocKeyHash split_hash_code) {
-  return DoSplitTablet(source_tablet_info, split_hash_code,
-      true /* is_manual_split */);
+  return DoSplitTablet(source_tablet_info, split_hash_code, ManualSplit::kTrue);
 }
 
 Status CatalogManager::TEST_IncrementTablePartitionListVersion(const TableId& table_id) {
@@ -2384,12 +2382,7 @@ bool CatalogManager::ShouldSplitValidCandidate(
   if (size < FLAGS_tablet_split_low_phase_size_threshold_bytes) {
     return false;
   }
-  TSDescriptorVector ts_descs;
-  {
-    auto blacklist_result = BlacklistSetFromPB();
-    master_->ts_manager()->GetAllLiveDescriptors(
-        &ts_descs, blacklist_result.ok() ? *blacklist_result : BlacklistSet());
-  }
+  TSDescriptorVector ts_descs = GetAllLiveNotBlacklistedTServers();
 
   size_t num_servers = 0;
   auto table_replication_info_or_status = GetTableReplicationInfo(tablet_info);
@@ -2424,7 +2417,7 @@ bool CatalogManager::ShouldSplitValidCandidate(
 
 Status CatalogManager::DoSplitTablet(
     const scoped_refptr<TabletInfo>& source_tablet_info, std::string split_encoded_key,
-    std::string split_partition_key, bool is_manual_split) {
+    std::string split_partition_key, const ManualSplit is_manual_split) {
   auto source_table_lock = source_tablet_info->table()->LockForWrite();
   auto source_tablet_lock = source_tablet_info->LockForWrite();
 
@@ -2435,10 +2428,7 @@ Status CatalogManager::DoSplitTablet(
   //
   // If this is a manual split, then we should select all potential tablets for the split
   // (i.e. ignore the disabled tablets list and ignore TTL validation).
-  RETURN_NOT_OK(tablet_split_manager_.ValidateSplitCandidateTable(
-      *source_tablet_info->table(), is_manual_split /* ignore_disabled_list */));
-  RETURN_NOT_OK(tablet_split_manager_.ValidateSplitCandidateTablet(
-      *source_tablet_info, is_manual_split /* ignore_ttl_validation */));
+  RETURN_NOT_OK(ValidateSplitCandidate(source_tablet_info, is_manual_split));
 
   auto drive_info = VERIFY_RESULT(source_tablet_info->GetLeaderReplicaDriveInfo());
   if (!is_manual_split &&
@@ -2506,8 +2496,8 @@ Status CatalogManager::DoSplitTablet(
 }
 
 Status CatalogManager::DoSplitTablet(
-    const scoped_refptr<TabletInfo>& source_tablet_info, docdb::DocKeyHash split_hash_code,
-    bool is_manual_split) {
+    const scoped_refptr<TabletInfo>& source_tablet_info, const docdb::DocKeyHash split_hash_code,
+    const ManualSplit is_manual_split) {
   docdb::KeyBytes split_encoded_key;
   docdb::DocKeyEncoderAfterTableIdStep(&split_encoded_key)
       .Hash(split_hash_code, std::vector<docdb::PrimitiveValue>());
@@ -2530,7 +2520,7 @@ Result<scoped_refptr<TabletInfo>> CatalogManager::GetTabletInfo(const TabletId&
 
 void CatalogManager::SplitTabletWithKey(
     const scoped_refptr<TabletInfo>& tablet, const std::string& split_encoded_key,
-    const std::string& split_partition_key, const bool is_manual_split) {
+    const std::string& split_partition_key, const ManualSplit is_manual_split) {
   // Note that DoSplitTablet() will trigger an async SplitTablet task, and will only return not OK()
   // if it failed to submit that task. In other words, any failures here are not retriable, and
   // success indicates that an async and automatically retrying task was submitted.
@@ -2540,11 +2530,15 @@ void CatalogManager::SplitTabletWithKey(
                         tablet->tablet_id()));
 }
 
-Status CatalogManager::SplitTablet(const TabletId& tablet_id, bool is_manual_split) {
-  LOG(INFO) << "Got tablet to split: " << tablet_id;
+Status CatalogManager::SplitTablet(const TabletId& tablet_id, const ManualSplit is_manual_split) {
+  LOG(INFO) << "Got tablet to split: " << tablet_id << ", is manual split: " << is_manual_split;
 
   const auto tablet = VERIFY_RESULT(GetTabletInfo(tablet_id));
+  return SplitTablet(tablet, is_manual_split);
+}
 
+Status CatalogManager::SplitTablet(
+    const scoped_refptr<TabletInfo>& tablet, const ManualSplit is_manual_split) {
   VLOG(2) << "Scheduling GetSplitKey request to leader tserver for source tablet ID: "
           << tablet->tablet_id();
   auto call = std::make_shared<AsyncGetTabletSplitKey>(
@@ -2573,8 +2567,22 @@ Status CatalogManager::SplitTablet(const TabletId& tablet_id, bool is_manual_spl
 
 Status CatalogManager::SplitTablet(
     const SplitTabletRequestPB* req, SplitTabletResponsePB* resp, rpc::RpcContext* rpc) {
-  const auto source_tablet_id = req->tablet_id();
-  return SplitTablet(source_tablet_id, true /* is_manual_split */);
+  const auto is_manual_split = ManualSplit::kTrue;
+  const auto tablet = VERIFY_RESULT(GetTabletInfo(req->tablet_id()));
+
+  RETURN_NOT_OK(ValidateSplitCandidate(tablet, is_manual_split));
+  return SplitTablet(tablet, is_manual_split);
+}
+
+Status CatalogManager::ValidateSplitCandidate(
+    const scoped_refptr<TabletInfo>& tablet, const ManualSplit is_manual_split) {
+  const IgnoreDisabledList ignore_disabled_list { is_manual_split.get() };
+  RETURN_NOT_OK(tablet_split_manager_.ValidateSplitCandidateTable(
+      *tablet->table(), ignore_disabled_list));
+
+  const IgnoreTtlValidation ignore_ttl_validation { is_manual_split.get() };
+  return tablet_split_manager_.ValidateSplitCandidateTablet(
+      *tablet, ignore_ttl_validation, ignore_disabled_list);
 }
 
 Status CatalogManager::DeleteNotServingTablet(
@@ -2937,6 +2945,14 @@ size_t CatalogManager::GetNumLiveTServersForPlacement(const PlacementId& placeme
   return ts_descs.size();
 }
 
+TSDescriptorVector CatalogManager::GetAllLiveNotBlacklistedTServers() const {
+  TSDescriptorVector ts_descs;
+  auto blacklist = BlacklistSetFromPB();
+  master_->ts_manager()->GetAllLiveDescriptors(
+      &ts_descs, blacklist.ok() ? *blacklist : BlacklistSet());
+  return ts_descs;
+}
+
 namespace {
 
 int GetNumReplicasFromPlacementInfo(const PlacementInfoPB& placement_info) {
@@ -3196,10 +3212,11 @@ Status CatalogManager::CreateTable(const CreateTableRequestPB* orig_req,
   master_->ts_manager()->GetAllLiveDescriptors(&all_ts_descs);
   RETURN_NOT_OK(CheckNumReplicas(placement_info, all_ts_descs, partitions, resp));
 
-  ValidateReplicationInfoResponsePB validate_resp;
-  s = CheckValidPlacementInfo(placement_info, all_ts_descs, &validate_resp);
-  if (!s.ok()) {
-    return SetupError(resp->mutable_error(), validate_resp.mutable_error()->code(), s);
+  if (!FLAGS_TEST_skip_placement_validation_createtable_api) {
+    ValidateReplicationInfoRequestPB validate_req;
+    validate_req.mutable_replication_info()->CopyFrom(replication_info);
+    ValidateReplicationInfoResponsePB validate_resp;
+    RETURN_NOT_OK(ValidateReplicationInfo(&validate_req, &validate_resp));
   }
 
   LOG(INFO) << "Set number of tablets: " << num_tablets;
@@ -3540,15 +3557,18 @@ Status CatalogManager::VerifyTablePgLayer(scoped_refptr<TableInfo> table, bool r
   // Upon Transaction completion, check pg system table using OID to ensure SUCCESS.
   const uint32_t database_oid = VERIFY_RESULT(GetPgsqlDatabaseOidByTableId(table->id()));
   const auto pg_table_id = GetPgsqlTableId(database_oid, kPgClassTableOid);
-  auto table_storage_id = GetPgsqlTableOid(table->id());
-  {
-    SharedLock lock(mutex_);
-    if (matview_pg_table_ids_map_.find(table->id()) != matview_pg_table_ids_map_.end()) {
-      table_storage_id = GetPgsqlTableOid(matview_pg_table_ids_map_[table->id()]);
-    }
+
+  auto entry_id = table->id();
+  auto relfilenode_id = TableId();
+
+  if (!table->matview_pg_table_id().empty()) {
+    relfilenode_id = entry_id;
+    entry_id = table->matview_pg_table_id();
   }
+
   auto entry_exists = VERIFY_RESULT(
-      ysql_transaction_->PgEntryExists(pg_table_id, table_storage_id));
+      ysql_transaction_->PgEntryExists(pg_table_id, GetPgsqlTableOid(entry_id),
+                                       relfilenode_id));
   auto l = table->LockForWrite();
   auto& metadata = table->mutable_metadata()->mutable_dirty()->pb;
 
@@ -3612,19 +3632,19 @@ Result<TabletInfos> CatalogManager::CreateTabletsFromTable(const vector<Partitio
 Status CatalogManager::CheckValidPlacementInfo(const PlacementInfoPB& placement_info,
                                                const TSDescriptorVector& ts_descs,
                                                ValidateReplicationInfoResponsePB* resp) {
-  // Verify that the total number of tablets is reasonable, relative to the number
-  // of live tablet servers.
-  int num_live_tservers = ts_descs.size();
-  int num_replicas = GetNumReplicasFromPlacementInfo(placement_info);
+  size_t num_live_tservers = ts_descs.size();
+  size_t num_replicas = GetNumReplicasFromPlacementInfo(placement_info);
   Status s;
   string msg;
 
-  // Verify that the number of replicas isn't larger than the number of live tablet
-  // servers.
+  // Verify that the number of replicas isn't larger than the required number of live tservers.
+  // To ensure quorum, we need n/2 + 1 live tservers.
+  size_t replica_quorum_needed = num_replicas / 2 + 1;
   if (FLAGS_catalog_manager_check_ts_count_for_create_table &&
-      num_replicas > num_live_tservers) {
+      replica_quorum_needed > num_live_tservers) {
     msg = Substitute("Not enough live tablet servers to create table with replication factor $0. "
-                     "$1 tablet servers are alive.", num_replicas, num_live_tservers);
+                     "Need at least $1 tablet servers whereas $2 are alive.",
+                     num_replicas, replica_quorum_needed, num_live_tservers);
     LOG(WARNING) << msg
                  << ". Placement info: " << placement_info.ShortDebugString()
                  << ", replication factor flag: " << FLAGS_replication_factor;
@@ -3632,7 +3652,7 @@ Status CatalogManager::CheckValidPlacementInfo(const PlacementInfoPB& placement_
     return SetupError(resp->mutable_error(), MasterErrorPB::REPLICATION_FACTOR_TOO_HIGH, s);
   }
 
-  // Verify that placement requests are reasonable and we can satisfy the minimums.
+  // Verify that placement requests are reasonable.
   if (!placement_info.placement_blocks().empty()) {
     int minimum_sum = 0;
     for (const auto& pb : placement_info.placement_blocks()) {
@@ -3644,7 +3664,8 @@ Status CatalogManager::CheckValidPlacementInfo(const PlacementInfoPB& placement_
         return SetupError(resp->mutable_error(), MasterErrorPB::INVALID_SCHEMA, s);
       }
     }
-
+    // Total replicas requested should be at least the sum of minimums
+    // requested in individual placement blocks.
     if (minimum_sum > num_replicas) {
       msg = Substitute("Sum of minimum replicas per placement ($0) is greater than num_replicas "
                        " ($1)", minimum_sum, num_replicas);
@@ -3653,16 +3674,59 @@ Status CatalogManager::CheckValidPlacementInfo(const PlacementInfoPB& placement_
       return SetupError(resp->mutable_error(), MasterErrorPB::INVALID_SCHEMA, s);
     }
 
-    if (!FLAGS_TEST_skip_placement_validation_createtable_api) {
-      // Loop through placements and verify that there are sufficient TServers to satisfy the
-      // minimum required replicas.
-      for (const auto& pb : placement_info.placement_blocks()) {
-        RETURN_NOT_OK(FindTServersForPlacementBlock(pb, ts_descs));
+    // Verify that there are enough TServers in the requested placements
+    // to match the total required replication factor.
+    auto allowed_ts = VERIFY_RESULT(FindTServersForPlacementInfo(placement_info, ts_descs));
+
+    // Fail if we don't have enough tablet servers in the areas requested.
+    // We need n/2 + 1 for quorum.
+    if (allowed_ts.size() < replica_quorum_needed) {
+      msg = Substitute("Not enough tablet servers in the requested placements. "
+                        "Need at least $0, have $1",
+                        replica_quorum_needed, allowed_ts.size());
+      s = STATUS(InvalidArgument, msg);
+      LOG(WARNING) << msg;
+      return SetupError(resp->mutable_error(), MasterErrorPB::REPLICATION_FACTOR_TOO_HIGH, s);
+    }
+
+    // Try allocating tservers for the replicas and see if we can place a quorum
+    // number of replicas.
+    // Essentially, the logic is:
+    // 1. We satisfy whatever we can from the minimums.
+    // 2. We then satisfy whatever we can from the slack.
+    //    Here it doesn't whether where we put the slack replicas as long as
+    //    the tservers are chosen from any of the valid placement blocks.
+    // Overall, if in this process we are able to place n/2 + 1 replicas
+    // then we succeed otherwise we fail.
+    size_t total_extra_replicas = num_replicas - minimum_sum;
+    size_t total_feasible_replicas = 0;
+    size_t total_extra_servers = 0;
+    for (const auto& pb : placement_info.placement_blocks()) {
+      auto allowed_ts = VERIFY_RESULT(FindTServersForPlacementBlock(pb, ts_descs));
+      size_t allowed_ts_size = allowed_ts.size();
+      size_t min_num_replicas = pb.min_num_replicas();
+      // For every placement block, we can only satisfy upto the number of
+      // tservers present in that particular placement block.
+      total_feasible_replicas += min(allowed_ts_size, min_num_replicas);
+      // Extra tablet servers beyond min_num_replicas will be used to place
+      // the extra replicas over and above the minimums.
+      if (allowed_ts_size > min_num_replicas) {
+        total_extra_servers += allowed_ts_size - min_num_replicas;
       }
+    }
+    // The total number of extra replicas that we can put cannot be more than
+    // the total tablet servers that are extra.
+    total_feasible_replicas += min(total_extra_replicas, total_extra_servers);
 
-      // Verify that there are enough TServers to match the total required replication factor (which
-      // could be more than the sum of the minimums).
-      RETURN_NOT_OK(FindTServersForPlacementInfo(placement_info, ts_descs));
+    // If we place the replicas in accordance with above, we should be able to place
+    // at least replica_quorum_needed otherwise we fail.
+    if (total_feasible_replicas < replica_quorum_needed) {
+      msg = Substitute("Not enough tablet servers in the requested placements. "
+                        "Can only find $0 tablet servers for the replicas but need at least "
+                        "$1.", total_feasible_replicas, replica_quorum_needed);
+      s = STATUS(InvalidArgument, msg);
+      LOG(WARNING) << msg;
+      return SetupError(resp->mutable_error(), MasterErrorPB::REPLICATION_FACTOR_TOO_HIGH, s);
     }
   }
 
@@ -4210,6 +4274,13 @@ scoped_refptr<TableInfo> CatalogManager::CreateTableInfo(const CreateTableReques
     metadata->set_is_pg_shared_table(true);
   }
 
+  if (req.is_matview()) {
+    metadata->set_is_matview(true);
+    if (req.has_matview_pg_table_id()) {
+      metadata->set_matview_pg_table_id(req.matview_pg_table_id());
+    }
+  }
+
   return table;
 }
 
@@ -4380,11 +4451,11 @@ Result<string> CatalogManager::GetPgSchemaName(const TableInfoPtr& table_info) {
 
   const uint32_t database_oid = VERIFY_RESULT(GetPgsqlDatabaseOid(table_info->namespace_id()));
   uint32_t table_oid = VERIFY_RESULT(GetPgsqlTableOid(table_info->id()));
-  {
-    if (matview_pg_table_ids_map_.find(table_info->id()) != matview_pg_table_ids_map_.end()) {
-      table_oid = VERIFY_RESULT(GetPgsqlTableOid(matview_pg_table_ids_map_[table_info->id()]));
-    }
+
+  if (!table_info->matview_pg_table_id().empty()) {
+      table_oid = VERIFY_RESULT(GetPgsqlTableOid(table_info->matview_pg_table_id()));
   }
+
   const uint32_t relnamespace_oid = VERIFY_RESULT(
       sys_catalog_->ReadPgClassRelnamespace(database_oid, table_oid));
   return sys_catalog_->ReadPgNamespaceNspname(database_oid, relnamespace_oid);
@@ -4425,7 +4496,9 @@ Status CatalogManager::TruncateTable(const TableId& table_id,
 
   // Truncate on a colocated table should not hit master because it should be handled by a write
   // DML that creates a table-level tombstone.
-  LOG_IF(WARNING, table->IsColocatedUserTable()) << "cannot truncate a colocated table on master";
+  RSTATUS_DCHECK(!table->IsColocatedUserTable(),
+                 InternalError,
+                 Format("Cannot truncate colocated table $0 on master", table->name()));
 
   if (!FLAGS_enable_delete_truncate_xcluster_replicated_table && IsCdcEnabled(*table)) {
     return STATUS(NotSupported,
@@ -4441,13 +4514,15 @@ Status CatalogManager::TruncateTable(const TableId& table_id,
             << RequestorString(rpc);
   background_tasks_->Wake();
 
-  // Truncate indexes also.
-  // Note: PG table does not have references to indexes in the base table, so associated indexes
-  //       must be truncated from the PG code separately.
-  const bool is_index = IsIndex(l->pb);
-  DCHECK(!is_index || l->pb.indexes().empty()) << "indexes should be empty for index table";
-  for (const auto& index_info : l->pb.indexes()) {
-    RETURN_NOT_OK(TruncateTable(index_info.table_id(), resp, rpc));
+  // Truncate indexes also.  For YSQL, truncate for each index is sent separately in
+  // YBCTruncateTable, so don't handle it here.  Also, it would be incorrect to handle it here in
+  // case the index is part of a tablegroup.
+  if (table->GetTableType() != PGSQL_TABLE_TYPE) {
+    const bool is_index = IsIndex(l->pb);
+    DCHECK(!is_index || l->pb.indexes().empty()) << "indexes should be empty for index table";
+    for (const auto& index_info : l->pb.indexes()) {
+      RETURN_NOT_OK(TruncateTable(index_info.table_id(), resp, rpc));
+    }
   }
 
   return Status::OK();
@@ -5734,6 +5809,7 @@ Status CatalogManager::ListTables(const ListTablesRequestPB* req,
   bool has_rel_filter = req->relation_type_filter_size() > 0;
   bool include_user_table = has_rel_filter ? false : true;
   bool include_user_index = has_rel_filter ? false : true;
+  bool include_user_matview = has_rel_filter ? false : true;
   bool include_system_table = req->exclude_system_tables() ? false
       : (has_rel_filter ? false : true);
 
@@ -5744,6 +5820,8 @@ Status CatalogManager::ListTables(const ListTablesRequestPB* req,
       include_user_table = true;
     } else if (relation == INDEX_TABLE_RELATION) {
       include_user_index = true;
+    } else if (relation == MATVIEW_TABLE_RELATION) {
+      include_user_matview = true;
     }
   }
 
@@ -5774,6 +5852,11 @@ Status CatalogManager::ListTables(const ListTablesRequestPB* req,
         continue;
       }
       relation_type = INDEX_TABLE_RELATION;
+    } else if (IsMatviewTable(table_info)) {
+      if (!include_user_matview) {
+        continue;
+      }
+      relation_type = MATVIEW_TABLE_RELATION;
     } else if (IsUserTableUnlocked(table_info)) {
       if (!include_user_table) {
         continue;
@@ -5985,6 +6068,10 @@ bool CatalogManager::IsSequencesSystemTable(const TableInfo& table) const {
   return false;
 }
 
+bool CatalogManager::IsMatviewTable(const TableInfo& table) const {
+  return table.GetTableType() == PGSQL_TABLE_TYPE && table.is_matview();
+}
+
 void CatalogManager::NotifyTabletDeleteFinished(const TabletServerId& tserver_uuid,
                                                 const TabletId& tablet_id,
                                                 const TableInfoPtr& table) {
@@ -7075,7 +7162,7 @@ Status CatalogManager::VerifyNamespacePgLayer(
   // Upon Transaction completion, check pg system table using OID to ensure SUCCESS.
   const auto pg_table_id = GetPgsqlTableId(atoi(kSystemNamespaceId), kPgDatabaseTableOid);
   auto entry_exists = VERIFY_RESULT(
-      ysql_transaction_->PgEntryExists(pg_table_id, GetPgsqlDatabaseOid(ns->id())));
+      ysql_transaction_->PgEntryExists(pg_table_id, GetPgsqlDatabaseOid(ns->id()), TableId()));
   auto l = ns->LockForWrite();
   SysNamespaceEntryPB& metadata = ns->mutable_metadata()->mutable_dirty()->pb;
 
@@ -8997,9 +9084,17 @@ Status CatalogManager::HandleTabletSchemaVersionReport(
   return MultiStageAlterTable::LaunchNextTableInfoVersionIfNecessary(this, table, version);
 }
 
-Status CatalogManager::ProcessPendingAssignments(const TabletInfos& tablets) {
+Status CatalogManager::ProcessPendingAssignmentsPerTable(
+    const TableId& table_id, const TabletInfos& tablets, CMGlobalLoadState* global_load_state) {
   VLOG(1) << "Processing pending assignments";
 
+  TSDescriptorVector ts_descs = GetAllLiveNotBlacklistedTServers();
+
+  // Initialize this table load state.
+  CMPerTableLoadState table_load_state(global_load_state);
+  InitializeTableLoadState(table_id, ts_descs, &table_load_state);
+  table_load_state.SortLoad();
+
   // Take write locks on all tablets to be processed, and ensure that they are
   // unlocked at the end of this scope.
   for (const scoped_refptr<TabletInfo>& tablet : tablets) {
@@ -9044,18 +9139,14 @@ Status CatalogManager::ProcessPendingAssignments(const TabletInfos& tablets) {
   }
 
   // For those tablets which need to be created in this round, assign replicas.
-  TSDescriptorVector ts_descs;
-  {
-    BlacklistSet blacklist = VERIFY_RESULT(BlacklistSetFromPB());
-    master_->ts_manager()->GetAllLiveDescriptors(&ts_descs, blacklist);
-  }
   Status s;
   std::unordered_set<TableInfo*> ok_status_tables;
   for (TabletInfo *tablet : deferred.needs_create_rpc) {
     // NOTE: if we fail to select replicas on the first pass (due to
     // insufficient Tablet Servers being online), we will still try
     // again unless the tablet/table creation is cancelled.
-    s = SelectReplicasForTablet(ts_descs, tablet);
+    LOG(INFO) << "Selecting replicas for tablet " << tablet->id();
+    s = SelectReplicasForTablet(ts_descs, tablet, &table_load_state, global_load_state);
     if (!s.ok()) {
       s = s.CloneAndPrepend(Substitute(
           "An error occurred while selecting replicas for tablet $0: $1",
@@ -9134,8 +9225,9 @@ Status CatalogManager::ProcessPendingAssignments(const TabletInfos& tablets) {
   return SendCreateTabletRequests(deferred.needs_create_rpc);
 }
 
-Status CatalogManager::SelectReplicasForTablet(const TSDescriptorVector& ts_descs,
-                                               TabletInfo* tablet) {
+Status CatalogManager::SelectReplicasForTablet(
+    const TSDescriptorVector& ts_descs, TabletInfo* tablet,
+    CMPerTableLoadState* per_table_state, CMGlobalLoadState* global_state) {
   auto table_guard = tablet->table()->LockForRead();
 
   if (!table_guard->pb.IsInitialized()) {
@@ -9156,7 +9248,8 @@ Status CatalogManager::SelectReplicasForTablet(const TSDescriptorVector& ts_desc
   consensus::RaftConfigPB *config = cstate->mutable_config();
   config->set_opid_index(consensus::kInvalidOpIdIndex);
 
-  Status s = HandlePlacementUsingReplicationInfo(replication_info, ts_descs, config);
+  Status s = HandlePlacementUsingReplicationInfo(
+      replication_info, ts_descs, config, per_table_state, global_state);
   if (!s.ok()) {
     return s;
   }
@@ -9176,31 +9269,67 @@ Status CatalogManager::SelectReplicasForTablet(const TSDescriptorVector& ts_desc
   return Status::OK();
 }
 
+void CatalogManager::GetTsDescsFromPlacementInfo(const PlacementInfoPB& placement_info,
+                                                 const TSDescriptorVector& all_ts_descs,
+                                                 TSDescriptorVector* ts_descs) {
+  ts_descs->clear();
+  for (const auto& ts_desc : all_ts_descs) {
+    if (placement_info.has_placement_uuid()) {
+      string placement_uuid = placement_info.placement_uuid();
+      if (ts_desc->placement_uuid() == placement_uuid) {
+        ts_descs->push_back(ts_desc);
+      }
+    } else if (ts_desc->placement_uuid() == "") {
+      // Since the placement info has no placement id, we know it is live, so we add this ts.
+      ts_descs->push_back(ts_desc);
+    }
+  }
+}
+
 Status CatalogManager::HandlePlacementUsingReplicationInfo(
     const ReplicationInfoPB& replication_info,
     const TSDescriptorVector& all_ts_descs,
-    consensus::RaftConfigPB* config) {
-  return HandlePlacementUsingPlacementInfo(replication_info.live_replicas(),
-                                           all_ts_descs, PeerMemberType::VOTER, config);
+    consensus::RaftConfigPB* config,
+    CMPerTableLoadState* per_table_state,
+    CMGlobalLoadState* global_state) {
+  // Validate if we have enough tservers to put the replicas.
+  ValidateReplicationInfoRequestPB req;
+  req.mutable_replication_info()->CopyFrom(replication_info);
+  ValidateReplicationInfoResponsePB resp;
+  RETURN_NOT_OK(ValidateReplicationInfo(&req, &resp));
+
+  TSDescriptorVector ts_descs;
+  GetTsDescsFromPlacementInfo(replication_info.live_replicas(), all_ts_descs, &ts_descs);
+  RETURN_NOT_OK(HandlePlacementUsingPlacementInfo(
+      replication_info.live_replicas(), ts_descs, PeerMemberType::VOTER,
+      config, per_table_state, global_state));
+  for (int i = 0; i < replication_info.read_replicas_size(); i++) {
+    GetTsDescsFromPlacementInfo(replication_info.read_replicas(i), all_ts_descs, &ts_descs);
+    RETURN_NOT_OK(HandlePlacementUsingPlacementInfo(
+        replication_info.read_replicas(i), ts_descs, PeerMemberType::OBSERVER,
+        config, per_table_state, global_state));
+  }
+  return Status::OK();
 }
 
 Status CatalogManager::HandlePlacementUsingPlacementInfo(const PlacementInfoPB& placement_info,
                                                          const TSDescriptorVector& ts_descs,
                                                          PeerMemberType member_type,
-                                                         consensus::RaftConfigPB* config) {
-  int nreplicas = GetNumReplicasFromPlacementInfo(placement_info);
-  if (ts_descs.size() < nreplicas) {
-    return STATUS_SUBSTITUTE(InvalidArgument,
-        "Not enough tablet servers in the requested placements. Need at least $0, have $1",
-        nreplicas, ts_descs.size());
-  }
+                                                         consensus::RaftConfigPB* config,
+                                                         CMPerTableLoadState* per_table_state,
+                                                         CMGlobalLoadState* global_state) {
+  size_t nreplicas = GetNumReplicasFromPlacementInfo(placement_info);
+  size_t ntservers = ts_descs.size();
   // Keep track of servers we've already selected, so that we don't attempt to
   // put two replicas on the same host.
-  set<shared_ptr<TSDescriptor>> already_selected_ts;
+  set<TabletServerId> already_selected_ts;
   if (placement_info.placement_blocks().empty()) {
     // If we don't have placement info, just place the replicas as before, distributed across the
     // whole cluster.
-    SelectReplicas(ts_descs, nreplicas, config, &already_selected_ts, member_type);
+    // We cannot put more than ntservers replicas.
+    nreplicas = min(nreplicas, ntservers);
+    SelectReplicas(ts_descs, nreplicas, config, &already_selected_ts, member_type,
+                   per_table_state, global_state);
   } else {
     // TODO(bogdan): move to separate function
     //
@@ -9211,19 +9340,30 @@ Status CatalogManager::HandlePlacementUsingPlacementInfo(const PlacementInfoPB&
     auto all_allowed_ts = VERIFY_RESULT(FindTServersForPlacementInfo(placement_info, ts_descs));
 
     // Loop through placements and assign to respective available TSs.
+    size_t min_replica_count_sum = 0;
     for (const auto& pb : placement_info.placement_blocks()) {
+      // This works because currently we don't allow placement blocks to overlap.
       auto available_ts_descs = VERIFY_RESULT(FindTServersForPlacementBlock(pb, ts_descs));
-      int num_replicas = pb.min_num_replicas();
-      SelectReplicas(available_ts_descs, num_replicas, config, &already_selected_ts, member_type);
-    }
-
-    int replicas_left = nreplicas - already_selected_ts.size();
+      size_t available_ts_descs_size = available_ts_descs.size();
+      size_t min_num_replicas = pb.min_num_replicas();
+      // We cannot put more than the available tablet servers in that placement block.
+      size_t num_replicas = min(min_num_replicas, available_ts_descs_size);
+      min_replica_count_sum += min_num_replicas;
+      SelectReplicas(available_ts_descs, num_replicas, config, &already_selected_ts, member_type,
+                     per_table_state, global_state);
+    }
+
+    size_t replicas_left = nreplicas - min_replica_count_sum;
+    size_t max_tservers_left = all_allowed_ts.size() - already_selected_ts.size();
+    // Upper bounded by the tservers left.
+    replicas_left = min(replicas_left, max_tservers_left);
     DCHECK_GE(replicas_left, 0);
     if (replicas_left > 0) {
       // No need to do an extra check here, as we checked early if we have enough to cover all
       // requested placements and checked individually per placement info, if we could cover the
       // minimums.
-      SelectReplicas(all_allowed_ts, replicas_left, config, &already_selected_ts, member_type);
+      SelectReplicas(all_allowed_ts, replicas_left, config, &already_selected_ts, member_type,
+                     per_table_state, global_state);
     }
   }
   return Status::OK();
@@ -9243,14 +9383,6 @@ Result<vector<shared_ptr<TSDescriptor>>> CatalogManager::FindTServersForPlacemen
     }
   }
 
-  // Fail if we don't have enough tablet servers in the areas requested.
-  const int nreplicas = placement_info.num_replicas();
-  if (all_allowed_ts.size() < nreplicas) {
-    return STATUS_SUBSTITUTE(InvalidArgument,
-        "Not enough tablet servers in the requested placements. Need at least $0, have $1",
-        nreplicas, all_allowed_ts.size());
-  }
-
   return all_allowed_ts;
 }
 
@@ -9266,15 +9398,7 @@ Result<vector<shared_ptr<TSDescriptor>>> CatalogManager::FindTServersForPlacemen
     }
   }
 
-  // Verify that there are sufficient TServers to satisfy min_num_replicas.
-  int num_replicas = placement_block.min_num_replicas();
-  if (allowed_ts.size() < num_replicas) {
-    return STATUS_SUBSTITUTE(InvalidArgument,
-          "Not enough tablet servers in $0. Need at least $1 but only have $2.",
-          TSDescriptor::generate_placement_id(cloud_info), num_replicas, allowed_ts.size());
-  }
-
-return allowed_ts;
+  return allowed_ts;
 }
 
 Status CatalogManager::SendCreateTabletRequests(const vector<TabletInfo*>& tablets) {
@@ -9336,12 +9460,7 @@ void CatalogManager::StartElectionIfReady(
   }
 
   // Find tservers that can be leaders for a tablet.
-  TSDescriptorVector ts_descs;
-  {
-    auto blacklist_result = BlacklistSetFromPB();
-    master_->ts_manager()->GetAllLiveDescriptors(
-        &ts_descs, blacklist_result.ok() ? *blacklist_result : BlacklistSet());
-  }
+  TSDescriptorVector ts_descs = GetAllLiveNotBlacklistedTServers();
 
   std::vector<std::string> possible_leaders;
   for (const auto& replica : *replicas) {
@@ -9392,99 +9511,44 @@ void CatalogManager::StartElectionIfReady(
   WARN_NOT_OK(task->Run(), "Failed to send new tablet start election request");
 }
 
-shared_ptr<TSDescriptor> CatalogManager::PickBetterReplicaLocation(
-    const TSDescriptorVector& two_choices) {
-  DCHECK_EQ(two_choices.size(), 2);
-
-  const auto& a = two_choices[0];
-  const auto& b = two_choices[1];
-
-  // When creating replicas, we consider two aspects of load:
-  //   (1) how many tablet replicas are already on the server, and
-  //   (2) how often we've chosen this server recently.
-  //
-  // The first factor will attempt to put more replicas on servers that
-  // are under-loaded (eg because they have newly joined an existing cluster, or have
-  // been reformatted and re-joined).
-  //
-  // The second factor will ensure that we take into account the recent selection
-  // decisions even if those replicas are still in the process of being created (and thus
-  // not yet reported by the server). This is important because, while creating a table,
-  // we batch the selection process before sending any creation commands to the
-  // servers themselves.
-  //
-  // TODO: in the future we may want to factor in other items such as available disk space,
-  // actual request load, etc.
-  double load_a = a->RecentReplicaCreations() + a->num_live_replicas();
-  double load_b = b->RecentReplicaCreations() + b->num_live_replicas();
-  if (load_a < load_b) {
-    return a;
-  } else if (load_b < load_a) {
-    return b;
-  } else {
-    // If the load is the same, we can just pick randomly.
-    return two_choices[rng_.Uniform(2)];
-  }
-}
-
 shared_ptr<TSDescriptor> CatalogManager::SelectReplica(
     const TSDescriptorVector& ts_descs,
-    const set<shared_ptr<TSDescriptor>>& excluded) {
-  // The replica selection algorithm follows the idea from
-  // "Power of Two Choices in Randomized Load Balancing"[1]. For each replica,
-  // we randomly select two tablet servers, and then assign the replica to the
-  // less-loaded one of the two. This has some nice properties:
-  //
-  // 1) because the initial selection of two servers is random, we get good
-  //    spreading of replicas across the cluster. In contrast if we sorted by
-  //    load and always picked under-loaded servers first, we'd end up causing
-  //    all tablets of a new table to be placed on an empty server. This wouldn't
-  //    give good load balancing of that table.
-  //
-  // 2) because we pick the less-loaded of two random choices, we do end up with a
-  //    weighting towards filling up the underloaded one over time, without
-  //    the extreme scenario above.
-  //
-  // 3) because we don't follow any sequential pattern, every server is equally
-  //    likely to replicate its tablets to every other server. In contrast, a
-  //    round-robin design would enforce that each server only replicates to its
-  //    adjacent nodes in the TS sort order, limiting recovery bandwidth (see
-  //    KUDU-1317).
-  //
-  // [1] http://www.eecs.harvard.edu/~michaelm/postscripts/mythesis.pdf
-
-  // Pick two random servers, excluding those we've already picked.
-  // If we've only got one server left, 'two_choices' will actually
-  // just contain one element.
-  vector<shared_ptr<TSDescriptor>> two_choices;
-  rng_.ReservoirSample(ts_descs, 2, excluded, &two_choices);
+    set<TabletServerId>* excluded,
+    CMPerTableLoadState* per_table_state, CMGlobalLoadState* global_state) {
+  shared_ptr<TSDescriptor> found_ts;
+  for (const auto& sorted_load : per_table_state->sorted_load_) {
+    // Don't consider a tserver that has already been considered for this tablet.
+    if (excluded->count(sorted_load)) {
+      continue;
+    }
+    // Only choose from the set of allowed tservers for this tablet.
+    auto it = std::find_if(ts_descs.begin(), ts_descs.end(), [&sorted_load](const auto& ts) {
+      return ts->permanent_uuid() == sorted_load;
+    });
 
-  if (two_choices.size() == 2) {
-    // Pick the better of the two.
-    return PickBetterReplicaLocation(two_choices);
+    if (it != ts_descs.end()) {
+      found_ts = *it;
+      break;
+    }
   }
 
-  // If we couldn't randomly sample two servers, it's because we only had one
-  // more non-excluded choice left.
-  CHECK_EQ(1, two_choices.size()) << "ts_descs: " << ts_descs.size()
-                                  << " already_sel: " << excluded.size();
-  return two_choices[0];
+  return found_ts;
 }
 
 void CatalogManager::SelectReplicas(
-    const TSDescriptorVector& ts_descs, int nreplicas, consensus::RaftConfigPB* config,
-    set<shared_ptr<TSDescriptor>>* already_selected_ts, PeerMemberType member_type) {
+    const TSDescriptorVector& ts_descs, size_t nreplicas, consensus::RaftConfigPB* config,
+    set<TabletServerId>* already_selected_ts, PeerMemberType member_type,
+    CMPerTableLoadState* per_table_state, CMGlobalLoadState* global_state) {
   DCHECK_LE(nreplicas, ts_descs.size());
 
-  for (int i = 0; i < nreplicas; ++i) {
-    // We have to derefence already_selected_ts here, as the inner mechanics uses ReservoirSample,
-    // which in turn accepts only a reference to the set, not a pointer. Alternatively, we could
-    // have passed it in as a non-const reference, but that goes against our argument passing
-    // convention.
-    //
-    // TODO(bogdan): see if we indeed want to switch back to non-const reference.
-    shared_ptr<TSDescriptor> ts = SelectReplica(ts_descs, *already_selected_ts);
-    InsertOrDie(already_selected_ts, ts);
+  for (size_t i = 0; i < nreplicas; ++i) {
+    shared_ptr<TSDescriptor> ts = SelectReplica(
+        ts_descs, already_selected_ts, per_table_state, global_state);
+    InsertOrDie(already_selected_ts, ts->permanent_uuid());
+    // Update the load state at global and table level.
+    per_table_state->per_ts_load_[ts->permanent_uuid()]++;
+    global_state->per_ts_load_[ts->permanent_uuid()]++;
+    per_table_state->SortLoad();
 
     // Increment the number of pending replicas so that we take this selection into
     // account when assigning replicas for other tablets of the same table. This
@@ -10007,7 +10071,16 @@ Status CatalogManager::SetClusterConfig(
 Status CatalogManager::ValidateReplicationInfo(
     const ValidateReplicationInfoRequestPB* req, ValidateReplicationInfoResponsePB* resp) {
   TSDescriptorVector all_ts_descs;
-  master_->ts_manager()->GetAllLiveDescriptors(&all_ts_descs);
+  {
+    auto blacklist = VERIFY_RESULT(BlacklistSetFromPB());
+    master_->ts_manager()->GetAllLiveDescriptors(&all_ts_descs, blacklist);
+  }
+  // We don't need any validation checks for read replica placements
+  // because they aren't a part of any raft quorum underneath.
+  // Technically, it is ok to have even 0 read replica nodes for them upfront.
+  // We only need it for the primary cluster replicas.
+  TSDescriptorVector ts_descs;
+  GetTsDescsFromPlacementInfo(req->replication_info().live_replicas(), all_ts_descs, &ts_descs);
   Status s = CheckValidPlacementInfo(req->replication_info().live_replicas(), all_ts_descs, resp);
   if (!s.ok()) {
     return SetupError(resp->mutable_error(), MasterErrorPB::INVALID_TABLE_REPLICATION_INFO, s);
@@ -10629,5 +10702,44 @@ std::shared_ptr<ClusterConfigInfo> CatalogManager::ClusterConfig() const {
   return cluster_config_;
 }
 
+void CatalogManager::InitializeTableLoadState(
+    const TableId& table_id, TSDescriptorVector ts_descs, CMPerTableLoadState* state) {
+  for (const auto& ts : ts_descs) {
+    // Touch every tserver with 0 load.
+    state->per_ts_load_[ts->permanent_uuid()];
+    // Insert into the sorted list.
+    state->sorted_load_.emplace_back(ts->permanent_uuid());
+  }
+
+  auto table_info = GetTableInfo(table_id);
+
+  if (!table_info) {
+    return;
+  }
+  CatalogManagerUtil::FillTableLoadState(table_info, state);
+}
+
+void CatalogManager::InitializeGlobalLoadState(
+    TSDescriptorVector ts_descs, CMGlobalLoadState* state) {
+  for (const auto& ts : ts_descs) {
+    // Touch every tserver with 0 load.
+    state->per_ts_load_[ts->permanent_uuid()];
+  }
+
+  SharedLock l(mutex_);
+  for (const auto& id_and_info : *table_ids_map_) {
+    // Ignore system, colocated and deleting/deleted tables.
+    {
+      auto l = id_and_info.second->LockForRead();
+      if (IsSystemTable(*(id_and_info.second)) ||
+          id_and_info.second->IsColocatedUserTable() ||
+          l->started_deleting()) {
+        continue;
+      }
+    }
+    CatalogManagerUtil::FillTableLoadState(id_and_info.second, state);
+  }
+}
+
 }  // namespace master
 }  // namespace yb
diff --git a/src/yb/master/catalog_manager.h b/src/yb/master/catalog_manager.h
index 73ecd4e8a2..f0ed7ba950 100644
--- a/src/yb/master/catalog_manager.h
+++ b/src/yb/master/catalog_manager.h
@@ -59,6 +59,7 @@
 #include "yb/master/catalog_entity_info.h"
 #include "yb/master/catalog_manager_if.h"
 #include "yb/master/cdc_consumer_split_driver.h"
+#include "yb/master/catalog_manager_util.h"
 #include "yb/master/master_dcl.fwd.h"
 #include "yb/master/master_encryption.fwd.h"
 #include "yb/master/master_defaults.h"
@@ -577,6 +578,9 @@ class CatalogManager :
   // Is the table id from a table created for colocated database?
   bool IsColocatedParentTableId(const TableId& table_id) const;
 
+  // Is the table a materialized view?
+  bool IsMatviewTable(const TableInfo& table) const;
+
   // Is the table created by user?
   // Note that table can be regular table or index in this case.
   bool IsUserCreatedTable(const TableInfo& table) const override;
@@ -625,16 +629,25 @@ class CatalogManager :
 
   // Loops through the table's placement infos and populates the corresponding config from
   // each placement.
-  virtual CHECKED_STATUS HandlePlacementUsingReplicationInfo(
+  CHECKED_STATUS HandlePlacementUsingReplicationInfo(
       const ReplicationInfoPB& replication_info,
       const TSDescriptorVector& all_ts_descs,
-      consensus::RaftConfigPB* config);
+      consensus::RaftConfigPB* config,
+      CMPerTableLoadState* per_table_state,
+      CMGlobalLoadState* global_state);
 
   // Handles the config creation for a given placement.
   CHECKED_STATUS HandlePlacementUsingPlacementInfo(const PlacementInfoPB& placement_info,
                                                    const TSDescriptorVector& ts_descs,
                                                    consensus::PeerMemberType member_type,
-                                                   consensus::RaftConfigPB* config);
+                                                   consensus::RaftConfigPB* config,
+                                                   CMPerTableLoadState* per_table_state,
+                                                   CMGlobalLoadState* global_state);
+
+  // Populates ts_descs with all tservers belonging to a certain placement.
+  void GetTsDescsFromPlacementInfo(const PlacementInfoPB& placement_info,
+                                   const TSDescriptorVector& all_ts_descs,
+                                   TSDescriptorVector* ts_descs);
 
     // Set the current committed config.
   CHECKED_STATUS GetCurrentConfig(consensus::ConsensusStatePB *cpb) const override;
@@ -794,43 +807,43 @@ class CatalogManager :
     return *universe_key_client_;
   }
 
-  CHECKED_STATUS FlushSysCatalog(const FlushSysCatalogRequestPB* req,
-                                 FlushSysCatalogResponsePB* resp,
-                                 rpc::RpcContext* rpc);
+  Status FlushSysCatalog(const FlushSysCatalogRequestPB* req,
+                         FlushSysCatalogResponsePB* resp,
+                         rpc::RpcContext* rpc);
 
-  CHECKED_STATUS CompactSysCatalog(const CompactSysCatalogRequestPB* req,
-                                   CompactSysCatalogResponsePB* resp,
-                                   rpc::RpcContext* rpc);
+  Status CompactSysCatalog(const CompactSysCatalogRequestPB* req,
+                           CompactSysCatalogResponsePB* resp,
+                           rpc::RpcContext* rpc);
 
-  CHECKED_STATUS SplitTablet(const TabletId& tablet_id, bool is_manual_split) override;
+  Status SplitTablet(const TabletId& tablet_id, ManualSplit is_manual_split) override;
 
   // Splits tablet specified in the request using middle of the partition as a split point.
-  CHECKED_STATUS SplitTablet(
+  Status SplitTablet(
       const SplitTabletRequestPB* req, SplitTabletResponsePB* resp, rpc::RpcContext* rpc);
 
   // Deletes a tablet that is no longer serving user requests. This would require that the tablet
   // has been split and both of its children are now in RUNNING state and serving user requests
   // instead.
-  CHECKED_STATUS DeleteNotServingTablet(
+  Status DeleteNotServingTablet(
       const DeleteNotServingTabletRequestPB* req, DeleteNotServingTabletResponsePB* resp,
       rpc::RpcContext* rpc);
 
-  CHECKED_STATUS DdlLog(
+  Status DdlLog(
       const DdlLogRequestPB* req, DdlLogResponsePB* resp, rpc::RpcContext* rpc);
 
   // Test wrapper around protected DoSplitTablet method.
-  CHECKED_STATUS TEST_SplitTablet(
+  Status TEST_SplitTablet(
       const scoped_refptr<TabletInfo>& source_tablet_info,
       docdb::DocKeyHash split_hash_code) override;
 
-  CHECKED_STATUS TEST_SplitTablet(
+  Status TEST_SplitTablet(
       const TabletId& tablet_id, const std::string& split_encoded_key,
       const std::string& split_partition_key) override;
 
-  CHECKED_STATUS TEST_IncrementTablePartitionListVersion(const TableId& table_id) override;
+  Status TEST_IncrementTablePartitionListVersion(const TableId& table_id) override;
 
   // Schedule a task to run on the async task thread pool.
-  CHECKED_STATUS ScheduleTask(std::shared_ptr<RetryingTSRpcTask> task) override;
+  Status ScheduleTask(std::shared_ptr<RetryingTSRpcTask> task) override;
 
   // Time since this peer became master leader. Caller should verify that it is leader before.
   MonoDelta TimeSinceElectedLeader();
@@ -1094,26 +1107,26 @@ class CatalogManager :
 
   // Task that takes care of the tablet assignments/creations.
   // Loops through the "not created" tablets and sends a CreateTablet() request.
-  CHECKED_STATUS ProcessPendingAssignments(const TabletInfos& tablets);
-
-  // Given 'two_choices', which should be a vector of exactly two elements, select which
-  // one is the better choice for a new replica.
-  std::shared_ptr<TSDescriptor> PickBetterReplicaLocation(const TSDescriptorVector& two_choices);
+  CHECKED_STATUS ProcessPendingAssignmentsPerTable(
+      const TableId& table_id, const TabletInfos& tablets, CMGlobalLoadState* global_load_state);
 
   // Select a tablet server from 'ts_descs' on which to place a new replica.
   // Any tablet servers in 'excluded' are not considered.
   // REQUIRES: 'ts_descs' must include at least one non-excluded server.
   std::shared_ptr<TSDescriptor> SelectReplica(
       const TSDescriptorVector& ts_descs,
-      const std::set<std::shared_ptr<TSDescriptor>>& excluded);
+      std::set<TabletServerId>* excluded,
+      CMPerTableLoadState* per_table_state, CMGlobalLoadState* global_state);
 
   // Select N Replicas from online tablet servers (as specified by
   // 'ts_descs') for the specified tablet and populate the consensus configuration
   // object. If 'ts_descs' does not specify enough online tablet
   // servers to select the N replicas, return Status::InvalidArgument.
   //
-  // This method is called by "ProcessPendingAssignments()".
-  CHECKED_STATUS SelectReplicasForTablet(const TSDescriptorVector& ts_descs, TabletInfo* tablet);
+  // This method is called by "ProcessPendingAssignmentsPerTable()".
+  CHECKED_STATUS SelectReplicasForTablet(
+      const TSDescriptorVector& ts_descs, TabletInfo* tablet,
+      CMPerTableLoadState* per_table_state, CMGlobalLoadState* global_state);
 
   // Select N Replicas from the online tablet servers that have been chosen to respect the
   // placement information provided. Populate the consensus configuration object with choices and
@@ -1123,9 +1136,11 @@ class CatalogManager :
   // This method is called by "SelectReplicasForTablet".
   void SelectReplicas(
       const TSDescriptorVector& ts_descs,
-      int nreplicas, consensus::RaftConfigPB* config,
-      std::set<std::shared_ptr<TSDescriptor>>* already_selected_ts,
-      consensus::PeerMemberType member_type);
+      size_t nreplicas, consensus::RaftConfigPB* config,
+      std::set<TabletServerId>* already_selected_ts,
+      consensus::PeerMemberType member_type,
+      CMPerTableLoadState* per_table_state,
+      CMGlobalLoadState* global_state);
 
   void HandleAssignPreparingTablet(TabletInfo* tablet,
                                    DeferredAssignmentActions* deferred);
@@ -1147,7 +1162,7 @@ class CatalogManager :
   // after the timeout, we regenerate a new one and proceed with a new
   // assignment/creation.
   //
-  // This method is part of the "ProcessPendingAssignments()"
+  // This method is part of the "ProcessPendingAssignmentsPerTable()"
   //
   // This must be called after persisting the tablet state as
   // CREATING to ensure coherent state after Master failover.
@@ -1309,12 +1324,12 @@ class CatalogManager :
 
   CHECKED_STATUS DoSplitTablet(
       const scoped_refptr<TabletInfo>& source_tablet_info, std::string split_encoded_key,
-      std::string split_partition_key, bool is_manual_split);
+      std::string split_partition_key, ManualSplit is_manual_split);
 
   // Splits tablet using specified split_hash_code as a split point.
   CHECKED_STATUS DoSplitTablet(
       const scoped_refptr<TabletInfo>& source_tablet_info, docdb::DocKeyHash split_hash_code,
-      bool is_manual_split);
+      ManualSplit is_manual_split);
 
   // Calculate the total number of replicas which are being handled by servers in state.
   int64_t GetNumRelevantReplicas(const BlacklistPB& state, bool leaders_only);
@@ -1424,9 +1439,6 @@ class CatalogManager :
   Master *master_;
   Atomic32 closing_;
 
-  // Random number generator used for selecting replica locations.
-  ThreadSafeRandom rng_;
-
   std::unique_ptr<SysCatalogTable> sys_catalog_;
 
   // Mutex to avoid concurrent remote bootstrap sessions.
@@ -1562,20 +1574,24 @@ class CatalogManager :
   // Performs the provided action with the sys catalog shared tablet instance, or sets up an error
   // if the tablet is not found.
   template <class Req, class Resp, class F>
-  CHECKED_STATUS PerformOnSysCatalogTablet(
-      const Req& req, Resp* resp, const F& f);
+  Status PerformOnSysCatalogTablet(const Req& req, Resp* resp, const F& f);
 
   virtual bool CDCStreamExistsUnlocked(const CDCStreamId& id) REQUIRES_SHARED(mutex_);
 
-  CHECKED_STATUS CollectTable(
+  Status CollectTable(
       const TableDescription& table_description,
       CollectFlags flags,
       std::vector<TableDescription>* all_tables,
       std::unordered_set<NamespaceId>* parent_colocated_table_ids);
 
+  Status SplitTablet(const scoped_refptr<TabletInfo>& tablet, ManualSplit is_manual_split);
+
   void SplitTabletWithKey(
       const scoped_refptr<TabletInfo>& tablet, const std::string& split_encoded_key,
-      const std::string& split_partition_key, bool is_manual_split);
+      const std::string& split_partition_key, ManualSplit is_manual_split);
+
+  Status ValidateSplitCandidate(
+      const scoped_refptr<TabletInfo>& tablet, ManualSplit is_manual_split);
 
   // From the list of TServers in 'ts_descs', return the ones that match any placement policy
   // in 'placement_info'. Returns error if there are insufficient TServers to match the
@@ -1595,7 +1611,7 @@ class CatalogManager :
 
   bool IsReplicationInfoSet(const ReplicationInfoPB& replication_info);
 
-  CHECKED_STATUS ValidateTableReplicationInfo(const ReplicationInfoPB& replication_info);
+  Status ValidateTableReplicationInfo(const ReplicationInfoPB& replication_info);
 
   // Return the tablespaces in the system and their associated replication info from
   // pg catalog tables.
@@ -1613,7 +1629,7 @@ class CatalogManager :
   void ScheduleRefreshTablespaceInfoTask(const bool schedule_now = false);
 
   // Helper function to refresh the tablespace info.
-  CHECKED_STATUS DoRefreshTablespaceInfo();
+  Status DoRefreshTablespaceInfo();
 
   // Processes committed consensus state for specified tablet from ts_desc.
   // Returns true if tablet was mutated.
@@ -1636,7 +1652,7 @@ class CatalogManager :
   using ReportedTablets = std::vector<ReportedTablet>;
 
   // Process tablets batch while processing tablet report.
-  CHECKED_STATUS ProcessTabletReportBatch(
+  Status ProcessTabletReportBatch(
       TSDescriptor* ts_desc,
       bool is_incremental,
       ReportedTablets::iterator begin,
@@ -1646,8 +1662,16 @@ class CatalogManager :
 
   size_t GetNumLiveTServersForPlacement(const PlacementId& placement_id);
 
+  TSDescriptorVector GetAllLiveNotBlacklistedTServers() const;
+
   const YQLPartitionsVTable& GetYqlPartitionsVtable() const;
 
+  void InitializeTableLoadState(
+      const TableId& table_id, TSDescriptorVector ts_descs, CMPerTableLoadState* state);
+
+  void InitializeGlobalLoadState(
+      TSDescriptorVector ts_descs, CMGlobalLoadState* state);
+
   // Should be bumped up when tablet locations are changed.
   std::atomic<uintptr_t> tablet_locations_version_{0};
 
diff --git a/src/yb/master/catalog_manager_bg_tasks.cc b/src/yb/master/catalog_manager_bg_tasks.cc
index 9151d7796f..8941017371 100644
--- a/src/yb/master/catalog_manager_bg_tasks.cc
+++ b/src/yb/master/catalog_manager_bg_tasks.cc
@@ -148,11 +148,17 @@ void CatalogManagerBgTasks::Run() {
 
       bool processed_tablets = false;
       if (!to_process.empty()) {
+        // For those tablets which need to be created in this round, assign replicas.
+        TSDescriptorVector ts_descs = catalog_manager_->GetAllLiveNotBlacklistedTServers();
+        CMGlobalLoadState global_load_state;
+        catalog_manager_->InitializeGlobalLoadState(ts_descs, &global_load_state);
         // Transition tablet assignment state from preparing to creating, send
         // and schedule creation / deletion RPC messages, etc.
+        // This is done table by table.
         for (const auto& entries : to_process) {
           LOG(INFO) << "Processing pending assignments for table: " << entries.first;
-          Status s = catalog_manager_->ProcessPendingAssignments(entries.second);
+          Status s = catalog_manager_->ProcessPendingAssignmentsPerTable(
+              entries.first, entries.second, &global_load_state);
           WARN_NOT_OK(s, "Assignment failed");
           // Set processed_tablets as true if the call succeeds for at least one table.
           processed_tablets = processed_tablets || s.ok();
@@ -170,11 +176,13 @@ void CatalogManagerBgTasks::Run() {
       }
 
       TableInfoMap table_info_map;
+      TabletInfoMap tablet_info_map;
       {
         CatalogManager::SharedLock lock(catalog_manager_->mutex_);
         table_info_map = *catalog_manager_->table_ids_map_;
+        tablet_info_map = *catalog_manager_->tablet_map_;
       }
-      catalog_manager_->tablet_split_manager()->MaybeDoSplitting(table_info_map);
+      catalog_manager_->tablet_split_manager()->MaybeDoSplitting(table_info_map, tablet_info_map);
 
       if (!to_delete.empty() || catalog_manager_->AreTablesDeleting()) {
         catalog_manager_->CleanUpDeletedTables();
diff --git a/src/yb/master/catalog_manager_if.h b/src/yb/master/catalog_manager_if.h
index f3abe25329..63f9012f86 100644
--- a/src/yb/master/catalog_manager_if.h
+++ b/src/yb/master/catalog_manager_if.h
@@ -218,8 +218,7 @@ class CatalogManagerIf {
   virtual scoped_refptr<TableInfo> NewTableInfo(TableId id) = 0;
 
   // If is_manual_split is true, we will not call ShouldSplitValidCandidate.
-  virtual CHECKED_STATUS SplitTablet(
-      const TabletId& tablet_id, bool is_manual_split) = 0;
+  virtual CHECKED_STATUS SplitTablet(const TabletId& tablet_id, ManualSplit is_manual_split) = 0;
 
   virtual CHECKED_STATUS TEST_SplitTablet(
       const scoped_refptr<TabletInfo>& source_tablet_info, docdb::DocKeyHash split_hash_code) = 0;
diff --git a/src/yb/master/catalog_manager_util.cc b/src/yb/master/catalog_manager_util.cc
index 0af66764fe..96b29d5bc8 100644
--- a/src/yb/master/catalog_manager_util.cc
+++ b/src/yb/master/catalog_manager_util.cc
@@ -464,5 +464,20 @@ void CatalogManagerUtil::GetAllAffinitizedZones(
   }
 }
 
+bool CMPerTableLoadState::CompareLoads(const TabletServerId &ts1, const TabletServerId &ts2) {
+  if (per_ts_load_[ts1] != per_ts_load_[ts2]) {
+    return per_ts_load_[ts1] < per_ts_load_[ts2];
+  }
+  if (global_load_state_->GetGlobalLoad(ts1) == global_load_state_->GetGlobalLoad(ts2)) {
+    return ts1 < ts2;
+  }
+  return global_load_state_->GetGlobalLoad(ts1) < global_load_state_->GetGlobalLoad(ts2);
+}
+
+void CMPerTableLoadState::SortLoad() {
+  Comparator comp(this);
+  std::sort(sorted_load_.begin(), sorted_load_.end(), comp);
+}
+
 } // namespace master
 } // namespace yb
diff --git a/src/yb/master/catalog_manager_util.h b/src/yb/master/catalog_manager_util.h
index 30ade4be81..91a9419d86 100644
--- a/src/yb/master/catalog_manager_util.h
+++ b/src/yb/master/catalog_manager_util.h
@@ -18,6 +18,7 @@
 #include <vector>
 
 #include "yb/consensus/consensus_fwd.h"
+#include "yb/master/catalog_entity_info.h"
 #include "yb/master/master_fwd.h"
 #include "yb/master/ts_descriptor.h"
 
@@ -30,6 +31,7 @@ namespace master {
 using ZoneToDescMap = std::unordered_map<string, TSDescriptorVector>;
 
 class SetPreferredZonesRequestPB;
+struct Comparator;
 
 class CatalogManagerUtil {
  public:
@@ -104,12 +106,69 @@ class CatalogManagerUtil {
   static void GetAllAffinitizedZones(
       const ReplicationInfoPB* replication_info, vector<AffinitizedZonesSet>* affinitized_zones);
 
+  template<class LoadState>
+  static void FillTableLoadState(const scoped_refptr<TableInfo>& table_info, LoadState* state) {
+    auto tablets = table_info->GetTablets(IncludeInactive::kTrue);
+
+    for (const auto& tablet : tablets) {
+      // Ignore if tablet is not running.
+      {
+        auto tablet_lock = tablet->LockForRead();
+        if (!tablet_lock->is_running()) {
+          continue;
+        }
+      }
+      auto replica_locs = tablet->GetReplicaLocations();
+
+      for (const auto& loc : *replica_locs) {
+        // Ignore replica if not present in the tserver list passed.
+        if (state->per_ts_load_.count(loc.first) == 0) {
+          continue;
+        }
+        // Account for this load.
+        state->per_ts_load_[loc.first]++;
+      }
+    }
+  }
+
  private:
   CatalogManagerUtil();
 
   DISALLOW_COPY_AND_ASSIGN(CatalogManagerUtil);
 };
 
+class CMGlobalLoadState {
+ public:
+  uint32_t GetGlobalLoad(const TabletServerId& id) {
+    return per_ts_load_[id];
+  }
+  std::unordered_map<TabletServerId, uint32_t> per_ts_load_;
+};
+
+class CMPerTableLoadState {
+ public:
+  explicit CMPerTableLoadState(CMGlobalLoadState* global_state)
+    : global_load_state_(global_state) {}
+
+  bool CompareLoads(const TabletServerId& ts1, const TabletServerId& ts2);
+
+  void SortLoad();
+
+  std::vector<TabletServerId> sorted_load_;
+  std::unordered_map<TabletServerId, uint32_t> per_ts_load_;
+  CMGlobalLoadState* global_load_state_;
+};
+
+struct Comparator {
+  explicit Comparator(CMPerTableLoadState* state) : state_(state) {}
+
+  bool operator()(const TabletServerId& id1, const TabletServerId& id2) {
+    return state_->CompareLoads(id1, id2);
+  }
+
+  CMPerTableLoadState* state_;
+};
+
 } // namespace master
 } // namespace yb
 
diff --git a/src/yb/master/cdc_consumer_split_driver.h b/src/yb/master/cdc_consumer_split_driver.h
index a987f28d0b..4951872559 100644
--- a/src/yb/master/cdc_consumer_split_driver.h
+++ b/src/yb/master/cdc_consumer_split_driver.h
@@ -14,7 +14,7 @@
 #define YB_MASTER_CDC_CONSUMER_SPLIT_DRIVER_H
 
 #include "yb/common/entity_ids_types.h"
-#include "yb/master/catalog_entity_info.h"
+#include "yb/master/tablet_split_fwd.h"
 #include "yb/util/status.h"
 
 namespace yb {
diff --git a/src/yb/master/cluster_balance.cc b/src/yb/master/cluster_balance.cc
index 3e4656417f..c5f9458a82 100644
--- a/src/yb/master/cluster_balance.cc
+++ b/src/yb/master/cluster_balance.cc
@@ -475,6 +475,8 @@ void ClusterLoadBalancer::RunLoadBalancerWithOptions(Options* options) {
   VLOG(1) << "Number of remote bootstraps before running load balancer: "
           << global_state_->total_starting_tablets_;
 
+  bool task_added = false;
+
   // Iterate over all the tables to take actions based on the data collected on the previous loop.
   for (const auto& table : GetTableMap()) {
     state_ = nullptr;
@@ -507,7 +509,8 @@ void ClusterLoadBalancer::RunLoadBalancerWithOptions(Options* options) {
     // Handle adding and moving replicas.
     for ( ; remaining_adds > 0; --remaining_adds) {
       if (state_->allow_only_leader_balancing_) {
-        LOG(INFO) << "Skipping Add replicas. Only leader balancing table " << table.first;
+        YB_LOG_EVERY_N_SECS(INFO, 30) << "Skipping Add replicas. Only leader balancing table "
+                                      << table.first;
         break;
       }
       auto handle_add = HandleAddReplicas(&out_tablet_id, &out_from_ts, &out_to_ts);
@@ -520,6 +523,8 @@ void ClusterLoadBalancer::RunLoadBalancerWithOptions(Options* options) {
       if (!*handle_add) {
         break;
       }
+
+      task_added = true;
     }
     if (PREDICT_FALSE(FLAGS_TEST_load_balancer_handle_under_replicated_tablets_only)) {
       LOG(INFO) << "Skipping remove replicas and leader moves for " << table.first;
@@ -529,7 +534,8 @@ void ClusterLoadBalancer::RunLoadBalancerWithOptions(Options* options) {
     // Handle cleanup after over-replication.
     for ( ; remaining_removals > 0; --remaining_removals) {
       if (state_->allow_only_leader_balancing_) {
-        LOG(INFO) << "Skipping remove replicas. Only leader balancing table " << table.first;
+        YB_LOG_EVERY_N_SECS(INFO, 30) << "Skipping remove replicas. Only leader balancing table "
+                                      << table.first;
         break;
       }
       auto handle_remove = HandleRemoveReplicas(&out_tablet_id, &out_from_ts);
@@ -542,6 +548,8 @@ void ClusterLoadBalancer::RunLoadBalancerWithOptions(Options* options) {
       if (!*handle_remove) {
         break;
       }
+
+      task_added = true;
     }
 
     // Handle tablet servers with too many leaders.
@@ -563,10 +571,12 @@ void ClusterLoadBalancer::RunLoadBalancerWithOptions(Options* options) {
       if (!*handle_leader) {
         break;
       }
+
+      task_added = true;
     }
   }
 
-  RecordActivity(master_errors);
+  RecordActivity(task_added, master_errors);
 }
 
 void ClusterLoadBalancer::RunLoadBalancer(Options* options) {
@@ -596,7 +606,7 @@ void ClusterLoadBalancer::RunLoadBalancer(Options* options) {
   }
 }
 
-void ClusterLoadBalancer::RecordActivity(uint32_t master_errors) {
+void ClusterLoadBalancer::RecordActivity(bool tasks_added_in_this_run, uint32_t master_errors) {
   // Update the list of tables for whom load-balancing has been
   // skipped in this run.
   {
@@ -609,6 +619,12 @@ void ClusterLoadBalancer::RecordActivity(uint32_t master_errors) {
     table_tasks += table.second->NumLBTasks();
   }
 
+  if (!master_errors && !table_tasks && tasks_added_in_this_run) {
+    VLOG(1) << "Tasks scheduled by Load balancer have already completed. Force setting table tasks "
+               "count to 1 so that it does not appear idle";
+    ++table_tasks;
+  }
+
   struct ActivityInfo ai {table_tasks, master_errors};
 
   // Update circular buffer summary.
diff --git a/src/yb/master/cluster_balance.h b/src/yb/master/cluster_balance.h
index ad056e9004..abaecb3622 100644
--- a/src/yb/master/cluster_balance.h
+++ b/src/yb/master/cluster_balance.h
@@ -415,7 +415,8 @@ class ClusterLoadBalancer {
   bool can_perform_global_operations_ = false;
 
   // Record load balancer activity for tables and tservers.
-  void RecordActivity(uint32_t master_errors) REQUIRES_SHARED(catalog_manager_->mutex_);
+  void RecordActivity(bool tasks_added_in_this_run, uint32_t master_errors)
+      REQUIRES_SHARED(catalog_manager_->mutex_);
 
   typedef rw_spinlock LockType;
   mutable LockType mutex_;
diff --git a/src/yb/master/cluster_balance_util.cc b/src/yb/master/cluster_balance_util.cc
index 3a8aa4f478..6664ccd55c 100644
--- a/src/yb/master/cluster_balance_util.cc
+++ b/src/yb/master/cluster_balance_util.cc
@@ -152,17 +152,19 @@ Status PerTableLoadState::UpdateTablet(TabletInfo *tablet) {
       if (!blacklisted_servers_.count(ts_uuid)) {
         if (GetAtomicFlag(&FLAGS_allow_leader_balancing_dead_node)) {
           allow_only_leader_balancing_ = true;
-          LOG(INFO) << strings::Substitute("Master leader not received "
-                "heartbeat from ts $0. Only performing leader balancing for tables with replicas"
-                " in this TS.", ts_uuid);
+          YB_LOG_EVERY_N_SECS(INFO, 30)
+              << strings::Substitute("Master leader not received heartbeat from ts $0. "
+                                     "Only performing leader balancing for tables with replicas"
+                                     " in this TS.", ts_uuid);
         } else {
           return STATUS_SUBSTITUTE(LeaderNotReadyToServe, "Master leader has not yet received "
               "heartbeat from ts $0. Aborting load balancing.", ts_uuid);
         }
       } else {
-        LOG(INFO) << strings::Substitute("Master leader not received heartbeat from ts $0"
-                              " but it is blacklisted. Continuing LB operations for tables"
-                              " with replicas in this TS.", ts_uuid);
+        YB_LOG_EVERY_N_SECS(INFO, 30)
+            << strings::Substitute("Master leader not received heartbeat from ts $0 but it is "
+                                   "blacklisted. Continuing LB operations for tables with replicas"
+                                   " in this TS.", ts_uuid);
       }
     }
 
@@ -411,8 +413,8 @@ Result<bool> PerTableLoadState::CanAddTabletToTabletServer(
   }
   // If we ask to use placement information, check against it.
   if (placement_info && !GetValidPlacement(to_ts, placement_info).has_value()) {
-    LOG(INFO) << "tablet server " << to_ts << " has invalid placement info. "
-              << "Not allowing it to take more tablets.";
+    YB_LOG_EVERY_N_SECS(INFO, 30) << "tablet server " << to_ts << " has invalid placement info. "
+                                  << "Not allowing it to take more tablets.";
     return false;
   }
   // If this server has a pending tablet delete, don't use it.
diff --git a/src/yb/master/master-test.cc b/src/yb/master/master-test.cc
index 6da233517c..a64a485b62 100644
--- a/src/yb/master/master-test.cc
+++ b/src/yb/master/master-test.cc
@@ -554,6 +554,13 @@ TEST_F(MasterTest, TestCatalog) {
     ASSERT_EQ(0, tables.tables_size());
   }
 
+  {
+    ListTablesRequestPB req;
+    req.add_relation_type_filter(MATVIEW_TABLE_RELATION);
+    DoListTables(req, &tables);
+    ASSERT_EQ(0, tables.tables_size());
+  }
+
   {
     ListTablesRequestPB req;
     req.add_relation_type_filter(SYSTEM_TABLE_RELATION);
diff --git a/src/yb/master/master_ddl.proto b/src/yb/master/master_ddl.proto
index 3f6abee96c..dcc2328097 100644
--- a/src/yb/master/master_ddl.proto
+++ b/src/yb/master/master_ddl.proto
@@ -105,6 +105,8 @@ message CreateTableRequestPB {
   // This field should be used to create or restore table with a fixed set of partitions.
   repeated PartitionPB partitions = 22;
 
+  optional bool is_matview = 26;
+
   optional bytes matview_pg_table_id = 24;
 }
 
diff --git a/src/yb/master/master_fwd.h b/src/yb/master/master_fwd.h
index c4c4ec9094..9007ab0408 100644
--- a/src/yb/master/master_fwd.h
+++ b/src/yb/master/master_fwd.h
@@ -27,6 +27,7 @@
 #include "yb/gutil/ref_counted.h"
 
 #include "yb/master/master_backup.fwd.h"
+#include "yb/master/tablet_split_fwd.h"
 
 #include "yb/util/enums.h"
 #include "yb/util/math_util.h"
@@ -74,8 +75,6 @@ class SnapshotState;
 class SysCatalogTable;
 class SysConfigInfo;
 class SysRowEntries;
-class TabletSplitCompleteHandlerIf;
-class TabletSplitManager;
 class TSDescriptor;
 class TSManager;
 class UDTypeInfo;
@@ -85,7 +84,6 @@ class YsqlTablespaceManager;
 class YsqlTransactionDdl;
 
 struct CDCConsumerStreamInfo;
-struct SplitTabletIds;
 struct TableDescription;
 struct TabletReplica;
 struct TabletReplicaDriveInfo;
diff --git a/src/yb/master/master_types.proto b/src/yb/master/master_types.proto
index c286744f9e..74d940a2c8 100644
--- a/src/yb/master/master_types.proto
+++ b/src/yb/master/master_types.proto
@@ -23,6 +23,7 @@ enum RelationType {
   SYSTEM_TABLE_RELATION = 1;
   USER_TABLE_RELATION = 2;
   INDEX_TABLE_RELATION = 3;
+  MATVIEW_TABLE_RELATION = 4;
 }
 
 enum SysRowEntryType {
diff --git a/src/yb/master/tablet_split_complete_handler.h b/src/yb/master/tablet_split_complete_handler.h
index 39b0df3da0..69a35e43aa 100644
--- a/src/yb/master/tablet_split_complete_handler.h
+++ b/src/yb/master/tablet_split_complete_handler.h
@@ -19,7 +19,7 @@
 
 #include "yb/common/entity_ids.h"
 
-#include "yb/master/catalog_entity_info.h"
+#include "yb/master/tablet_split_fwd.h"
 
 namespace yb {
 namespace master {
diff --git a/src/yb/master/tablet_split_driver.h b/src/yb/master/tablet_split_driver.h
index e774fbd153..ec112d58be 100644
--- a/src/yb/master/tablet_split_driver.h
+++ b/src/yb/master/tablet_split_driver.h
@@ -19,6 +19,8 @@
 
 #include "yb/common/entity_ids_types.h"
 
+#include "yb/master/tablet_split_fwd.h"
+
 #include "yb/util/status_fwd.h"
 
 namespace yb {
@@ -27,10 +29,11 @@ namespace master {
 class TabletSplitDriverIf {
  public:
   virtual ~TabletSplitDriverIf() {}
-  virtual CHECKED_STATUS SplitTablet(
-      const TabletId& tablet_id, bool is_manual_split) = 0;
+  virtual Status SplitTablet(
+      const TabletId& tablet_id, ManualSplit is_manual_split) = 0;
 };
 
 }  // namespace master
 }  // namespace yb
+
 #endif // YB_MASTER_TABLET_SPLIT_DRIVER_H
diff --git a/src/yb/master/tablet_split_fwd.h b/src/yb/master/tablet_split_fwd.h
new file mode 100644
index 0000000000..76bb9bf8a2
--- /dev/null
+++ b/src/yb/master/tablet_split_fwd.h
@@ -0,0 +1,34 @@
+// Copyright (c) YugaByte, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#ifndef YB_MASTER_TABLET_SPLIT_FWD_H
+#define YB_MASTER_TABLET_SPLIT_FWD_H
+
+#include "yb/util/strongly_typed_bool.h"
+
+namespace yb {
+namespace master {
+
+class TabletSplitCandidateFilterIf;
+class TabletSplitCompleteHandlerIf;
+class TabletSplitDriverIf;
+class TabletSplitManager;
+
+struct SplitTabletIds;
+
+YB_STRONGLY_TYPED_BOOL(ManualSplit);
+
+} // namespace master
+} // namespace yb
+
+#endif // YB_MASTER_TABLET_SPLIT_FWD_H
diff --git a/src/yb/master/tablet_split_manager.cc b/src/yb/master/tablet_split_manager.cc
index 90ac6a4fd6..3a6f1f93d5 100644
--- a/src/yb/master/tablet_split_manager.cc
+++ b/src/yb/master/tablet_split_manager.cc
@@ -18,12 +18,16 @@
 #include "yb/gutil/casts.h"
 #include "yb/gutil/map-util.h"
 
+#include "yb/common/partition.h"
 #include "yb/common/schema.h"
 
 #include "yb/master/async_rpc_tasks.h"
+#include "yb/master/catalog_entity_info.h"
 #include "yb/master/cdc_consumer_split_driver.h"
 #include "yb/master/master_error.h"
 #include "yb/master/master_fwd.h"
+#include "yb/master/tablet_split_candidate_filter.h"
+#include "yb/master/tablet_split_driver.h"
 #include "yb/master/tablet_split_manager.h"
 #include "yb/master/ts_descriptor.h"
 
@@ -52,6 +56,10 @@ DEFINE_uint64(outstanding_tablet_split_limit, 5,
               "Limit of the number of outstanding tablet splits. Limitation is disabled if this "
               "value is set to 0.");
 
+DEFINE_uint64(outstanding_tablet_split_limit_per_tserver, 1,
+              "Limit of the number of outstanding tablet splits per node. Limitation is disabled "
+              "if this value is set to 0.");
+
 DECLARE_bool(TEST_validate_all_tablet_candidates);
 
 DEFINE_bool(enable_tablet_split_of_pitr_tables, true,
@@ -87,6 +95,33 @@ namespace master {
 using strings::Substitute;
 using namespace std::literals;
 
+namespace {
+
+template <typename IdType>
+Status ValidateAgainstDisabledList(const IdType& id,
+                                   std::unordered_map<IdType, CoarseTimePoint>* map) {
+  const auto entry = map->find(id);
+  if (entry == map->end()) {
+    return Status::OK();
+  }
+
+  const auto ignored_until = entry->second;
+  if (ignored_until <= CoarseMonoClock::Now()) {
+    map->erase(entry);
+    return Status::OK();
+  }
+
+  VLOG(1) << Format("Table/tablet is ignored for splitting until $0. id: $1",
+                    ToString(ignored_until), id);
+  return STATUS_FORMAT(
+      IllegalState,
+      "Table/tablet is ignored for splitting until $0. id: $1",
+      ToString(ignored_until), id);
+}
+
+} // namespace
+
+
 TabletSplitManager::TabletSplitManager(
     TabletSplitCandidateFilterIf* filter,
     TabletSplitDriverIf* driver,
@@ -98,55 +133,65 @@ TabletSplitManager::TabletSplitManager(
     splitting_disabled_until_(CoarseDuration::zero()),
     last_run_time_(CoarseDuration::zero()) {}
 
-struct SplitCandidate {
-  TabletInfoPtr tablet;
-  uint64_t leader_sst_size;
-};
-
-static inline bool LargestTabletFirst(const SplitCandidate& c1, const SplitCandidate& c2) {
-  return c1.leader_sst_size > c2.leader_sst_size;
+Status TabletSplitManager::ValidateTableAgainstDisabledList(const TableId& table_id) {
+  UniqueLock<decltype(mutex_)> lock(mutex_);
+  return ValidateAgainstDisabledList(table_id, &ignore_table_for_splitting_until_);
 }
 
-Status TabletSplitManager::ValidateAgainstDisabledList(
-    const std::string& id, std::unordered_map<std::string, CoarseTimePoint>* map) {
+Status TabletSplitManager::ValidateTabletAgainstDisabledList(const TabletId& tablet_id) {
   UniqueLock<decltype(mutex_)> lock(mutex_);
-  const auto entry = map->find(id);
-  if (entry != map->end()) {
-    const auto ignored_until = entry->second;
-    if (ignored_until > CoarseMonoClock::Now()) {
-      VLOG(1) << Substitute("Table/tablet is ignored for splitting until $0. id: $1",
-                            ToString(ignored_until), id);
-      return STATUS_FORMAT(
-          NotSupported,
-          "Table/tablet is ignored for splitting until $0. id: $1", ToString(ignored_until), id);
-    } else {
-      map->erase(entry);
-    }
+  return ValidateAgainstDisabledList(tablet_id, &ignore_tablet_for_splitting_until_);
+}
+
+Status TabletSplitManager::ValidateIndexTablePartitioning(const TableInfo& table) {
+  if (!table.is_index()) {
+    return Status::OK();
   }
-  return Status::OK();
+
+  auto table_locked = table.LockForRead();
+  if (!table_locked->schema().has_table_properties() || !table_locked->pb.has_partition_schema()) {
+    return Status::OK();
+  }
+
+  // Nothing to validate for hash partitioned tables
+  if (PartitionSchema::IsHashPartitioning(table_locked->pb.partition_schema())) {
+    return Status::OK();
+  }
+
+  // Check partition key version is valid for tablet splitting
+  const auto& table_properties = table_locked->schema().table_properties();
+  if (table_properties.has_partition_key_version() &&
+      table_properties.partition_key_version() > 0) {
+    return Status::OK();
+  }
+
+  return STATUS_FORMAT(NotSupported,
+                       "Tablet splitting is not supported for the index table"
+                       " \"$0\" with table_id \"$1\". Please, rebuild the index!",
+                       table.name(), table.id());
 }
 
-Status TabletSplitManager::ValidateSplitCandidateTable(const TableInfo& table,
-    bool ignore_disabled_list) {
+Status TabletSplitManager::ValidateSplitCandidateTable(
+    const TableInfo& table,
+    const IgnoreDisabledList ignore_disabled_list) {
   if (PREDICT_FALSE(FLAGS_TEST_validate_all_tablet_candidates)) {
     return Status::OK();
   }
   if (table.is_deleted()) {
-    VLOG(1) << Substitute("Table is deleted; ignoring for splitting. table_id: $0", table.id());
+    VLOG(1) << Format("Table is deleted; ignoring for splitting. table_id: $0", table.id());
     return STATUS_FORMAT(
-        NotSupported,
-        "Table is deleted; ignoring for splitting. table_id: $0", table.id());
+        NotSupported, "Table is deleted; ignoring for splitting. table_id: $0", table.id());
   }
 
   if (!ignore_disabled_list) {
-    RETURN_NOT_OK(ValidateAgainstDisabledList(table.id(), &ignore_table_for_splitting_until_));
+    RETURN_NOT_OK(ValidateTableAgainstDisabledList(table.id()));
   }
 
   // Check if this table is covered by a PITR schedule.
   if (!FLAGS_enable_tablet_split_of_pitr_tables &&
       VERIFY_RESULT(filter_->IsTablePartOfSomeSnapshotSchedule(table))) {
-    VLOG(1) << Substitute("Tablet splitting is not supported for tables that are a part of"
-                          " some active PITR schedule, table_id: $0", table.id());
+    VLOG(1) << Format("Tablet splitting is not supported for tables that are a part of"
+                      " some active PITR schedule, table_id: $0", table.id());
     return STATUS_FORMAT(
         NotSupported,
         "Tablet splitting is not supported for tables that are a part of"
@@ -155,32 +200,32 @@ Status TabletSplitManager::ValidateSplitCandidateTable(const TableInfo& table,
   // Check if this table is part of a cdc stream.
   if (PREDICT_TRUE(!FLAGS_enable_tablet_split_of_xcluster_replicated_tables) &&
       filter_->IsCdcEnabled(table)) {
-    VLOG(1) << Substitute("Tablet splitting is not supported for tables that are a part of"
-                          " a CDC stream, table_id: $0", table.id());
+    VLOG(1) << Format("Tablet splitting is not supported for tables that are a part of"
+                      " a CDC stream, table_id: $0", table.id());
     return STATUS_FORMAT(
         NotSupported,
         "Tablet splitting is not supported for tables that are a part of"
         " a CDC stream, tablet_id: $0", table.id());
   }
   if (table.GetTableType() == TableType::TRANSACTION_STATUS_TABLE_TYPE) {
-    VLOG(1) << Substitute("Tablet splitting is not supported for transaction status tables, "
-                          "table_id: $0", table.id());
+    VLOG(1) << Format("Tablet splitting is not supported for transaction status tables, "
+                      "table_id: $0", table.id());
     return STATUS_FORMAT(
         NotSupported,
         "Tablet splitting is not supported for transaction status tables, table_id: $0",
         table.id());
   }
   if (table.is_system()) {
-    VLOG(1) << Substitute("Tablet splitting is not supported for system table: $0 with "
-                          "table_id: $1", table.name(), table.id());
+    VLOG(1) << Format("Tablet splitting is not supported for system table: $0 with "
+                      "table_id: $1", table.name(), table.id());
     return STATUS_FORMAT(
         NotSupported,
         "Tablet splitting is not supported for system table: $0 with table_id: $1",
         table.name(), table.id());
   }
   if (table.GetTableType() == REDIS_TABLE_TYPE) {
-    VLOG(1) << Substitute("Tablet splitting is not supported for YEDIS tables, table_id: $0",
-                          table.id());
+    VLOG(1) << Format("Tablet splitting is not supported for YEDIS tables, table_id: $0",
+                      table.id());
     return STATUS_FORMAT(
         NotSupported,
         "Tablet splitting is not supported for YEDIS tables, table_id: $0", table.id());
@@ -189,22 +234,25 @@ Status TabletSplitManager::ValidateSplitCandidateTable(const TableInfo& table,
       table.NumPartitions() >= FLAGS_tablet_split_limit_per_table) {
     // TODO(tsplit): Avoid tablet server of scanning tablets for the tables that already
     //  reached the split limit of tablet #6220
-    VLOG(1) << Substitute("Too many tablets for the table, table_id: $0, limit: $1",
-                          table.id(), FLAGS_tablet_split_limit_per_table);
+    VLOG(1) << Format("Too many tablets for the table, table_id: $0, limit: $1",
+                      table.id(), FLAGS_tablet_split_limit_per_table);
     return STATUS_EC_FORMAT(IllegalState, MasterError(MasterErrorPB::REACHED_SPLIT_LIMIT),
                             "Too many tablets for the table, table_id: $0, limit: $1",
                             table.id(), FLAGS_tablet_split_limit_per_table);
   }
   if (table.IsBackfilling()) {
-    VLOG(1) << Substitute("Backfill operation in progress, table_id: $0", table.id());
+    VLOG(1) << Format("Backfill operation in progress, table_id: $0", table.id());
     return STATUS_EC_FORMAT(IllegalState, MasterError(MasterErrorPB::SPLIT_OR_BACKFILL_IN_PROGRESS),
                             "Backfill operation in progress, table_id: $0", table.id());
   }
-  return Status::OK();
+
+  return ValidateIndexTablePartitioning(table);
 }
 
-Status TabletSplitManager::ValidateSplitCandidateTablet(const TabletInfo& tablet,
-    bool ignore_ttl_validation) {
+Status TabletSplitManager::ValidateSplitCandidateTablet(
+    const TabletInfo& tablet,
+    const IgnoreTtlValidation ignore_ttl_validation,
+    const IgnoreDisabledList ignore_disabled_list) {
   if (PREDICT_FALSE(FLAGS_TEST_validate_all_tablet_candidates)) {
     return Status::OK();
   }
@@ -227,7 +275,9 @@ Status TabletSplitManager::ValidateSplitCandidateTablet(const TabletInfo& tablet
         tablet.tablet_id());
   }
 
-  RETURN_NOT_OK(ValidateAgainstDisabledList(tablet.id(), &ignore_tablet_for_splitting_until_));
+  if (!ignore_disabled_list) {
+    RETURN_NOT_OK(ValidateTabletAgainstDisabledList(tablet.id()));
+  }
 
   {
     auto tablet_state = tablet.LockForRead()->pb.state();
@@ -258,9 +308,8 @@ void TabletSplitManager::MarkSmallKeyRangeTabletForSplitIgnore(const TabletId& t
   }
 }
 
-bool AllReplicasHaveFinishedCompaction(const TabletInfo& tablet_info) {
-  auto replica_map = tablet_info.GetReplicaLocations();
-  for (auto const& replica : *replica_map) {
+bool AllReplicasHaveFinishedCompaction(const TabletReplicaMap& replicas) {
+  for (const auto& replica : replicas) {
     if (replica.second.drive_info.may_have_orphaned_post_split_data) {
       return false;
     }
@@ -268,9 +317,9 @@ bool AllReplicasHaveFinishedCompaction(const TabletInfo& tablet_info) {
   return true;
 }
 
-void TabletSplitManager::ScheduleSplits(const unordered_set<TabletId>& splits_to_schedule) {
+void TabletSplitManager::ScheduleSplits(const std::unordered_set<TabletId>& splits_to_schedule) {
   for (const auto& tablet_id : splits_to_schedule) {
-    auto s = driver_->SplitTablet(tablet_id, false /* is_manual_split */);
+    auto s = driver_->SplitTablet(tablet_id, ManualSplit::kFalse);
     if (!s.ok()) {
       WARN_NOT_OK(s, Format("Failed to start/restart split for tablet_id: $0.", tablet_id));
     } else {
@@ -279,25 +328,183 @@ void TabletSplitManager::ScheduleSplits(const unordered_set<TabletId>& splits_to
   }
 }
 
-void TabletSplitManager::DoSplitting(const TableInfoMap& table_info_map) {
-  // Splits which are tracked by an AsyncGetTabletSplitKey or AsyncSplitTablet task.
-  unordered_set<TabletId> splits_with_task;
-  // Splits for which at least one child tablet is still undergoing compaction.
-  unordered_set<TabletId> compacting_splits;
-  // Splits that need to be started / restarted.
-  unordered_set<TabletId> splits_to_schedule;
-  // New split candidates. The chosen candidates are eventually added to splits_to_schedule.
-  vector<SplitCandidate> new_split_candidates;
+// A cache of the shared_ptrs to each tablet's replicas, to avoid having to repeatedly lock the
+// tablet, and to ensure that we use a consistent set of replicas for each tablet within each
+// iteration of the tablet split manager.
+class TabletReplicaMapCache {
+ public:
+  const std::shared_ptr<const TabletReplicaMap> GetOrAdd(const TabletInfo& tablet) {
+    auto it = replica_cache_.find(tablet.id());
+    if (it != replica_cache_.end()) {
+      return it->second;
+    } else {
+      const std::shared_ptr<const TabletReplicaMap> replicas = tablet.GetReplicaLocations();
+      if (replicas->empty()) {
+        LOG(WARNING) << "No replicas found for tablet. Id: " << tablet.id();
+      }
+      return replica_cache_[tablet.id()] = replicas;
+    }
+  }
+
+ private:
+  std::unordered_map<TabletId, std::shared_ptr<const TabletReplicaMap>> replica_cache_;
+};
+
+class OutstandingSplitState {
+ public:
+  OutstandingSplitState(
+      const TabletInfoMap& tablet_info_map, TabletReplicaMapCache* replica_cache):
+      tablet_info_map_{tablet_info_map}, replica_cache_{replica_cache} {}
 
   // Helper method to determine if more splits can be scheduled, or if we should exit early.
-  const auto can_split_more = [&]() {
-    uint64_t outstanding_splits = splits_with_task.size() +
-                                  compacting_splits.size() +
-                                  splits_to_schedule.size();
+  bool CanSplitMoreGlobal() const {
+    uint64_t outstanding_splits = splits_with_task_.size() +
+                                  compacting_splits_.size() +
+                                  splits_to_schedule_.size();
     return FLAGS_outstanding_tablet_split_limit == 0 ||
            outstanding_splits < FLAGS_outstanding_tablet_split_limit;
+  }
+
+  bool CanSplitMoreOnReplicas(const TabletReplicaMap& replicas) const {
+    for (const auto& location : replicas) {
+      auto it = ts_to_ongoing_splits_.find(location.first);
+      if (it != ts_to_ongoing_splits_.end() &&
+          it->second.size() >= FLAGS_outstanding_tablet_split_limit_per_tserver) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  bool HasSplitWithTask(const TabletId& split_tablet_id) const {
+    return splits_with_task_.count(split_tablet_id) != 0;
+  }
+
+  void AddSplitWithTask(const TabletId& split_tablet_id) {
+    splits_with_task_.insert(split_tablet_id);
+    auto it = tablet_info_map_.find(split_tablet_id);
+    if (it == tablet_info_map_.end()) {
+      LOG(WARNING) << "Split tablet with task not found in tablet info map. ID: "
+                   << split_tablet_id;
+      return;
+    }
+    TrackTserverSplits(split_tablet_id, *replica_cache_->GetOrAdd(*it->second));
+    auto l = it->second->LockForRead();
+    for (auto child_id : l->pb.split_tablet_ids()) {
+      // Track split_tablet_id as an ongoing split on its children's tservers.
+      TrackTserverSplits(split_tablet_id, child_id);
+    }
+  }
+
+  void AddSplitToRestart(const TabletId& split_tablet_id, const TabletInfo& split_child) {
+    if (compacting_splits_.count(split_tablet_id) == 0) {
+      bool inserted_split_to_schedule = splits_to_schedule_.insert(split_tablet_id).second;
+      if (inserted_split_to_schedule) {
+        // Track split_tablet_id as an ongoing split on its tservers. This is required since it is
+        // possible that one of the split children is not running yet, but we still want to count
+        // the split against the limits of the tservers on which the children will eventually
+        // appear.
+        TrackTserverSplits(split_tablet_id, split_tablet_id);
+      }
+    }
+    TrackTserverSplits(split_tablet_id, *replica_cache_->GetOrAdd(split_child));
+  }
+
+  void AddCompactingSplit(
+      const TabletId& split_tablet_id, const TabletInfo& split_child) {
+    // It's possible that one child subtablet leads us to insert the parent tablet id into
+    // splits_to_schedule, and another leads us to insert into compacting_splits. In this
+    // case, it means one of the children is live, thus both children have been created and
+    // the split RPC does not need to be scheduled.
+    bool was_scheduled_for_split = splits_to_schedule_.erase(split_tablet_id);
+    if (was_scheduled_for_split) {
+      LOG(INFO) << Substitute("Found compacting split child ($0), so removing split parent "
+                              "($1) from splits to schedule.", split_child.id(), split_tablet_id);
+    }
+    bool inserted_compacting_split = compacting_splits_.insert(split_tablet_id).second;
+    if (inserted_compacting_split && !was_scheduled_for_split) {
+      // Track split_tablet_id as an ongoing split on its tservers. This is required since it is
+      // possible that one of the split children is not running yet, but we still want to count
+      // the split against the limits of the tservers on which the children will eventually
+      // appear.
+      TrackTserverSplits(split_tablet_id, split_tablet_id);
+    }
+    TrackTserverSplits(split_tablet_id, *replica_cache_->GetOrAdd(split_child));
+  }
+
+  const std::unordered_set<TabletId>& GetSplitsToSchedule() const {
+    return splits_to_schedule_;
+  }
+
+  void AddCandidate(TabletInfoPtr tablet, uint64_t leader_sst_size) {
+    new_split_candidates_.emplace_back(SplitCandidate{tablet, leader_sst_size});
+  }
+
+  void ProcessCandidates() {
+    // Add any new splits to the set of splits to schedule (while respecting the max number of
+    // outstanding splits).
+    if (CanSplitMoreGlobal()) {
+      if (FLAGS_sort_automatic_tablet_splitting_candidates) {
+        sort(new_split_candidates_.begin(), new_split_candidates_.end(), LargestTabletFirst);
+      }
+      for (const auto& candidate : new_split_candidates_) {
+        if (!CanSplitMoreGlobal()) {
+          break;
+        }
+        auto replicas = replica_cache_->GetOrAdd(*candidate.tablet);
+        if (!CanSplitMoreOnReplicas(*replicas)) {
+          continue;
+        }
+        splits_to_schedule_.insert(candidate.tablet->id());
+        TrackTserverSplits(candidate.tablet->id(), *replicas);
+      }
+    }
+  }
+
+ private:
+  const TabletInfoMap& tablet_info_map_;
+  TabletReplicaMapCache* replica_cache_;
+  // Splits which are tracked by an AsyncGetTabletSplitKey or AsyncSplitTablet task.
+  std::unordered_set<TabletId> splits_with_task_;
+  // Splits for which at least one child tablet is still undergoing compaction.
+  std::unordered_set<TabletId> compacting_splits_;
+  // Splits that need to be started / restarted.
+  std::unordered_set<TabletId> splits_to_schedule_;
+
+  struct SplitCandidate {
+    TabletInfoPtr tablet;
+    uint64_t leader_sst_size;
   };
+  // New split candidates. The chosen candidates are eventually added to splits_to_schedule.
+  vector<SplitCandidate> new_split_candidates_;
+
+  std::unordered_map<TabletServerId, std::unordered_set<TabletId>> ts_to_ongoing_splits_;
+
+  // Tracks split_tablet_id as an ongoing split on the replicas of replica_tablet_id, which is
+  // either split_tablet_id itself or one of split_tablet_id's children.
+  void TrackTserverSplits(const TabletId& split_tablet_id, const TabletId& replica_tablet_id) {
+    auto it = tablet_info_map_.find(replica_tablet_id);
+    if (it == tablet_info_map_.end()) {
+      LOG(INFO) << "Tablet not found in tablet info map. ID: " << replica_tablet_id;
+      return;
+    }
+    TrackTserverSplits(split_tablet_id, *replica_cache_->GetOrAdd(*it->second));
+  }
+
+  void TrackTserverSplits(const TabletId& tablet_id, const TabletReplicaMap& split_replicas) {
+    for (const auto& location : split_replicas) {
+      VLOG(1) << "Inserting location " << location.first << " for tablet " << tablet_id;
+      ts_to_ongoing_splits_[location.first].insert(tablet_id);
+    }
+  }
+
+  static inline bool LargestTabletFirst(const SplitCandidate& c1, const SplitCandidate& c2) {
+    return c1.leader_sst_size > c2.leader_sst_size;
+  }
+};
 
+void TabletSplitManager::DoSplitting(
+    const TableInfoMap& table_info_map, const TabletInfoMap& tablet_info_map) {
   // TODO(asrivastava): We might want to loop over all running tables when determining outstanding
   // splits, to avoid missing outstanding splits for tables that have recently become invalid for
   // splitting. This is most critical for tables that frequently switch between being valid and
@@ -310,16 +517,25 @@ void TabletSplitManager::DoSplitting(const TableInfoMap& table_info_map) {
     }
   }
 
+  TabletReplicaMapCache replica_cache;
+  OutstandingSplitState state(tablet_info_map, &replica_cache);
   for (const auto& table : valid_tables) {
     for (const auto& task : table->GetTasks()) {
       // These tasks will retry automatically until they succeed or fail.
       if (task->type() == yb::server::MonitoredTask::ASYNC_GET_TABLET_SPLIT_KEY ||
           task->type() == yb::server::MonitoredTask::ASYNC_SPLIT_TABLET) {
         const TabletId tablet_id = static_cast<AsyncTabletLeaderTask*>(task.get())->tablet_id();
-        splits_with_task.insert(tablet_id);
+        auto tablet_info_it = tablet_info_map.find(tablet_id);
+        if (tablet_info_it != tablet_info_map.end()) {
+          const auto& tablet = tablet_info_it->second;
+          state.AddSplitWithTask(tablet->id());
+        } else {
+          LOG(WARNING) << "Could not find tablet info for tablet with task. Tablet id: "
+                      << tablet_id;
+        }
         LOG(INFO) << Substitute("Found split with ongoing task. Task type: $0. "
                                 "Split parent id: $1.", task->type_name(), tablet_id);
-        if (!can_split_more()) {
+        if (!state.CanSplitMoreGlobal()) {
           return;
         }
       }
@@ -328,79 +544,64 @@ void TabletSplitManager::DoSplitting(const TableInfoMap& table_info_map) {
 
   for (const auto& table : valid_tables) {
     for (const auto& tablet : table->GetTablets()) {
-      if (!can_split_more()) {
+      if (!state.CanSplitMoreGlobal()) {
         break;
       }
-      if (splits_with_task.count(tablet->id())) {
+      if (state.HasSplitWithTask(tablet->id())) {
         continue;
       }
 
       auto tablet_lock = tablet->LockForRead();
-      // Ignore a tablet as a new split candidate if it is part of an outstanding split.
-      bool ignore_as_candidate = false;
       if (tablet_lock->pb.has_split_parent_tablet_id()) {
         const TabletId& parent_id = tablet_lock->pb.split_parent_tablet_id();
-        if (splits_with_task.count(parent_id) != 0) {
+        if (state.HasSplitWithTask(parent_id)) {
           continue;
         }
+
+        // If a split child is not running, schedule a restart for the split.
         if (!tablet_lock->is_running()) {
-          // Recently split child is not running; restart the split.
-          ignore_as_candidate = true;
           LOG(INFO) << Substitute("Found split child ($0) that is not running. Adding parent ($1) "
                                   "to list of splits to reschedule.", tablet->id(), parent_id);
-          splits_to_schedule.insert(parent_id);
-        } else if (!AllReplicasHaveFinishedCompaction(*tablet)) {
-          // This (running) tablet is the child of a split and is still compacting. We assume that
-          // this split will eventually complete for both tablets.
-          ignore_as_candidate = true;
-          LOG(INFO) << Substitute("Found split child ($0) that is compacting. Adding parent ($1) "
-                                  " to list of compacting splits.", tablet->id(), parent_id);
-          compacting_splits.insert(parent_id);
-        }
-        if (splits_to_schedule.count(parent_id) != 0 && compacting_splits.count(parent_id) != 0) {
-          // It's possible that one child subtablet leads us to insert the parent tablet id into
-          // splits_to_schedule, and another leads us to insert into compacting_splits. In this
-          // case, it means one of the children is live, thus both children have been created and
-          // the split RPC does not need to be scheduled.
-          LOG(INFO) << Substitute("Found compacting split child ($0), so removing split parent "
-                                  "($1) from splits to schedule.", tablet->id(), parent_id);
-          splits_to_schedule.erase(parent_id);
+          state.AddSplitToRestart(parent_id, *tablet);
+          continue;
         }
-      }
 
-      if (!ignore_as_candidate) {
-        auto drive_info_opt = tablet->GetLeaderReplicaDriveInfo();
-        if (!drive_info_opt.ok()) {
+        // If this (running) tablet is the child of a split and is still compacting, track it as a
+        // compacting split but do not schedule a restart (we assume that this split will eventually
+        // complete for both tablets).
+        if (!AllReplicasHaveFinishedCompaction(*replica_cache.GetOrAdd(*tablet))) {
+          LOG(INFO) << Substitute("Found split child ($0) that is compacting. Adding parent ($1) "
+                                  "to list of compacting splits.", tablet->id(), parent_id);
+          state.AddCompactingSplit(parent_id, *tablet);
           continue;
         }
-        if (ValidateSplitCandidateTablet(*tablet).ok() &&
-            filter_->ShouldSplitValidCandidate(*tablet, drive_info_opt.get()) &&
-            AllReplicasHaveFinishedCompaction(*tablet)) {
-          SplitCandidate candidate = {tablet, drive_info_opt.get().sst_files_size};
-          new_split_candidates.push_back(std::move(candidate));
+      }
+
+      // Check if this tablet is a valid candidate for splitting, and if so, add it to the list of
+      // split candidates.
+      auto drive_info_opt = tablet->GetLeaderReplicaDriveInfo();
+      if (!drive_info_opt.ok()) {
+        continue;
+      }
+      if (ValidateSplitCandidateTablet(*tablet).ok() &&
+          filter_->ShouldSplitValidCandidate(*tablet, drive_info_opt.get())) {
+        const auto replicas = replica_cache.GetOrAdd(*tablet);
+        if (AllReplicasHaveFinishedCompaction(*replicas) &&
+            state.CanSplitMoreOnReplicas(*replicas)) {
+          state.AddCandidate(tablet, drive_info_opt.get().sst_files_size);
         }
       }
     }
-    if (!can_split_more()) {
+    if (!state.CanSplitMoreGlobal()) {
       break;
     }
   }
 
-  // Add any new splits to the set of splits to schedule (while respecting the max number of
-  // outstanding splits).
-  if (can_split_more()) {
-    if (FLAGS_sort_automatic_tablet_splitting_candidates) {
-      sort(new_split_candidates.begin(), new_split_candidates.end(), LargestTabletFirst);
-    }
-    for (const auto& candidate : new_split_candidates) {
-      if (!can_split_more()) {
-        break;
-      }
-      splits_to_schedule.insert(candidate.tablet->id());
-    }
-  }
-
-  ScheduleSplits(splits_to_schedule);
+  // Sort candidates if required and add as many desired candidates to the list of splits to
+  // schedule as possible (while respecting the limits on ongoing splits).
+  state.ProcessCandidates();
+  // Schedule any new splits and any splits that need to be restarted.
+  ScheduleSplits(state.GetSplitsToSchedule());
 }
 
 bool TabletSplitManager::HasOutstandingTabletSplits(const TableInfoMap& table_info_map) {
@@ -445,7 +646,8 @@ void TabletSplitManager::DisableSplittingFor(const MonoDelta& disable_duration)
   splitting_disabled_until_ = CoarseMonoClock::Now() + disable_duration;
 }
 
-void TabletSplitManager::MaybeDoSplitting(const TableInfoMap& table_info_map) {
+void TabletSplitManager::MaybeDoSplitting(
+    const TableInfoMap& table_info_map, const TabletInfoMap& tablet_info_map) {
   if (!FLAGS_enable_automatic_tablet_splitting) {
     return;
   }
@@ -462,7 +664,7 @@ void TabletSplitManager::MaybeDoSplitting(const TableInfoMap& table_info_map) {
     is_running_ = false;
     return;
   }
-  DoSplitting(table_info_map);
+  DoSplitting(table_info_map, tablet_info_map);
   is_running_ = false;
   last_run_time_ = CoarseMonoClock::Now();
 }
diff --git a/src/yb/master/tablet_split_manager.h b/src/yb/master/tablet_split_manager.h
index d510344889..e20a5c8503 100644
--- a/src/yb/master/tablet_split_manager.h
+++ b/src/yb/master/tablet_split_manager.h
@@ -17,14 +17,13 @@
 #include <unordered_set>
 
 #include "yb/master/master_fwd.h"
-#include "yb/master/tablet_split_candidate_filter.h"
 #include "yb/master/tablet_split_complete_handler.h"
-#include "yb/master/tablet_split_driver.h"
 
 namespace yb {
 namespace master {
 
-using std::unordered_set;
+YB_STRONGLY_TYPED_BOOL(IgnoreTtlValidation);
+YB_STRONGLY_TYPED_BOOL(IgnoreDisabledList);
 
 class TabletSplitManager : public TabletSplitCompleteHandlerIf {
  public:
@@ -42,7 +41,7 @@ class TabletSplitManager : public TabletSplitCompleteHandlerIf {
   bool IsTabletSplittingComplete(const TableInfoMap& table_info_map);
 
   // Perform one round of tablet splitting. This method is not thread-safe.
-  void MaybeDoSplitting(const TableInfoMap& table_info_map);
+  void MaybeDoSplitting(const TableInfoMap& table_info_map, const TabletInfoMap& tablet_info_map);
 
   void ProcessSplitTabletResult(const Status& status,
                                 const TableId& consumer_table_id,
@@ -50,28 +49,32 @@ class TabletSplitManager : public TabletSplitCompleteHandlerIf {
 
   // Validate whether a candidate table is eligible for a split.
   // Any temporarily disabled tablets are assumed ineligible by default.
-  CHECKED_STATUS ValidateSplitCandidateTable(
-      const TableInfo& table, bool ignore_disabled_list = false);
+  Status ValidateSplitCandidateTable(
+      const TableInfo& table,
+      IgnoreDisabledList ignore_disabled_list = IgnoreDisabledList::kFalse);
 
   // Validate whether a candidate tablet is eligible for a split.
   // Any tablets with default TTL and a max file size for compaction limit are assumed
   // ineligible by default.
-  CHECKED_STATUS ValidateSplitCandidateTablet(
-      const TabletInfo& tablet, bool ignore_ttl_validation = false);
+  Status ValidateSplitCandidateTablet(
+      const TabletInfo& tablet,
+      IgnoreTtlValidation ignore_ttl_validation = IgnoreTtlValidation::kFalse,
+      IgnoreDisabledList ignore_disabled_list = IgnoreDisabledList::kFalse);
 
   void MarkTtlTableForSplitIgnore(const TableId& table_id);
 
   void MarkSmallKeyRangeTabletForSplitIgnore(const TabletId& tablet_id);
 
  private:
-  void ScheduleSplits(const unordered_set<TabletId>& splits_to_schedule);
+  void ScheduleSplits(const std::unordered_set<TabletId>& splits_to_schedule);
 
   bool HasOutstandingTabletSplits(const TableInfoMap& table_info_map);
 
-  void DoSplitting(const TableInfoMap& table_info_map);
+  void DoSplitting(const TableInfoMap& table_info_map, const TabletInfoMap& tablet_info_map);
 
-  Status ValidateAgainstDisabledList(const std::string& id,
-                                     std::unordered_map<std::string, CoarseTimePoint>* map);
+  Status ValidateIndexTablePartitioning(const TableInfo& table);
+  Status ValidateTableAgainstDisabledList(const TableId& table_id);
+  Status ValidateTabletAgainstDisabledList(const TabletId& tablet_id);
 
   TabletSplitCandidateFilterIf* filter_;
   TabletSplitDriverIf* driver_;
@@ -85,11 +88,12 @@ class TabletSplitManager : public TabletSplitCompleteHandlerIf {
   CoarseTimePoint splitting_disabled_until_;
   CoarseTimePoint last_run_time_;
 
-  std::mutex mutex_;
-  std::unordered_map<TableId, CoarseTimePoint> ignore_table_for_splitting_until_ GUARDED_BY(mutex_);
-  std::unordered_map<TabletId, CoarseTimePoint> ignore_tablet_for_splitting_until_
-      GUARDED_BY(mutex_);
+  template <typename IdType>
+  using DisabledSet = std::unordered_map<IdType, CoarseTimePoint>;
 
+  std::mutex mutex_;
+  DisabledSet<TableId> ignore_table_for_splitting_until_ GUARDED_BY(mutex_);
+  DisabledSet<TabletId> ignore_tablet_for_splitting_until_ GUARDED_BY(mutex_);
 };
 
 }  // namespace master
diff --git a/src/yb/master/ysql_transaction_ddl.cc b/src/yb/master/ysql_transaction_ddl.cc
index 6fb7ba4c04..ce55c938e7 100644
--- a/src/yb/master/ysql_transaction_ddl.cc
+++ b/src/yb/master/ysql_transaction_ddl.cc
@@ -133,7 +133,8 @@ void YsqlTransactionDdl::TransactionReceived(
   }
 }
 
-Result<bool> YsqlTransactionDdl::PgEntryExists(TableId pg_table_id, Result<uint32_t> entry_oid) {
+Result<bool> YsqlTransactionDdl::PgEntryExists(TableId pg_table_id, Result<uint32_t> entry_oid,
+                                               TableId relfilenode_oid) {
   auto tablet_peer = sys_catalog_->tablet_peer();
   if (!tablet_peer || !tablet_peer->tablet()) {
     return STATUS(ServiceUnavailable, "SysCatalog unavailable");
@@ -142,6 +143,8 @@ Result<bool> YsqlTransactionDdl::PgEntryExists(TableId pg_table_id, Result<uint3
   const Schema& pg_database_schema =
       *VERIFY_RESULT(catalog_tablet->metadata()->GetTableInfo(pg_table_id))->schema;
 
+  bool is_matview = relfilenode_oid.empty() ? false : true;
+
   // Use Scan to query the 'pg_database' table, filtering by our 'oid'.
   Schema projection;
   RETURN_NOT_OK(pg_database_schema.CreateProjectionByNames({"oid"}, &projection,
@@ -167,6 +170,14 @@ Result<bool> YsqlTransactionDdl::PgEntryExists(TableId pg_table_id, Result<uint3
   QLTableRow row;
   if (VERIFY_RESULT(iter->HasNext())) {
     RETURN_NOT_OK(iter->NextRow(&row));
+    if (is_matview) {
+      const auto relfilenode_col_id = VERIFY_RESULT(projection.ColumnIdByName("relfilenode")).rep();
+      const auto& relfilenode = row.GetValue(relfilenode_col_id);
+      if (relfilenode->uint32_value() != VERIFY_RESULT(GetPgsqlTableOid(relfilenode_oid))) {
+        return false;
+      }
+      return true;
+    }
     return true;
   }
   return false;
diff --git a/src/yb/master/ysql_transaction_ddl.h b/src/yb/master/ysql_transaction_ddl.h
index 01b45258d8..83ba83d911 100644
--- a/src/yb/master/ysql_transaction_ddl.h
+++ b/src/yb/master/ysql_transaction_ddl.h
@@ -51,7 +51,7 @@ class YsqlTransactionDdl {
 
   void VerifyTransaction(const TransactionMetadata& transaction,
                          std::function<Status(bool /* is_success */)> complete_callback);
-  Result<bool> PgEntryExists(TableId tableId, Result<uint32_t> entry_oid);
+  Result<bool> PgEntryExists(TableId tableId, Result<uint32_t> entry_oid, TableId relfilenode_oid);
 
  protected:
   void TransactionReceived(const TransactionMetadata& transaction,
diff --git a/src/yb/rocksdb/db/db_impl.cc b/src/yb/rocksdb/db/db_impl.cc
index cb5c8bd950..1105c3fd40 100644
--- a/src/yb/rocksdb/db/db_impl.cc
+++ b/src/yb/rocksdb/db/db_impl.cc
@@ -149,6 +149,11 @@ DEFINE_int32(compaction_priority_step_size, 5,
 DEFINE_int32(small_compaction_extra_priority, 1,
              "Small compaction will get small_compaction_extra_priority extra priority.");
 
+DEFINE_int32(automatic_compaction_extra_priority, 50,
+             "Assigns automatic compactions extra priority. This deprioritizes manual "
+             "compactions including those induced by the tserver (e.g. post-split compactions). "
+             "Suggested value between 0 and 50.");
+
 DEFINE_bool(rocksdb_use_logging_iterator, false,
             "Wrap newly created RocksDB iterators in a logging wrapper");
 
@@ -344,6 +349,13 @@ class DBImpl::CompactionTask : public ThreadPoolTask {
       result += FLAGS_small_compaction_extra_priority;
     }
 
+    // Adding extra priority to automatic compactions can have a large positive impact on
+    // performance for situations with many manual major compactions (e.g. insert-heavy workloads
+    // with tablet splitting enabled).
+    if (!compaction_->is_manual_compaction()) {
+      result += FLAGS_automatic_compaction_extra_priority;
+    }
+
     return result;
   }
 
diff --git a/src/yb/rpc/inbound_call.cc b/src/yb/rpc/inbound_call.cc
index ff750401ba..9baa4ac0c8 100644
--- a/src/yb/rpc/inbound_call.cc
+++ b/src/yb/rpc/inbound_call.cc
@@ -153,11 +153,13 @@ MonoDelta InboundCall::GetTimeInQueue() const {
   return timing_.time_handled.GetDeltaSince(timing_.time_received);
 }
 
-ThreadPoolTask* InboundCall::BindTask(InboundCallHandler* handler) {
+ThreadPoolTask* InboundCall::BindTask(InboundCallHandler* handler, int64_t rpc_queue_limit) {
   auto shared_this = shared_from(this);
-  if (!handler->CallQueued()) {
+  boost::optional<int64_t> rpc_queue_position = handler->CallQueued(rpc_queue_limit);
+  if (!rpc_queue_position) {
     return nullptr;
   }
+  rpc_queue_position_ = *rpc_queue_position;
   tracker_ = handler;
   task_.Bind(handler, shared_this);
   return &task_;
diff --git a/src/yb/rpc/inbound_call.h b/src/yb/rpc/inbound_call.h
index 1d762fb681..0d0739b33f 100644
--- a/src/yb/rpc/inbound_call.h
+++ b/src/yb/rpc/inbound_call.h
@@ -35,6 +35,7 @@
 #include <string>
 #include <vector>
 
+#include <boost/optional/optional.hpp>
 #include <glog/logging.h>
 
 #include "yb/gutil/stl_util.h"
@@ -85,7 +86,7 @@ class InboundCallHandler {
 
   virtual void Failure(const InboundCallPtr& call, const Status& status) = 0;
 
-  virtual bool CallQueued() = 0;
+  virtual boost::optional<int64_t> CallQueued(int64_t rpc_queue_limit) = 0;
 
   virtual void CallDequeued() = 0;
 
@@ -158,7 +159,9 @@ class InboundCall : public RpcCall, public MPSCQueueEntry<InboundCall> {
   // it gets handled.
   MonoDelta GetTimeInQueue() const;
 
-  ThreadPoolTask* BindTask(InboundCallHandler* handler);
+  virtual ThreadPoolTask* BindTask(InboundCallHandler* handler) {
+    return BindTask(handler, std::numeric_limits<int64_t>::max());
+  }
 
   void ResetCallProcessedListener() {
     call_processed_listener_ = decltype(call_processed_listener_)();
@@ -203,7 +206,11 @@ class InboundCall : public RpcCall, public MPSCQueueEntry<InboundCall> {
 
   const CallData& request_data() const { return request_data_; }
 
+  int64_t GetRpcQueuePosition() const { return rpc_queue_position_; }
+
  protected:
+  ThreadPoolTask* BindTask(InboundCallHandler* handler, int64_t rpc_queue_limit);
+
   void NotifyTransferred(const Status& status, Connection* conn) override;
 
   virtual void Clear();
@@ -265,6 +272,7 @@ class InboundCall : public RpcCall, public MPSCQueueEntry<InboundCall> {
   InboundCallHandler* tracker_ = nullptr;
 
   size_t method_index_ = 0;
+  int64_t rpc_queue_position_ = -1;
 
   DISALLOW_COPY_AND_ASSIGN(InboundCall);
 };
diff --git a/src/yb/rpc/service_pool.cc b/src/yb/rpc/service_pool.cc
index fefc018902..9bed9ea83a 100644
--- a/src/yb/rpc/service_pool.cc
+++ b/src/yb/rpc/service_pool.cc
@@ -42,6 +42,7 @@
 #include <vector>
 
 #include <boost/asio/strand.hpp>
+#include <boost/optional/optional.hpp>
 #include <cds/container/basket_queue.h>
 #include <cds/gc/dhp.h>
 #include <glog/logging.h>
@@ -386,19 +387,20 @@ class ServicePoolImpl final : public InboundCallHandler {
     return log_prefix_;
   }
 
-  bool CallQueued() override {
+  boost::optional<int64_t> CallQueued(int64_t rpc_queue_limit) override {
     auto queued_calls = queued_calls_.fetch_add(1, std::memory_order_acq_rel);
     if (queued_calls < 0) {
       YB_LOG_EVERY_N_SECS(DFATAL, 5) << "Negative number of queued calls: " << queued_calls;
     }
 
-    if (queued_calls >= max_queued_calls_) {
+    size_t max_queued_calls = std::min(max_queued_calls_, implicit_cast<size_t>(rpc_queue_limit));
+    if (implicit_cast<size_t>(queued_calls) >= max_queued_calls) {
       queued_calls_.fetch_sub(1, std::memory_order_relaxed);
-      return false;
+      return boost::none;
     }
 
     rpcs_in_queue_->Increment();
-    return true;
+    return queued_calls;
   }
 
   void CallDequeued() override {
diff --git a/src/yb/server/pgsql_webserver_wrapper.cc b/src/yb/server/pgsql_webserver_wrapper.cc
index dc4b4b2dc2..d0e12b7895 100644
--- a/src/yb/server/pgsql_webserver_wrapper.cc
+++ b/src/yb/server/pgsql_webserver_wrapper.cc
@@ -48,50 +48,37 @@ static const char *METRIC_TYPE_SERVER = "server";
 static const char *METRIC_ID_YB_YSQLSERVER = "yb.ysqlserver";
 
 static const char *PSQL_SERVER_CONNECTION_TOTAL = "yb_ysqlserver_connection_total";
-static const char *PSQL_SERVER_CONNECTION = "yb_ysqlserver_connection_info";
-
-static const char *CONN_BACKEND_TYPE = "backend_type";
-static const char *CONN_BACKEND_STATUS = "backend_status";
-static const char *CONN_APPLICATION_NAME = "application_name";
+static const char *PSQL_SERVER_ACTIVE_CONNECTION_TOTAL = "yb_ysqlserver_active_connection_total";
 
 namespace {
-// A helper function to init an empty AttributeMap and fills
-// it with proper default lables.
-MetricEntity::AttributeMap initEmptyAttributes() {
-  MetricEntity::AttributeMap connAttri;
-  connAttri[EXPORTED_INSTANCE] = prometheus_attr[EXPORTED_INSTANCE];
-  connAttri[METRIC_TYPE] = prometheus_attr[METRIC_TYPE];
-  connAttri[METRIC_ID] = prometheus_attr[METRIC_ID];
-  return connAttri;
-}
 
 void emitConnectionMetrics(PrometheusWriter *pwriter) {
   pgCallbacks.pullRpczEntries();
   rpczEntry *entry = *rpczResultPointer;
 
   uint64_t tot_connections = 0;
+  uint64_t tot_active_connections = 0;
   for (int i = 0; i < *num_backends; ++i, ++entry) {
     if (entry->proc_id > 0) {
-      auto connAttri = initEmptyAttributes();
-
-      connAttri[CONN_BACKEND_TYPE] = entry->backend_type;
-      connAttri[CONN_BACKEND_STATUS] = entry->backend_status;
-      connAttri[CONN_APPLICATION_NAME] = entry->application_name;
-
-      std::ostringstream errMsg;
-      errMsg << "Cannot publish connection metric to Promethesu-metrics endpoint for DB: "
-             << (entry->db_name ? entry->db_name : "Unknown DB");
-
-      WARN_NOT_OK(
-          pwriter->WriteSingleEntryNonTable(connAttri, PSQL_SERVER_CONNECTION, 1), errMsg.str());
+      if (entry->backend_active != 0u) {
+        tot_active_connections++;
+      }
       tot_connections++;
     }
   }
 
+  std::ostringstream errMsg;
+  errMsg << "Cannot publish connection metric to Promethesu-metrics endpoint";
+
+  WARN_NOT_OK(
+      pwriter->WriteSingleEntryNonTable(
+          prometheus_attr, PSQL_SERVER_ACTIVE_CONNECTION_TOTAL, tot_active_connections),
+      errMsg.str());
+
   WARN_NOT_OK(
       pwriter->WriteSingleEntryNonTable(
           prometheus_attr, PSQL_SERVER_CONNECTION_TOTAL, tot_connections),
-      "Cannot publish connection count metrics to Prometheus-metrics endpoint");
+      errMsg.str());
   pgCallbacks.freeRpczEntries();
 }
 
diff --git a/src/yb/tablet/tablet.cc b/src/yb/tablet/tablet.cc
index 569c02a2ef..8be8335f2b 100644
--- a/src/yb/tablet/tablet.cc
+++ b/src/yb/tablet/tablet.cc
@@ -1531,7 +1531,6 @@ void SetBackfillSpecForYsqlBackfill(
   out_spec.set_limit(limit);
   out_spec.set_count(in_spec.count() + row_count);
   response->set_is_backfill_batch_done(!response->has_paging_state());
-  VLOG(2) << " limit is " << limit << " set_count to " << out_spec.count();
   if (limit >= 0 && out_spec.count() >= limit) {
     // Hint postgres to stop scanning now. And set up the
     // next_row_key based on the paging state.
@@ -1973,9 +1972,11 @@ string GenerateSerializedBackfillSpec(size_t batch_size, const string& next_row_
   backfill_spec.set_limit(batch_size);
   backfill_spec.set_next_row_key(next_row_to_backfill);
   backfill_spec.SerializeToString(&serialized_backfill_spec);
-  VLOG(2) << "Generating backfill_spec " << yb::ToString(backfill_spec) << " encoded as "
-          << b2a_hex(serialized_backfill_spec) << " a string of length "
-          << serialized_backfill_spec.length();
+  VLOG(2) << "Generating backfill_spec " << yb::ToString(backfill_spec)
+          << (VLOG_IS_ON(3) ? Format(" encoded as $0 a string of length $1",
+                                     b2a_hex(serialized_backfill_spec),
+                                     serialized_backfill_spec.length())
+                            : "");
   return serialized_backfill_spec;
 }
 
@@ -2149,7 +2150,8 @@ Status Tablet::BackfillIndexesForYsql(
     *backfilled_until = spec.next_row_key();
 
     VLOG(2) << "Backfilled " << *number_of_rows_processed << " rows. "
-            << "Setting backfilled_until to " << b2a_hex(*backfilled_until) << " of length "
+            << "Setting backfilled_until to "
+            << (backfilled_until->empty() ? "(empty)" : b2a_hex(*backfilled_until)) << " of length "
             << backfilled_until->length();
 
     MaybeSleepToThrottleBackfill(backfill_params.start_time, *number_of_rows_processed);
diff --git a/src/yb/tablet/tablet_metadata.cc b/src/yb/tablet/tablet_metadata.cc
index 0ca292054e..b8a39e3fd5 100644
--- a/src/yb/tablet/tablet_metadata.cc
+++ b/src/yb/tablet/tablet_metadata.cc
@@ -1177,6 +1177,10 @@ const std::string& RaftGroupMetadata::indexed_table_id(const TableId& table_id)
   return index_info ? index_info->indexed_table_id() : kEmptyString;
 }
 
+bool RaftGroupMetadata::is_index(const TableId& table_id) const {
+  return !indexed_table_id(table_id).empty();
+}
+
 bool RaftGroupMetadata::is_local_index(const TableId& table_id) const {
   DCHECK_NE(state_, kNotLoadedYet);
   std::lock_guard<MutexType> lock(data_mutex_);
diff --git a/src/yb/tablet/tablet_metadata.h b/src/yb/tablet/tablet_metadata.h
index 0e10caea90..953748c0e5 100644
--- a/src/yb/tablet/tablet_metadata.h
+++ b/src/yb/tablet/tablet_metadata.h
@@ -238,6 +238,8 @@ class RaftGroupMetadata : public RefCountedThreadSafe<RaftGroupMetadata> {
 
   const std::string& indexed_table_id(const TableId& table_id = "") const;
 
+  bool is_index(const TableId& table_id = "") const;
+
   bool is_local_index(const TableId& table_id = "") const;
 
   bool is_unique_index(const TableId& table_id = "") const;
@@ -273,8 +275,6 @@ class RaftGroupMetadata : public RefCountedThreadSafe<RaftGroupMetadata> {
 
   Status set_cdc_sdk_min_checkpoint_op_id(const OpId& cdc_min_checkpoint_op_id);
 
-  Status set_cdc_sdk_min_checkpoint_op_id(const OpId& cdc_min_checkpoint_op_id);
-
   int64_t cdc_min_replicated_index() const;
 
   OpId cdc_sdk_min_checkpoint_op_id() const;
diff --git a/src/yb/tablet/tablet_peer.cc b/src/yb/tablet/tablet_peer.cc
index 304fd4e194..d49fec88d5 100644
--- a/src/yb/tablet/tablet_peer.cc
+++ b/src/yb/tablet/tablet_peer.cc
@@ -841,6 +841,12 @@ void TabletPeer::GetInFlightOperations(Operation::TraceType trace_type,
 }
 
 Result<int64_t> TabletPeer::GetEarliestNeededLogIndex(std::string* details) const {
+  if (PREDICT_FALSE(!log_)) {
+    auto status = STATUS(Uninitialized, "Log not ready (tablet peer not yet initialized?)");
+    LOG(DFATAL) << status;
+    return status;
+  }
+
   // First, we anchor on the last OpId in the Log to establish a lower bound
   // and avoid racing with the other checks. This limits the Log GC candidate
   // segments before we check the anchors.
diff --git a/src/yb/tablet/tablet_peer.h b/src/yb/tablet/tablet_peer.h
index 4f3c7f57d8..b24acdb711 100644
--- a/src/yb/tablet/tablet_peer.h
+++ b/src/yb/tablet/tablet_peer.h
@@ -384,10 +384,6 @@ class TabletPeer : public consensus::ConsensusContext,
 
   OpId cdc_sdk_min_checkpoint_op_id();
 
-  Status set_cdc_sdk_min_checkpoint_op_id(const OpId& cdc_sdk_min_checkpoint_op_id);
-
-  OpId cdc_sdk_min_checkpoint_op_id();
-
   Status SetCDCSDKRetainOpIdAndTime(
       const OpId& cdc_sdk_op_id, const MonoDelta& cdc_sdk_op_id_expiration);
 
diff --git a/src/yb/tools/yb-admin_client.cc b/src/yb/tools/yb-admin_client.cc
index 65e7104d6d..da881e1e35 100644
--- a/src/yb/tools/yb-admin_client.cc
+++ b/src/yb/tools/yb-admin_client.cc
@@ -1223,6 +1223,9 @@ Status ClusterAdminClient::ListTables(bool include_db_type,
         case master::INDEX_TABLE_RELATION:
           str << " index";
           break;
+        case master::MATVIEW_TABLE_RELATION:
+          str << "matview";
+          break;
         default:
           str << " other";
       }
diff --git a/src/yb/tserver/pg_client.proto b/src/yb/tserver/pg_client.proto
index 401a037619..6c1e7174a1 100644
--- a/src/yb/tserver/pg_client.proto
+++ b/src/yb/tserver/pg_client.proto
@@ -174,6 +174,7 @@ message PgCreateTableRequestPB {
   bool is_unique_index = 16;
   bool skip_index_backfill = 17;
 
+  bool is_matview = 21;
   PgObjectIdPB matview_pg_table_oid = 18;
   string schema_name = 19;
 }
diff --git a/src/yb/tserver/pg_create_table.cc b/src/yb/tserver/pg_create_table.cc
index f8237518e7..61680b01b0 100644
--- a/src/yb/tserver/pg_create_table.cc
+++ b/src/yb/tserver/pg_create_table.cc
@@ -96,7 +96,8 @@ Status PgCreateTable::Exec(
   table_creator->table_name(table_name_).table_type(client::YBTableType::PGSQL_TABLE_TYPE)
                 .table_id(PgObjectId::GetYbTableIdFromPB(req_.table_id()))
                 .schema(&schema)
-                .colocated(req_.colocated());
+                .colocated(req_.colocated())
+                .is_matview(req_.is_matview());
   if (req_.is_pg_catalog_table()) {
     table_creator->is_pg_catalog_table();
   }
diff --git a/src/yb/tserver/tserver-path-handlers.cc b/src/yb/tserver/tserver-path-handlers.cc
index 9cc2487174..babec4f228 100644
--- a/src/yb/tserver/tserver-path-handlers.cc
+++ b/src/yb/tserver/tserver-path-handlers.cc
@@ -183,11 +183,15 @@ bool GetTabletPeer(TabletServer* tserver, const Webserver::WebRequest& req,
   return true;
 }
 
-bool TabletBootstrapping(const std::shared_ptr<TabletPeer>& peer, const string& tablet_id,
+bool TabletReady(const std::shared_ptr<TabletPeer>& peer, const string& tablet_id,
                          std::stringstream* out) {
-  if (peer->state() == tablet::BOOTSTRAPPING) {
+  auto state = peer->state();
+  if (state == tablet::BOOTSTRAPPING) {
     (*out) << "Tablet " << EscapeForHtmlToString(tablet_id) << " is still bootstrapping";
     return false;
+  } else if (state == tablet::NOT_STARTED) {
+    (*out) << "Tablet " << EscapeForHtmlToString(tablet_id) << " has not yet started";
+    return false;
   }
   return true;
 }
@@ -200,7 +204,7 @@ bool LoadTablet(TabletServer* tserver,
                 std::stringstream* out) {
   if (!GetTabletID(req, tablet_id, out)) return false;
   if (!GetTabletPeer(tserver, req, peer, *tablet_id, out)) return false;
-  if (!TabletBootstrapping(*peer, *tablet_id, out)) return false;
+  if (!TabletReady(*peer, *tablet_id, out)) return false;
   return true;
 }
 
diff --git a/src/yb/yql/cql/cqlserver/cql_processor.cc b/src/yb/yql/cql/cqlserver/cql_processor.cc
index f04a891e50..3e0f99fc91 100644
--- a/src/yb/yql/cql/cqlserver/cql_processor.cc
+++ b/src/yb/yql/cql/cqlserver/cql_processor.cc
@@ -268,6 +268,7 @@ void CQLProcessor::PrepareAndSendResponse(const unique_ptr<CQLResponse>& respons
     const CQLConnectionContext& context =
         static_cast<const CQLConnectionContext&>(call_->connection()->context());
     response->set_registered_events(context.registered_events());
+    response->set_rpc_queue_position(call_->GetRpcQueuePosition());
     SendResponse(*response);
   }
 }
diff --git a/src/yb/yql/cql/cqlserver/cql_rpc.cc b/src/yb/yql/cql/cqlserver/cql_rpc.cc
index 7e789b917d..2ebf0edafe 100644
--- a/src/yb/yql/cql/cqlserver/cql_rpc.cc
+++ b/src/yb/yql/cql/cqlserver/cql_rpc.cc
@@ -359,5 +359,10 @@ CoarseTimePoint CQLInboundCall::GetClientDeadline() const {
   return deadline_;
 }
 
+rpc::ThreadPoolTask* CQLInboundCall::BindTask(rpc::InboundCallHandler* handler) {
+  int64_t rpc_queue_limit = CQLRequest::ParseRpcQueueLimit(serialized_request_);
+  return rpc::InboundCall::BindTask(handler, rpc_queue_limit);
+}
+
 } // namespace cqlserver
 } // namespace yb
diff --git a/src/yb/yql/cql/cqlserver/cql_rpc.h b/src/yb/yql/cql/cqlserver/cql_rpc.h
index ab3d997529..665fbf039b 100644
--- a/src/yb/yql/cql/cqlserver/cql_rpc.h
+++ b/src/yb/yql/cql/cqlserver/cql_rpc.h
@@ -160,6 +160,8 @@ class CQLInboundCall : public rpc::InboundCall {
     return DynamicMemoryUsageOf(response_msg_buf_);
   }
 
+  rpc::ThreadPoolTask* BindTask(rpc::InboundCallHandler* handler) override;
+
  private:
   RefCntBuffer response_msg_buf_;
   const ql::QLSession::SharedPtr ql_session_;
diff --git a/src/yb/yql/cql/ql/exec/eval_where.cc b/src/yb/yql/cql/ql/exec/eval_where.cc
index 592d6f836e..b3b89c366d 100644
--- a/src/yb/yql/cql/ql/exec/eval_where.cc
+++ b/src/yb/yql/cql/ql/exec/eval_where.cc
@@ -247,8 +247,9 @@ Result<uint64_t> Executor::WhereClauseToPB(QLReadRequestPB *req,
       if (col_op.desc()->is_primary()) {
         if (cond->op() == QL_OP_IN) {
           int in_size = cond->operands(1).value().list_value().elems_size();
-          if (in_size == 0 || // Can happen when binding an empty list as 'IN' argument.
-              max_rows_estimate <= std::numeric_limits<uint64_t>::max() / in_size) {
+          if (in_size == 0) {  // Fast path for returning no results when 'IN' list is empty.
+            return 0;
+          } else if (max_rows_estimate <= std::numeric_limits<uint64_t>::max() / in_size) {
             max_rows_estimate *= in_size;
           } else {
             max_rows_estimate = std::numeric_limits<uint64_t>::max();
@@ -273,7 +274,7 @@ Result<uint64_t> Executor::WhereClauseToPB(QLReadRequestPB *req,
     }
   }
 
-  // If not all primary keys have '=' or 'IN' conditions the max rows estimate is not reliable.
+  // If not all primary keys have '=' or 'IN' conditions, the max rows estimate is not reliable.
   if (!static_cast<const PTSelectStmt*>(tnode_context->tnode())->HasPrimaryKeysSet()) {
     return std::numeric_limits<uint64_t>::max();
   }
diff --git a/src/yb/yql/cql/ql/util/cql_message.cc b/src/yb/yql/cql/ql/util/cql_message.cc
index 7d5c2a13f0..7b48c83a58 100644
--- a/src/yb/yql/cql/ql/util/cql_message.cc
+++ b/src/yb/yql/cql/ql/util/cql_message.cc
@@ -247,6 +247,19 @@ bool CQLRequest::ParseRequest(
   const uint8_t* body_data = body_size > 0 ? mesg.data() + kMessageHeaderLength : to_uchar_ptr("");
   unique_ptr<uint8_t[]> buffer;
 
+  if (header.flags & kMetadataFlag) {
+    if (body_size < kMetadataSize) {
+      error_response->reset(
+          new ErrorResponse(
+              header.stream_id, ErrorResponse::Code::PROTOCOL_ERROR,
+              "Metadata flag set, but request body too small"));
+      return false;
+    }
+    // Ignore the request metadata.
+    body_size -= kMetadataSize;
+    body_data += kMetadataSize;
+  }
+
   // If the message body is compressed, uncompress it.
   if (body_size > 0 && (header.flags & kCompressionFlag)) {
     if (header.opcode == Opcode::STARTUP) {
@@ -388,6 +401,16 @@ bool CQLRequest::ParseRequest(
   return true;
 }
 
+int64_t CQLRequest::ParseRpcQueueLimit(const Slice& mesg) {
+  Flags flags = LoadByte<Flags>(mesg, kHeaderPosFlags);
+  if (!(flags & kMetadataFlag)) {
+    return std::numeric_limits<int64_t>::max();
+  }
+  uint16_t queue_limit = LoadShort<uint16_t>(
+      mesg, kMessageHeaderLength + kMetadataQueueLimitOffset);
+  return static_cast<int64_t>(queue_limit);
+}
+
 CQLRequest::CQLRequest(const Header& header, const Slice& body) : CQLMessage(header), body_(body) {
 }
 
@@ -966,7 +989,10 @@ void SerializeValue(const CQLMessage::Value& value, faststring* mesg) {
 
 // ------------------------------------ CQL response -----------------------------------
 CQLResponse::CQLResponse(const CQLRequest& request, const Opcode opcode)
-    : CQLMessage(Header(request.version() | kResponseVersion, 0, request.stream_id(), opcode)) {
+    : CQLMessage(Header(request.version() | kResponseVersion,
+                        request.flags() & kMetadataFlag,
+                        request.stream_id(),
+                        opcode)) {
 }
 
 CQLResponse::CQLResponse(const StreamId stream_id, const Opcode opcode)
@@ -990,6 +1016,11 @@ void CQLResponse::Serialize(const CompressionScheme compression_scheme, faststri
   const size_t start_pos = mesg->size(); // save the start position
   const bool compress = (compression_scheme != CQLMessage::CompressionScheme::kNone);
   SerializeHeader(compress, mesg);
+  if (flags() & kMetadataFlag) {
+    uint8_t buffer[kMetadataSize] = {0};
+    SERIALIZE_SHORT(buffer, kMetadataQueuePosOffset, static_cast<uint16_t>(rpc_queue_position_));
+    mesg->append(buffer, sizeof(buffer));
+  }
   if (compress) {
     faststring body;
     SerializeBody(&body);
diff --git a/src/yb/yql/cql/ql/util/cql_message.h b/src/yb/yql/cql/ql/util/cql_message.h
index c1da5bfa6a..b306a0d920 100644
--- a/src/yb/yql/cql/ql/util/cql_message.h
+++ b/src/yb/yql/cql/ql/util/cql_message.h
@@ -43,6 +43,7 @@
 
 #include "yb/gutil/callback.h"
 #include "yb/gutil/callback_internal.h"
+#include "yb/gutil/casts.h"
 #include "yb/gutil/template_util.h"
 
 #include "yb/rpc/server_event.h"
@@ -96,6 +97,8 @@ class CQLMessage {
   static constexpr Flags kTracingFlag       = 0x02;
   static constexpr Flags kCustomPayloadFlag = 0x04; // Since V4
   static constexpr Flags kWarningFlag       = 0x08; // Since V4
+  // Not specified by CQL protocol - YB custom flag
+  static constexpr Flags kMetadataFlag      = 0x80;
 
   using StreamId = uint16_t;
   static constexpr StreamId kEventStreamId = 0xffff; // Special stream id for events.
@@ -131,6 +134,32 @@ class CQLMessage {
        : version(version), flags(0), stream_id(stream_id), opcode(opcode) { }
   };
 
+  // The CQL metadata consists of the first 8 bytes of the request body.
+  // The format of the request metadata is
+  //   0        16        32        48        64
+  //   +---------+---------+---------+---------+
+  //   |           unused            | Q limit |
+  //   +---------+---------+---------+---------+
+  // The format of the response metadata is
+  //   0        16        32        48        64
+  //   +---------+---------+---------+---------+
+  //   |           unused            |  Q pos  |
+  //   +---------+---------+---------+---------+
+  //
+  // The RPC queue limit ("Q limit") is a unsigned 16-bit integer indicating
+  // whether the request should be dropped. If the queue position of the
+  // incoming RPC is greater than the queue limit, then the request gets
+  // dropped immediately and an ERROR_SERVER_TOO_BUSY response is sent back
+  // to the client. If the queue limit is 0, then the request is not dropped
+  // based on the queue position.
+  //
+  // The RPC queue position ("Q pos") is a signed 16-bit integer indicating
+  // the queue position of the corresponding inbound RPC. If for some reason
+  // the queue position could not be determined, then a value of -1 is returned.
+  static constexpr size_t kMetadataSize = 8;
+  static constexpr size_t kMetadataQueueLimitOffset = 6;
+  static constexpr size_t kMetadataQueuePosOffset = 6;
+
   // STARTUP options.
   static constexpr char kCQLVersionOption[] = "CQL_VERSION";
   static constexpr char kCompressionOption[] = "COMPRESSION";
@@ -286,6 +315,8 @@ class CQLRequest : public CQLMessage {
     return body_.size();
   }
 
+  static int64_t ParseRpcQueueLimit(const Slice& mesg);
+
   virtual ~CQLRequest();
 
  protected:
@@ -498,6 +529,10 @@ class CQLResponse : public CQLMessage {
   Events registered_events() const { return registered_events_; }
   void set_registered_events(Events events) { registered_events_ = events; }
 
+  void set_rpc_queue_position(int64_t rpc_queue_position) {
+    rpc_queue_position_ = trim_cast<int16_t>(rpc_queue_position);
+  }
+
  protected:
   CQLResponse(const CQLRequest& request, Opcode opcode);
   CQLResponse(StreamId stream_id, Opcode opcode);
@@ -508,6 +543,7 @@ class CQLResponse : public CQLMessage {
 
  private:
   Events registered_events_ = kNoEvents;
+  int16_t rpc_queue_position_ = -1;
 };
 
 // ------------------------------ Individual CQL responses -----------------------------------
diff --git a/src/yb/yql/pggate/pg_ddl.cc b/src/yb/yql/pggate/pg_ddl.cc
index 7a87ca98ad..e997c74eea 100644
--- a/src/yb/yql/pggate/pg_ddl.cc
+++ b/src/yb/yql/pggate/pg_ddl.cc
@@ -177,6 +177,7 @@ PgCreateTable::PgCreateTable(PgSession::ScopedRefPtr pg_session,
                              const PgObjectId& tablegroup_oid,
                              const ColocationId colocation_id,
                              const PgObjectId& tablespace_oid,
+                             bool is_matview,
                              const PgObjectId& matview_pg_table_oid)
     : PgDdl(pg_session) {
   table_id.ToPB(req_.mutable_table_id());
@@ -194,6 +195,7 @@ PgCreateTable::PgCreateTable(PgSession::ScopedRefPtr pg_session,
     req_.set_colocation_id(colocation_id);
   }
   tablespace_oid.ToPB(req_.mutable_tablespace_oid());
+  req_.set_is_matview(is_matview);
   matview_pg_table_oid.ToPB(req_.mutable_matview_pg_table_oid());
 
   // Add internal primary key column to a Postgres table without a user-specified primary key.
diff --git a/src/yb/yql/pggate/pg_ddl.h b/src/yb/yql/pggate/pg_ddl.h
index 3ae0772bd7..4ed838fa0b 100644
--- a/src/yb/yql/pggate/pg_ddl.h
+++ b/src/yb/yql/pggate/pg_ddl.h
@@ -151,6 +151,7 @@ class PgCreateTable : public PgDdl {
                 const PgObjectId& tablegroup_oid,
                 const ColocationId colocation_id,
                 const PgObjectId& tablespace_oid,
+                bool is_matview,
                 const PgObjectId& matview_pg_table_oid);
 
   void SetupIndex(
diff --git a/src/yb/yql/pggate/pg_doc_op.cc b/src/yb/yql/pggate/pg_doc_op.cc
index 6e88f2c24b..558981065e 100644
--- a/src/yb/yql/pggate/pg_doc_op.cc
+++ b/src/yb/yql/pggate/pg_doc_op.cc
@@ -404,9 +404,9 @@ Status PgDocReadOp::CreateRequests() {
     return Status::OK();
   }
 
-  // All information from the SQL request has been collected and setup. This code populate
+  // All information from the SQL request has been collected and setup. This code populates
   // Protobuf requests before sending them to DocDB. For performance reasons, requests are
-  // constructed differently for different statement.
+  // constructed differently for different statements.
   if (template_op_->request().has_sampling_state()) {
     VLOG(1) << __PRETTY_FUNCTION__ << ": Preparing sampling requests ";
     return PopulateSamplingOps();
diff --git a/src/yb/yql/pggate/pg_session.cc b/src/yb/yql/pggate/pg_session.cc
index 5d4138beda..d89018a969 100644
--- a/src/yb/yql/pggate/pg_session.cc
+++ b/src/yb/yql/pggate/pg_session.cc
@@ -424,18 +424,12 @@ Status PgSession::RunHelper::Apply(std::shared_ptr<client::YBPgsqlOp> op,
     }
   }
 
-  TxnPriorityRequirement txn_priority_requirement = kLowerPriorityRange;
-  if (op->type() == YBOperation::Type::PGSQL_READ) {
-    const auto row_mark_type = GetRowMarkType(*op);
-    read_only = read_only && !IsValidRowMarkType(row_mark_type);
-    if (RowMarkNeedsHigherPriority(row_mark_type)) {
-      txn_priority_requirement = kHigherPriorityRange;
-    }
-  }
-
-  if (pg_session_.GetIsolationLevel() == PgIsolationLevel::READ_COMMITTED) {
-    txn_priority_requirement = kHighestPriority;
-  }
+  const auto row_mark_type = GetRowMarkType(*op);
+  const auto txn_priority_requirement =
+    pg_session_.GetIsolationLevel() == PgIsolationLevel::READ_COMMITTED ||
+    RowMarkNeedsHigherPriority(row_mark_type)
+        ? kHigherPriorityRange : kLowerPriorityRange;
+  read_only = read_only && !IsValidRowMarkType(row_mark_type);
 
   auto session = VERIFY_RESULT(pg_session_.GetSession(
       transactional_,
diff --git a/src/yb/yql/pggate/pggate.cc b/src/yb/yql/pggate/pggate.cc
index c52fb092b8..fd16509cfc 100644
--- a/src/yb/yql/pggate/pggate.cc
+++ b/src/yb/yql/pggate/pggate.cc
@@ -621,12 +621,13 @@ Status PgApiImpl::NewCreateTable(const char *database_name,
                                  const PgObjectId& tablegroup_oid,
                                  const ColocationId colocation_id,
                                  const PgObjectId& tablespace_oid,
+                                 bool is_matview,
                                  const PgObjectId& matview_pg_table_oid,
                                  PgStatement **handle) {
   auto stmt = std::make_unique<PgCreateTable>(
       pg_session_, database_name, schema_name, table_name,
       table_id, is_shared_table, if_not_exist, add_primary_key, colocated, tablegroup_oid,
-      colocation_id, tablespace_oid, matview_pg_table_oid);
+      colocation_id, tablespace_oid, is_matview, matview_pg_table_oid);
   if (pg_txn_manager_->IsDdlMode()) {
     stmt->UseTransaction(VERIFY_RESULT(Copy(pg_txn_manager_->GetDdlTxnMetadata().get())));
   }
@@ -856,7 +857,7 @@ Status PgApiImpl::NewCreateIndex(const char *database_name,
       pg_session_, database_name, schema_name, index_name, index_id, is_shared_index,
       if_not_exist, false /* add_primary_key */,
       tablegroup_oid.IsValid() ? false : true /* colocated */, tablegroup_oid, colocation_id,
-      tablespace_oid, PgObjectId() /* matview_pg_table_id */);
+      tablespace_oid, false /* is_matview */, PgObjectId() /* matview_pg_table_id */);
   stmt->SetupIndex(base_table_id, is_unique_index, skip_index_backfill);
   if (pg_txn_manager_->IsDdlMode()) {
     stmt->UseTransaction(VERIFY_RESULT(Copy(pg_txn_manager_->GetDdlTxnMetadata().get())));
diff --git a/src/yb/yql/pggate/pggate.h b/src/yb/yql/pggate/pggate.h
index 6c02b58086..7044ea3597 100644
--- a/src/yb/yql/pggate/pggate.h
+++ b/src/yb/yql/pggate/pggate.h
@@ -246,6 +246,7 @@ class PgApiImpl {
                                 const PgObjectId& tablegroup_oid,
                                 const ColocationId colocation_id,
                                 const PgObjectId& tablespace_oid,
+                                bool is_matview,
                                 const PgObjectId& matview_pg_table_oid,
                                 PgStatement **handle);
 
diff --git a/src/yb/yql/pggate/test/pggate_test_catalog.cc b/src/yb/yql/pggate/test/pggate_test_catalog.cc
index c35fc33182..46667bc572 100644
--- a/src/yb/yql/pggate/test/pggate_test_catalog.cc
+++ b/src/yb/yql/pggate/test/pggate_test_catalog.cc
@@ -47,6 +47,7 @@ TEST_F(PggateTestCatalog, TestDml) {
                                        kInvalidOid /* tablegroup_id */,
                                        kColocationIdNotSet /* colocation_id */,
                                        kInvalidOid /* tablespace_id */,
+                                       false /* is_matview */,
                                        kInvalidOid /* matview_pg_table_id */,
                                        &pg_stmt));
   CHECK_YBC_STATUS(YBCTestCreateTableAddColumn(pg_stmt, "company_id", ++col_count,
@@ -396,6 +397,7 @@ TEST_F(PggateTestCatalog, TestCopydb) {
                                        kInvalidOid /* tablegroup_id */,
                                        kColocationIdNotSet /* colocation_id */,
                                        kInvalidOid /* tablespace_id */,
+                                       false /* is_matview */,
                                        kInvalidOid /* matview_pg_table_id */,
                                        &pg_stmt));
   CHECK_YBC_STATUS(YBCTestCreateTableAddColumn(pg_stmt, "key", 1, DataType::INT32, false, true));
diff --git a/src/yb/yql/pggate/test/pggate_test_delete.cc b/src/yb/yql/pggate/test/pggate_test_delete.cc
index 3219a14eee..b653ac07e1 100644
--- a/src/yb/yql/pggate/test/pggate_test_delete.cc
+++ b/src/yb/yql/pggate/test/pggate_test_delete.cc
@@ -43,6 +43,7 @@ TEST_F(PggateTestDelete, TestDelete) {
                                        kInvalidOid /* tablegroup_id */,
                                        kColocationIdNotSet /* colocation_id */,
                                        kInvalidOid /* tablespace_id */,
+                                       false /* is_matview */,
                                        kInvalidOid /* matview_pg_table_id */,
                                        &pg_stmt));
   CHECK_YBC_STATUS(YBCTestCreateTableAddColumn(pg_stmt, "hash_key", ++col_count,
diff --git a/src/yb/yql/pggate/test/pggate_test_select.cc b/src/yb/yql/pggate/test/pggate_test_select.cc
index fcc8d05f85..cdc6ef94e3 100644
--- a/src/yb/yql/pggate/test/pggate_test_select.cc
+++ b/src/yb/yql/pggate/test/pggate_test_select.cc
@@ -43,6 +43,7 @@ TEST_F(PggateTestSelect, TestSelectOneTablet) {
                                        kInvalidOid /* tablegroup_id */,
                                        kColocationIdNotSet /* colocation_id */,
                                        kInvalidOid /* tablespace_id */,
+                                       false /* is_matview */,
                                        kInvalidOid /* matview_pg_table_id */,
                                        &pg_stmt));
   CHECK_YBC_STATUS(YBCTestCreateTableAddColumn(pg_stmt, "hash_key", ++col_count,
diff --git a/src/yb/yql/pggate/test/pggate_test_select_inequality.cc b/src/yb/yql/pggate/test/pggate_test_select_inequality.cc
index 5b58b9e06b..1c5729c71f 100644
--- a/src/yb/yql/pggate/test/pggate_test_select_inequality.cc
+++ b/src/yb/yql/pggate/test/pggate_test_select_inequality.cc
@@ -43,6 +43,7 @@ TEST_F(PggateTestSelectInequality, TestSelectInequality) {
                                        kInvalidOid /* tablegroup_id */,
                                        kColocationIdNotSet /* colocation_id */,
                                        kInvalidOid /* tablespace_id */,
+                                       false /* is_matview */,
                                        kInvalidOid /* matview_pg_table_id */,
                                        &pg_stmt));
   CHECK_YBC_STATUS(YBCTestCreateTableAddColumn(pg_stmt, "h", ++col_count,
diff --git a/src/yb/yql/pggate/test/pggate_test_select_multi_tablets.cc b/src/yb/yql/pggate/test/pggate_test_select_multi_tablets.cc
index 38073a466f..edd9bf21c4 100644
--- a/src/yb/yql/pggate/test/pggate_test_select_multi_tablets.cc
+++ b/src/yb/yql/pggate/test/pggate_test_select_multi_tablets.cc
@@ -43,6 +43,7 @@ TEST_F(PggateTestSelectMultiTablets, TestSelectMultiTablets) {
                                        kInvalidOid /* tablegroup_id */,
                                        kColocationIdNotSet /* colocation_id */,
                                        kInvalidOid /* tablespace_id */,
+                                       false /* is_matview */,
                                        kInvalidOid /* matview_pg_table_id */,
                                        &pg_stmt));
   CHECK_YBC_STATUS(YBCTestCreateTableAddColumn(pg_stmt, "hash_key", ++col_count,
diff --git a/src/yb/yql/pggate/test/pggate_test_update.cc b/src/yb/yql/pggate/test/pggate_test_update.cc
index 76db1cf9d6..b947aec298 100644
--- a/src/yb/yql/pggate/test/pggate_test_update.cc
+++ b/src/yb/yql/pggate/test/pggate_test_update.cc
@@ -43,6 +43,7 @@ TEST_F(PggateTestDelete, TestDelete) {
                                        kInvalidOid /* tablegroup_id */,
                                        kColocationIdNotSet /* colocation_id */,
                                        kInvalidOid /* tablespace_id */,
+                                       false /* is_matview */,
                                        kInvalidOid /* matview_pg_table_id */,
                                        &pg_stmt));
   CHECK_YBC_STATUS(YBCTestCreateTableAddColumn(pg_stmt, "hash_key", ++col_count,
diff --git a/src/yb/yql/pggate/ybc_pggate.cc b/src/yb/yql/pggate/ybc_pggate.cc
index 709f49bb73..3d1d2ef713 100644
--- a/src/yb/yql/pggate/ybc_pggate.cc
+++ b/src/yb/yql/pggate/ybc_pggate.cc
@@ -366,6 +366,7 @@ YBCStatus YBCPgNewCreateTable(const char *database_name,
                               const YBCPgOid tablegroup_oid,
                               const YBCPgOid colocation_id,
                               const YBCPgOid tablespace_oid,
+                              bool is_matview,
                               const YBCPgOid matview_pg_table_oid,
                               YBCPgStatement *handle) {
   const PgObjectId table_id(database_oid, table_oid);
@@ -375,7 +376,7 @@ YBCStatus YBCPgNewCreateTable(const char *database_name,
   return ToYBCStatus(pgapi->NewCreateTable(
       database_name, schema_name, table_name, table_id, is_shared_table,
       if_not_exist, add_primary_key, colocated, tablegroup_id, colocation_id, tablespace_id,
-      matview_pg_table_id, handle));
+      is_matview, matview_pg_table_id, handle));
 }
 
 YBCStatus YBCPgCreateTableAddColumn(YBCPgStatement handle, const char *attr_name, int attr_num,
diff --git a/src/yb/yql/pggate/ybc_pggate.h b/src/yb/yql/pggate/ybc_pggate.h
index b5099b407b..09e4768dce 100644
--- a/src/yb/yql/pggate/ybc_pggate.h
+++ b/src/yb/yql/pggate/ybc_pggate.h
@@ -179,6 +179,7 @@ YBCStatus YBCPgNewCreateTable(const char *database_name,
                               YBCPgOid tablegroup_oid,
                               YBCPgOid colocation_id,
                               YBCPgOid tablespace_oid,
+                              bool is_matview,
                               YBCPgOid matview_pg_table_oid,
                               YBCPgStatement *handle);
 
diff --git a/src/yb/yql/pgwrapper/geo_transactions-test.cc b/src/yb/yql/pgwrapper/geo_transactions-test.cc
index 01dad8ef9c..d631bb3ce6 100644
--- a/src/yb/yql/pgwrapper/geo_transactions-test.cc
+++ b/src/yb/yql/pgwrapper/geo_transactions-test.cc
@@ -55,7 +55,7 @@ YB_STRONGLY_TYPED_BOOL(SetGlobalTransactionSessionVar);
 constexpr auto kDatabaseName = "yugabyte";
 constexpr auto kTablePrefix = "test";
 const auto kStatusTabletCacheRefreshTimeout = MonoDelta::FromMilliseconds(10000);
-const auto kWaitLoadBalancerTimeout = MonoDelta::FromMilliseconds(30000);
+const auto kWaitLoadBalancerTimeout = MonoDelta::FromMilliseconds(60000);
 
 } // namespace
 
diff --git a/src/yb/yql/pgwrapper/pg_tablet_split-test.cc b/src/yb/yql/pgwrapper/pg_tablet_split-test.cc
index 8391a67ee6..5508cbf7a6 100644
--- a/src/yb/yql/pgwrapper/pg_tablet_split-test.cc
+++ b/src/yb/yql/pgwrapper/pg_tablet_split-test.cc
@@ -11,11 +11,17 @@
 // under the License.
 //
 
+#include "yb/common/schema.h"
+#include "yb/common/wire_protocol.h"
+
 #include "yb/gutil/dynamic_annotations.h"
 
+#include "yb/master/catalog_manager.h"
 #include "yb/master/catalog_manager_if.h"
+#include "yb/master/master_admin.pb.h"
 #include "yb/master/mini_master.h"
 
+#include "yb/tablet/tablet.h"
 #include "yb/tablet/tablet_peer.h"
 
 #include "yb/util/monotime.h"
@@ -37,7 +43,7 @@ namespace pgwrapper {
 class PgTabletSplitTest : public PgMiniTestBase {
 
  protected:
-  CHECKED_STATUS SplitSingleTablet(const TableId& table_id) {
+  Status SplitSingleTablet(const TableId& table_id) {
     auto master = VERIFY_RESULT(cluster_->GetLeaderMiniMaster());
     auto tablets = ListTableActiveTabletLeadersPeers(cluster_.get(), table_id);
     if (tablets.size() != 1) {
@@ -45,7 +51,20 @@ class PgTabletSplitTest : public PgMiniTestBase {
     }
     auto tablet_id = tablets.at(0)->tablet_id();
 
-    return master->catalog_manager().SplitTablet(tablet_id, true);
+    return master->catalog_manager().SplitTablet(tablet_id, master::ManualSplit::kTrue);
+  }
+
+  Status InvokeSplitTabletRpc(const std::string& tablet_id) {
+    master::SplitTabletRequestPB req;
+    req.set_tablet_id(tablet_id);
+    master::SplitTabletResponsePB resp;
+
+    auto master = VERIFY_RESULT(cluster_->GetLeaderMiniMaster());
+    RETURN_NOT_OK(master->catalog_manager_impl().SplitTablet(&req, &resp, nullptr));
+    if (resp.has_error()) {
+      RETURN_NOT_OK(StatusFromPB(resp.error().status()));
+    }
+    return Status::OK();
   }
 
  private:
@@ -92,5 +111,91 @@ TEST_F(PgTabletSplitTest, YB_DISABLE_TEST_IN_TSAN(SplitDuringLongRunningTransact
   ASSERT_OK(conn.CommitTransaction());
 }
 
+// TODO (tsplit): a test for automatic splitting of index table will be added in context of #12189;
+// as of now, it is ok to keep only one test as manual and automatic splitting use the same
+// execution path in context of table/tablet validation.
+TEST_F(PgTabletSplitTest, YB_DISABLE_TEST_IN_TSAN(ManualSplitIndexTablet)) {
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_enable_automatic_tablet_splitting) = false;
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_cleanup_split_tablets_interval_sec) = 1;
+  constexpr auto kNumRows = 1000;
+  constexpr auto kTableName = "t1";
+  constexpr auto kIdx1Name = "idx1";
+  constexpr auto kIdx2Name = "idx2";
+
+  auto conn = ASSERT_RESULT(Connect());
+  auto client = ASSERT_RESULT(cluster_->CreateClient());
+
+  ASSERT_OK(conn.Execute(Format("CREATE TABLE $0(k INT PRIMARY KEY, v TEXT);", kTableName)));
+  ASSERT_OK(conn.Execute(Format("CREATE INDEX $0 on $1(v ASC);", kIdx1Name, kTableName)));
+  ASSERT_OK(conn.Execute(Format("CREATE INDEX $0 on $1(v HASH);", kIdx2Name, kTableName)));
+
+  ASSERT_OK(conn.Execute(Format(
+      "INSERT INTO $0 SELECT i, i::text FROM (SELECT generate_series(1, $1) i) t2;",
+      kTableName, kNumRows)));
+
+  ASSERT_OK(cluster_->FlushTablets());
+
+  auto check_rows_count = [&conn](const std::string& table_name, size_t count) -> Status {
+    auto res = VERIFY_RESULT(conn.Fetch(Format("SELECT COUNT(*) FROM $0;", table_name)));
+    SCHECK_EQ(1, PQnfields(res.get()), IllegalState, "");
+    SCHECK_EQ(1, PQntuples(res.get()), IllegalState, "");
+    auto table_count = VERIFY_RESULT(GetInt64(res.get(), 0, 0));
+    SCHECK_EQ(count, static_cast<decltype(count)>(table_count), IllegalState, "");
+    return Status::OK();
+  };
+  ASSERT_OK(check_rows_count(kTableName, kNumRows));
+
+  // Try split range partitioned index table
+  {
+    auto table_id = ASSERT_RESULT(GetTableIDFromTableName(kIdx1Name));
+    auto tablets = ListTableActiveTabletLeadersPeers(cluster_.get(), table_id);
+    ASSERT_EQ(1, tablets.size());
+
+    auto parent_tablet = tablets.front();
+    auto status = InvokeSplitTabletRpc(parent_tablet->tablet_id());
+
+    auto version = parent_tablet->tablet()->schema()->table_properties().partition_key_version();
+    if (version == 0) {
+      // Index tablet split is not supported for old index tables with range partitioning
+      ASSERT_EQ(status.IsNotSupported(), true) << "Unexpected status: " << status.ToString();
+    } else {
+      ASSERT_OK(status);
+      ASSERT_OK(WaitFor([&]() -> Result<bool> {
+        return ListTableActiveTabletLeadersPeers(cluster_.get(), table_id).size() == 2;
+      }, 15s * kTimeMultiplier, "Wait for split completion."));
+
+      ASSERT_OK(check_rows_count(kTableName, kNumRows));
+    }
+  }
+
+  // Try split hash partitioned index table, it does not depend on a partition key version
+  {
+    auto table_id = ASSERT_RESULT(GetTableIDFromTableName(kIdx2Name));
+    auto tablets = ListTableActiveTabletLeadersPeers(cluster_.get(), table_id);
+    ASSERT_EQ(1, tablets.size());
+
+    auto parent_tablet = tablets.front();
+    ASSERT_OK(InvokeSplitTabletRpc(parent_tablet->tablet_id()));
+    ASSERT_OK(WaitFor([&]() -> Result<bool> {
+      return ListTableActiveTabletLeadersPeers(cluster_.get(), table_id).size() == 2;
+    }, 15s * kTimeMultiplier, "Wait for split completion."));
+    ASSERT_OK(check_rows_count(kTableName, kNumRows));
+  }
+
+  // Try split non-index tablet, it does not depend on a partition key version
+  {
+    auto table_id = ASSERT_RESULT(GetTableIDFromTableName(kTableName));
+    auto tablets = ListTableActiveTabletLeadersPeers(cluster_.get(), table_id);
+    ASSERT_EQ(1, tablets.size());
+
+    auto parent_tablet = tablets.front();
+    ASSERT_OK(InvokeSplitTabletRpc(parent_tablet->tablet_id()));
+    ASSERT_OK(WaitFor([&]() -> Result<bool> {
+      return ListTableActiveTabletLeadersPeers(cluster_.get(), table_id).size() == 2;
+    }, 15s * kTimeMultiplier, "Wait for split completion."));
+    ASSERT_OK(check_rows_count(kTableName, kNumRows));
+  }
+}
+
 } // namespace pgwrapper
 } // namespace yb
